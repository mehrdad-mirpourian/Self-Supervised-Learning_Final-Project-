# -*- coding: utf-8 -*-
"""Self Supervised Learning-Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i_qMqFgNEPoOQYeF4IKqfasrY55njWHa

## STEP 1: Import All Required Libraries and Set Up the Environment
"""

# ================================================================
# Core Libraries
# ================================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import torchvision
import torchvision.transforms as T
from torchvision.models import resnet50

import numpy as np
from tqdm import tqdm
import random
import math
import time

# ================================================================
# GPU / Device Setup (Lambda Cloud Compatible)
# ================================================================
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using GPU:", torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print(" No GPU detected — running on CPU.")

# ================================================================
# Set random seeds for reproducibility
# ================================================================
seed = 41
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("Environment setup complete.")

"""## STEP 2: Define MoCo-v2 Style Data Augmentations

This class builds the MoCo-v2 data augmentation pipeline and applies it twice to the same input image.
The result is two different strongly-augmented views of the same image, which are used as the positive pair in contrastive self-supervised learning.

In other words:

- Take one image
- Apply strong random augmentations twice
- Produce two correlated but different views
- Feed them to the contrastive model (query/key)

This encourages the model to learn invariance to color, crop, blur, distortion, etc., which is essential for MoCo-style SSL.
"""

# These augmentations generate *two strongly augmented views* of each image.
# They are essential for contrastive learning because the model must learn
# invariances to color, crop, blur, distortion, etc.

class MoCoTransform:
    """Applies two strong augmentations to the same image."""

    def __init__(self, image_size=96):
        # MoCo-v2 Augmentation pipeline:
        self.base_transform = T.Compose([
            # 1. Random resized crop (strong spatial diversity)
            T.RandomResizedCrop(image_size, scale=(0.2, 1.0)),

            # 2. Random horizontal flip
            T.RandomHorizontalFlip(p=0.5),

            # 3. Color jitter (brightness, contrast, saturation, hue)
            T.RandomApply(
                [T.ColorJitter(0.4, 0.4, 0.4, 0.1)],
                p=0.8
            ),

            # 4. Random grayscale conversion
            T.RandomGrayscale(p=0.2),

            # Mild solarization (improves invariance)
            # T.RandomApply(
            #     [T.RandomSolarize(128)],
            #     p=0.2
            # ),

            # 5. Gaussian blur (MoCo v2 uses it heavily)
            T.RandomApply(
                [T.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0))],
                p=0.5
            ),

            # 6. Convert PIL → Tensor
            T.ToTensor(),

            # 7. Normalize to standard ImageNet stats
            T.Normalize(
                mean=(0.485, 0.456, 0.406),
                std=(0.229, 0.224, 0.225)
            ),
        ])

    def __call__(self, x):
        # Return TWO differently augmented views of the same image
        return self.base_transform(x), self.base_transform(x)


print("MoCo-v2 augmentations initialized successfully.")

"""## STEP 3: Implement the Contrastive Loss (InfoNCE) + Covariance Regularization Loss

This module defines a hybrid loss for our self-supervised model that combines:

1. MoCo-style InfoNCE contrastive loss

This part forces the model to pull together the embeddings of two augmented views of the same image (q and k⁺) and push them away from a large set of negative samples stored in a FIFO queue.
This teaches invariance to augmentations and builds discriminative features.

2. VICReg-style covariance regularization

In addition to contrastive learning, this part penalizes correlations between embedding dimensions so each feature dimension carries unique information.
This reduces redundancy and improves representation quality.

We can choose whether this covariance term uses:

- only the positive pair (q, k⁺), or
- (optionally) samples from the negative queue as well.

3. Final output

The function returns:
- the combined loss used for backprop,
- the pure contrastive component,
- the pure covariance regularization component.

So overall, this block creates a hybrid MoCo + VICReg loss that encourages:
- invariance to augmentations (via contrastive loss)
- diversity across feature dimensions (via covariance regularization)

This helps our model learn richer, more stable representations.
"""

# This includes two main components:
# 1. MoCo InfoNCE contrastive loss.
# 2. VICReg-style covariance loss to decorrelate embedding dimensions.

# We support:
#   - Using only (q, k+) for covariance loss
#   - OR including samples from the negative queue (optional)
# via the flag `use_queue_for_cov`.


class MoCoCovLoss(nn.Module):
    def __init__(self, temperature=0.07, lambda_cov=1e-5, use_queue_for_cov=False):
        """
        temperature: contrastive temperature parameter (tau)
        lambda_cov: weight for covariance regularization
        use_queue_for_cov: whether to include queue samples in covariance loss
        """
        super().__init__()
        self.tau = temperature
        self.lambda_cov = lambda_cov
        self.use_queue_for_cov = use_queue_for_cov

    def forward(self, q, k_pos, queue_neg):
        """
        q: (B, D) query embeddings
        k_pos: (B, D) positive key embeddings
        queue_neg: (D, K) negative keys from FIFO queue
        """
        # ------------------------------------------------------------
        # 1. Compute InfoNCE contrastive loss
        # ------------------------------------------------------------
        # Positive logits: q · k+
        pos_logits = torch.sum(q * k_pos, dim=1, keepdim=True)  # (B, 1)

        # Negative logits: q · K
        neg_logits = torch.einsum('nd,dk->nk', q, queue_neg)     # (B, K)

        # Scale by temperature
        pos_logits = pos_logits / self.tau
        neg_logits = neg_logits / self.tau

        # Concatenate pos + neg logits
        logits = torch.cat([pos_logits, neg_logits], dim=1)      # (B, 1+K)

        # Labels: positive is always index 0
        labels = torch.zeros(q.size(0), dtype=torch.long, device=q.device)

        # Cross-entropy loss for contrastive learning
        loss_contrast = F.cross_entropy(logits, labels)

        # ------------------------------------------------------------
        # 2. Compute VICReg covariance regularization loss
        # ------------------------------------------------------------
        # Build feature matrix Z = [q; k+] or optionally add queue samples
        if self.use_queue_for_cov:
            # Take a random subset of negatives from the queue
            K = queue_neg.shape[1]
            num_samples = min(1024, K)  # limit sample size
            idx = torch.randperm(K, device=q.device)[:num_samples]
            queue_subset = queue_neg[:, idx].T  # shape: (num_samples, D)

            Z = torch.cat([q, k_pos, queue_subset], dim=0)  # (B + B + num_samples, D)
        else:
            Z = torch.cat([q, k_pos], dim=0)  # (2B, D)

        # Compute covariance matrix C = Cov(Z)
        Z = Z - Z.mean(dim=0, keepdim=True)      # center
        C = (Z.T @ Z) / (Z.size(0) - 1)          # covariance: (D, D)

        # Penalize off-diagonal elements of covariance
        diag_mask = torch.eye(C.size(0), device=C.device).bool()
        off_diag = C[~diag_mask]
        loss_cov = (off_diag ** 2).sum()

        # ------------------------------------------------------------
        # 3. Combine losses
        # ------------------------------------------------------------
        loss = loss_contrast + self.lambda_cov * loss_cov

        return loss, loss_contrast, loss_cov


print("MoCo InfoNCE + covariance loss module ready.")

"""## STEP 4 — MoCo-v2 Model Setup with 128-Dim Projection Head and Negative Queue

This block builds the full MoCo-v2 backbone we will use for self-supervised learning. It creates.
This entire block assembles a fully functional MoCo-v2 encoder (query + key), builds a matching 128-dim projection head, and initializes the large negative queue used for contrastive learning.

1. A Projection Head (2048 → 128)

A small MLP that takes ResNet-50 features (2048-dim) and maps them into a 128-dim contrastive space, normalized for InfoNCE training.
This is the standard MoCo-v2 projection design.

2. A Dual-Encoder MoCo Model (query + key)

The model contains:

- encoder_q – the normal ResNet-50 backbone + projection head (trainable)

- encoder_k – the momentum encoder (no gradient updates)

At initialization, encoder_k is copied from encoder_q, ensuring they start identical.
This is essential for stability in MoCo-style contrastive learning.

Both encoders output 128-dim embeddings, guaranteeing shape consistency with the queue and the loss function.

3. A 128-Dim FIFO Queue of Negative Samples

A memory queue that stores thousands of past key embeddings and acts as the large negative sample bank for contrastive learning.

As new batches arrive:

- new key embeddings are enqueued
- the oldest ones are removed
This keeps a constantly refreshed pool of negatives.

4. GPU Instantiation

Finally, the model and queue are moved to GPU, ready for SSL training.


"""

# ======================================================================
# FIXED & CORRECTED MoCo Model + Projection Head + Queue (128-dim)
# ======================================================================
# This block ensures that:
#   - encoder_q outputs 128-d features
#   - encoder_k outputs 128-d features
#   - queue stores 128-d features
#   - q, k, queue embeddings perfectly match in dimension
# ======================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

# Device from your earlier step
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----------------------------------------------------------
# 1. Projection Head: 2048 → 2048 → 128 (MoCo-v2 style MLP)
# ----------------------------------------------------------
class ProjectionMLP(nn.Module):
    """
    Standard MoCo-v2 projection head:
        - Input: 2048-dim backbone features (ResNet-50)
        - Output: 128-dim normalized projection for contrastive learning
    """
    def __init__(self, dim_in=2048, dim_hidden=2048, dim_out=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(dim_in, dim_hidden),
            nn.ReLU(inplace=True),
            nn.Linear(dim_hidden, dim_out)
        )

    def forward(self, x):
        x = self.mlp(x)
        return F.normalize(x, dim=1)


# ----------------------------------------------------------
# 2. MoCo Model (ResNet-50 backbone + projection head)
# ----------------------------------------------------------
class MoCoResNet50(nn.Module):
    """
    MoCo-v2 architecture:
      - encoder_q: trainable
      - encoder_k: EMA momentum encoder (no grads)
    """
    def __init__(self, dim_feature=128):
        super().__init__()

        # Load ResNet-50 backbone (random initialization)
        backbone = models.resnet50(weights=None)
        backbone.fc = nn.Identity()

        # Query encoder
        self.encoder_q = nn.Sequential(
            backbone,
            ProjectionMLP(dim_in=2048, dim_hidden=2048, dim_out=dim_feature)
        )

        # Key encoder (EMA)
        backbone_k = models.resnet50(weights=None)
        backbone_k.fc = nn.Identity()
        self.encoder_k = nn.Sequential(
            backbone_k,
            ProjectionMLP(dim_in=2048, dim_hidden=2048, dim_out=dim_feature)
        )

        # Initialize key encoder to match query encoder
        for param_q, param_k in zip(self.encoder_q.parameters(),
                                    self.encoder_k.parameters()):
            param_k.data.copy_(param_q.data)
            param_k.requires_grad = False


# ----------------------------------------------------------
# 3. FIFO Queue for Negative Samples (Dimension = 128)
# ----------------------------------------------------------
class MoCoQueue(nn.Module):
    """
    FIFO memory queue storing 128-d embeddings.
    """
    def __init__(self, size=8192, dim=128):
        super().__init__()
        self.size = size
        self.dim = dim

        # queue: (size, dim)
        self.register_buffer("queue", torch.randn(size, dim))
        self.queue = F.normalize(self.queue, dim=1)

        self.register_buffer("ptr", torch.zeros(1, dtype=torch.long))

    @torch.no_grad()
    def enqueue(self, keys):
        """
        Add new keys to queue, remove oldest entries.
        keys: (batch_size, dim)
        """
        batch_size = keys.shape[0]
        ptr = int(self.ptr)

        if ptr + batch_size <= self.size:
            self.queue[ptr:ptr+batch_size] = keys
            self.ptr[0] = (ptr + batch_size) % self.size
        else:
            # wrap around
            n1 = self.size - ptr
            n2 = batch_size - n1
            self.queue[ptr:] = keys[:n1]
            self.queue[:n2] = keys[n1:]
            self.ptr[0] = n2


# ----------------------------------------------------------
# 4. Instantiate Model + Queue (DIM=128) on GPU or CPU safely
# ----------------------------------------------------------
model = MoCoResNet50(dim_feature=128).to(device)
queue = MoCoQueue(size=8192, dim=128).to(device)

print(">>> MoCo-ResNet50 model + 128-d queue initialized successfully!")

"""## STEP 5: Local Dataset Preparation Using Snapshot Download and Random Subsampling


This block configures and prepares the unlabeled training dataset for our self-supervised pretraining. Instead of using the full 500k–700k Hugging Face dataset (which is large and slow to load), it downloads only the ZIP shards, extracts metadata, and saves a random subset of x images locally for fast pipeline testing.

Here’s what this code accomplishes at a high level:

1. Choose the data source

We specify that training images should come from a local x-image subset rather than the full Hugging Face dataset. This makes early debugging and model testing much faster.

2. Download raw dataset ZIP files

Using snapshot_download(), the code fetches only the zipped shards from the Hugging Face repo. This avoids downloading unnecessary metadata and keeps the process efficient.

3. Parse the ZIP files to see all image names

Instead of extracting everything, the code looks inside each ZIP and gathers a list of every .jpg in the dataset.

4. Randomly select x images

From the full list of available images, the script chooses a random sample of 30k images.
This subset is used to quickly test our pipeline before scaling up to the full dataset.

5. Extract and save the selected images locally

For each selected image:

read it directly from its ZIP file

This block gives us a clean folder of x .jpg files ready for our dataloader.

### Step 5
"""

# # ===============================================================
# # LOAD LOCAL 500K DATASET (EXTRACTED ON LAMBDA)
# # ===============================================================

# import os
# from torch.utils.data import DataLoader
# from torchvision import transforms

# DATA_DIR = "/home/ubuntu/dataset_500k/cc3m_all/train"
# print("Using local dataset:", DATA_DIR)

# # MoCo-style strong augmentations (same as your pipeline)
# ssl_transform = MoCoTransform(image_size=96)

# # Our SSL dataset: loads *.jpg and returns (view1, view2)
# ssl_dataset = LocalMoCoDataset(
#     root_dir=DATA_DIR,
#     transform=ssl_transform
# )

# train_loader = DataLoader(
#     ssl_dataset,
#     batch_size=256,        # Reduce to 128 if OOM
#     shuffle=True,
#     num_workers=8,
#     pin_memory=True,
#     drop_last=True
# )

# print("Loaded", len(ssl_dataset), "images.")

"""## STEP 6: Building a Local SSL Dataset and DataLoader for MoCo Training


This block constructs the dataset and dataloader that feed training images into our MoCo pipeline. It loads the locally saved JPEG images (e.g., the 30k subset we created earlier), applies the MoCo augmentation pipeline, and prepares batches for contrastive training.

Here’s the high-level purpose:

1. Custom Dataset for Local Images

The LocalMoCoDataset class:
- scans a directory of .jpg files,
- loads each image from disk,
- applies our MoCoTransform to produce two augmented views of the same image,
- returns these paired views for contrastive learning.

This turns our folder of unlabeled images into the correct supervised-by-augmentations structure needed for MoCo-style SSL.

2. Create a DataLoader for Efficient Training

The DataLoader:
- batches the augmented pairs (e.g., 128 images per batch),
- shuffles the dataset,
- preloads data with workers (num_workers=2),
- drops the last incomplete batch for consistency.

This ensures that our SSL training loop receives a steady stream of (x1, x2) pairs efficiently and with proper batching.
"""

# ================================================================
# Create dataset & dataloader for LOCAL images
# ================================================================
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import glob
import os

class LocalMoCoDataset(Dataset):
    """
    Loads images from a local directory.
    Returns two augmented views for MoCo training.
    """
    def __init__(self, root_dir, transform):
        self.root_dir = root_dir
        self.transform = transform
        self.files = sorted(glob.glob(os.path.join(root_dir, "*.jpg")))
        print(f"Local dataset: {len(self.files)} images found.")

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        img_path = self.files[idx]
        img = Image.open(img_path).convert("RGB")
        (x1, x2) = self.transform(img)
        return x1, x2


# ================================================================
# Instantiate dataset + loader
# ================================================================
ssl_transform = MoCoTransform(image_size=96)

ssl_dataset = LocalMoCoDataset(
    root_dir="/home/ubuntu/dataset_500k/cc3m_all/train",   # <= MUST be /home/ubuntu/... in Lambda
    transform=ssl_transform
)

ssl_loader = DataLoader(
    ssl_dataset,
    batch_size=256,
    shuffle=True,
    num_workers=8,
    pin_memory=True,
    drop_last=True
)

print("✓ Local SSL DataLoader created.")
print("Batches per epoch:", len(ssl_loader))

"""***

## STEP 7 — Setting Up Full MoCo-Cov Training Configuration (Hyperparameters, Optimizer, Scheduler, Checkpointing)

This block prepares everything needed for real MoCo-Cov self-supervised training, but does not start training yet. It sets up hyperparameters, optimizer, learning-rate schedule, and checkpoint paths—essentially the full training environment so our next step can immediately begin pretraining.

Here’s the high-level purpose:

1. Define All Training Hyperparameters

A single config dictionary stores every important setting for MoCo-Cov training, including:

- number of epochs
- batch size
- learning rate & momentum
- InfoNCE temperature
- EMA momentum for the key encoder
- covariance-loss strength
- queue size
- checkpoint file path

This gives our training loop a clear and centralized configuration we can easily adjust later.

2. Create the Optimizer (SGD + Momentum)

The model parameters are attached to an SGD optimizer, which is the standard optimizer used in MoCo-v2 and similar contrastive SSL methods.
This sets momentum and weight decay exactly as in the original papers.

3. Set Up a Cosine Learning-Rate Scheduler

We initialize a cosine-annealing schedule, which slowly decreases the LR from its initial value down to zero across all epochs.
This is a standard and effective schedule for contrastive pretraining.

4. Prepare Checkpointing Settings

Defines where to save training checkpoints and how often to save them (every epoch), ensuring training can resume if interrupted.
"""

# Goal:
#   - Define all hyperparameters for the REAL MoCo-Cov training
#   - Create optimizer (SGD)
#   - Create cosine LR scheduler
#   - Prepare checkpointing
#   - DO NOT start training yet (training will be Step 12)

# This block sets up everything needed for long SSL pretraining.
# ================================================================

import torch
import torch.nn as nn
import torch.optim as optim
import math
from torch.optim.lr_scheduler import CosineAnnealingLR

print("Configuring MoCo-Cov training ...")

# --------------------------------------------------------
# 1. Hyperparameters for sanity-check SSL training
# --------------------------------------------------------
config = {
    "epochs": 20,
    "batch_size": 256,
    "lr": 0.03,
    "momentum": 0.9,
    "weight_decay": 1e-4,
    "tau": 0.07,
    "m": 0.999,
    "lambda_cov": 1e-5,
    "queue_size": 8192,
    "save_every": 1,
    "checkpoint_path": "/home/ubuntu/moco_cov_checkpoint.pth"  # <— FIXED
}

# Print configuration
for k,v in config.items():
    print(f"{k}: {v}")

# --------------------------------------------------------
# 2. Re-create optimizer (SGD)
# --------------------------------------------------------
optimizer = optim.SGD(
    model.parameters(),
    lr=config["lr"],
    momentum=config["momentum"],
    weight_decay=config["weight_decay"],
)

print("\nOptimizer ready (SGD with momentum).")

# --------------------------------------------------------
# 3. Cosine learning rate schedule
# --------------------------------------------------------
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=config["epochs"],
    eta_min=0.0
)

print("Cosine LR scheduler ready.")

# --------------------------------------------------------
# 4. Final print-out
# --------------------------------------------------------

"""## STEP 8: MoCo-Cov Sanity-Check Pretraining Loop

This block runs a complete self-supervised training loop using the MoCo-ResNet50 model on our local subset. Each iteration:

1. Computes q and k embeddings from two augmented views.
2. Builds the contrastive logits using the positive pair and the queue of negatives.
3. Adds a covariance penalty to reduce redundancy in the embeddings.
4. Backpropagates through encoder_q and updates encoder_k with momentum.
5. Updates the negative queue with the new keys.
6. Logs metrics and saves a checkpoint at the end of each epoch.

In short, this block performs the actual MoCo-Cov self-supervised pretraining, combining contrastive learning + covariance regularization in a full end-to-end training loop.
"""

print("Starting SANITY-CHECK MoCo-Cov training on local subset...\n")

loss_fn = MoCoCovLoss(
    temperature=config["tau"],
    lambda_cov=config["lambda_cov"],
    use_queue_for_cov=False
).to(device)

num_epochs = config["epochs"]
log_every = 100
save_every = config["save_every"]
m = config["m"]

# -----------------------------
# Add GradScaler for AMP
# -----------------------------
scaler = torch.cuda.amp.GradScaler()

for epoch in range(num_epochs):

    model.train()
    epoch_loss = 0.0
    epoch_contrast = 0.0
    epoch_cov = 0.0

    print(f"\n==== Epoch {epoch+1}/{num_epochs} ====")

    for step, (x1, x2) in enumerate(ssl_loader):

        x1 = x1.to(device, non_blocking=True)
        x2 = x2.to(device, non_blocking=True)

        optimizer.zero_grad(set_to_none=True)

        # =============================================
        # 1. FORWARD PASS WITH AMP
        # =============================================
        with torch.cuda.amp.autocast():

            # embeddings
            q = model.encoder_q(x1)
            q = F.normalize(q, dim=1)

            with torch.no_grad():
                k = model.encoder_k(x2)
                k = F.normalize(k, dim=1)

            # contrastive loss
            l_pos = torch.einsum('nc,nc->n', q, k).unsqueeze(1)
            l_neg = torch.einsum('nc,kc->nk', q, queue.queue.clone().detach())

            logits = torch.cat([l_pos, l_neg], dim=1) / config["tau"]
            labels = torch.zeros(q.size(0), dtype=torch.long, device=device)
            loss_contrast = F.cross_entropy(logits, labels)

            # covariance loss
            Z = torch.cat([q, k], dim=0)
            Z = Z - Z.mean(dim=0, keepdim=True)
            C = (Z.T @ Z) / (Z.size(0) - 1)
            cov_loss = (C**2).sum() - (C.diag()**2).sum()
            loss_cov = config["lambda_cov"] * cov_loss

            # total loss
            loss = loss_contrast + loss_cov

        # =============================================
        # 2. BACKWARD + OPTIMIZER STEP WITH AMP
        # =============================================
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # =============================================
        # 3. EMA UPDATE
        # =============================================
        with torch.no_grad():
            for param_q, param_k in zip(model.encoder_q.parameters(),
                                        model.encoder_k.parameters()):
                param_k.data = param_k.data * m + param_q.data * (1 - m)

        # update queue
        queue.enqueue(k.detach())

        # metrics
        epoch_loss += loss.item()
        epoch_contrast += loss_contrast.item()
        epoch_cov += loss_cov.item()

        if (step + 1) % log_every == 0:
            print(
                f"[Epoch {epoch+1}/{num_epochs}] "
                f"Step {step+1}/{len(ssl_loader)} | "
                f"Loss: {loss.item():.4f} | "
                f"Contrast: {loss_contrast.item():.4f} | "
                f"Cov: {loss_cov.item():.4f} | "
                f"LR: {scheduler.get_last_lr()[0]:.6f}"
            )

    print(
        f"\n>>> Epoch {epoch+1} Summary: "
        f"AvgLoss={epoch_loss/len(ssl_loader):.4f}, "
        f"AvgContrast={epoch_contrast/len(ssl_loader):.4f}, "
        f"AvgCov={epoch_cov/len(ssl_loader):.4f}"
    )

    scheduler.step()

    torch.save(
        {
            "epoch": epoch+1,
            "state_dict": model.state_dict(),
            "optimizer": optimizer.state_dict(),
            "scheduler": scheduler.state_dict(),
            "queue": queue.queue.clone(),
        },
        config["checkpoint_path"]
    )
    print(f"Checkpoint saved to {config['checkpoint_path']}")

print("\n=== SANITY-CHECK TRAINING COMPLETE ===")

"""## STEP 9. Preparing CIFAR-100 for Downstream Evaluation  

This block loads the CIFAR-100 dataset and prepares it for downstream evaluation of our SSL encoder. It applies simple, non-contrastive transforms (resize + normalize), then builds train/test DataLoaders.
"""

import torchvision
import torchvision.transforms as T
from torch.utils.data import DataLoader

# Basic eval transforms (no strong augmentations)
cifar_transform = T.Compose([
    T.Resize((96, 96)),     # match our backbone resolution
    T.ToTensor(),
    T.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225)
    ),
])

# CIFAR-100 path for Lambda
cifar100_root = "/home/ubuntu/cifar100"   # <-- FIXED

# Load CIFAR-100 train/test
cifar_train = torchvision.datasets.CIFAR100(
    root=cifar100_root,
    train=True,
    download=True,
    transform=cifar_transform
)

cifar_test = torchvision.datasets.CIFAR100(
    root=cifar100_root,
    train=False,
    download=True,
    transform=cifar_transform
)

# Dataloaders
cifar_train_loader = DataLoader(
    cifar_train,
    batch_size=256,
    shuffle=True,
    num_workers=8,
    pin_memory=True
)

cifar_test_loader = DataLoader(
    cifar_test,
    batch_size=256,
    shuffle=False,
    num_workers=8,
    pin_memory=True
)

print("CIFAR-100 loaded successfully!")
print("Train samples:", len(cifar_train))
print("Test samples:", len(cifar_test))

"""## STEP 10: Extracting Frozen Features for Downstream Evaluation


This block defines a helper function that runs our pretrained encoder_q over an evaluation dataset (like CIFAR-100) to extract 128-dim feature vectors for every image. It keeps the encoder frozen, normalizes the embeddings, and returns:

one big tensor of all features

one big tensor of their labels

These extracted features are later used for downstream tasks such as k-NN classification or a linear probe.
"""

import torch
import torch.nn.functional as F

@torch.no_grad()
def extract_features(encoder_q, dataloader, device):
    """
    Extract frozen features from encoder_q.
    Returns:
        features: Tensor of shape [N, 128]
        labels:   Tensor of shape [N]
    """
    encoder_q.eval()
    features_list = []
    labels_list = []

    for images, labels in dataloader:
        images = images.to(device, non_blocking=True)

        # Forward pass through frozen encoder
        feats = encoder_q(images)

        # Normalize (safe)
        feats = F.normalize(feats, dim=1)

        features_list.append(feats.cpu())
        labels_list.append(labels.cpu())

    # Concatenate all batches into big tensors
    all_features = torch.cat(features_list, dim=0)
    all_labels = torch.cat(labels_list, dim=0)

    print("Finished extracting features.")
    print("Feature tensor shape:", all_features.shape)
    print("Labels tensor shape:", all_labels.shape)

    return all_features, all_labels

train_feats, train_labels = extract_features(
    model.encoder_q,
    cifar_train_loader,
    device
)

test_feats, test_labels = extract_features(
    model.encoder_q,
    cifar_test_loader,
    device
)

"""## STEP 11: k-NN Classifier for Evaluating SSL Representations


This block implements a k-nearest-neighbors classifier used to evaluate how good our SSL features are. It compares test features to all training features using cosine similarity, finds each sample’s top-k neighbors, weights their votes by similarity, and predicts the class.
"""

import torch
import torch.nn.functional as F

def knn_classifier(train_feats, train_labels, test_feats, test_labels, k=16, temperature=0.07, device=None):
    """
    Improved k-NN classifier:
      - uses GPU for similarity (if available)
      - avoids Python loops for class score accumulation
      - much faster and scalable
    """

    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Normalize
    train_feats = F.normalize(train_feats, dim=1).to(device)
    test_feats = F.normalize(test_feats, dim=1).to(device)

    # labels stay on CPU (safer, easier)
    train_labels = train_labels.cpu()
    test_labels = test_labels.cpu()

    num_test = test_feats.size(0)
    num_train = train_feats.size(0)
    num_classes = train_labels.max().item() + 1

    batch_size = 400   # small enough for 4090/A100/V100 memory

    correct = 0

    print("Running improved k-NN evaluation...")

    for i in range(0, num_test, batch_size):
        end = min(i + batch_size, num_test)
        batch = test_feats[i:end]  # [B, D]

        # ---------------------------
        # 1. Cosine similarity on GPU
        # ---------------------------
        sim = torch.mm(batch, train_feats.t())     # [B, N_train]

        # top-k neighbors
        sim_val, sim_idx = sim.topk(k=k, dim=1)    # [B, k]

        # bring labels for neighbors
        neighbor_labels = train_labels[sim_idx.cpu()]  # [B, k]

        # softmax weights on GPU
        weights = torch.exp(sim_val / temperature)  # [B, k]

        # ---------------------------
        # 2. FAST scoring:
        #    convert labels → one-hot, weighted sum
        # ---------------------------
        # one-hot encode neighbor labels
        one_hot = torch.zeros(end - i, k, num_classes)
        one_hot.scatter_(2, neighbor_labels.unsqueeze(2), 1)

        # weight by similarity × temperature
        # (sim weights are still on GPU)
        weighted = one_hot.to(device) * weights.unsqueeze(2)

        # sum over k neighbors → final class scores
        class_scores = weighted.sum(dim=1)   # [B, C]

        # get predictions
        preds = class_scores.argmax(dim=1).cpu()

        # count correct predictions
        correct += (preds == test_labels[i:end]).sum().item()

    acc = 100.0 * correct / num_test
    print(f"Improved k-NN accuracy (k={k}): {acc:.2f}%")

    return acc

"""Running the KNN"""

knn_acc = knn_classifier(
    train_feats, train_labels,
    test_feats, test_labels,
    k=16,
    temperature=0.07,
    device=device        # <--- REQUIRED
)

"""## STEP 12: Linear Probe Classifier for Evaluating SSL Features


This block trains a simple linear classifier on top of frozen SSL features to measure how well the MoCo encoder supports supervised tasks. It:

- Wraps the features/labels into DataLoaders.
- Trains a single linear layer (no nonlinearities) for a small number of epochs.
- Evaluates accuracy on a test set after each epoch.
- Returns the final accuracy.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader

def linear_probe(
    train_feats, train_labels,
    test_feats, test_labels,
    num_epochs=20,
    batch_size=256,
    lr=0.01,
    device=None
):

    # ---- Safe device handling ----
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ---- Dataset ----
    train_dataset = TensorDataset(train_feats, train_labels)
    test_dataset = TensorDataset(test_feats, test_labels)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # ---- Determine number of classes ----
    num_classes = train_labels.max().item() + 1

    # ---- Linear classifier on frozen features ----
    classifier = nn.Linear(train_feats.size(1), num_classes).to(device)

    # ---- Optimizer (Adam is fine) ----
    optimizer = torch.optim.Adam(
        classifier.parameters(),
        lr=lr,
        weight_decay=1e-4
    )

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs
    )

    criterion = nn.CrossEntropyLoss()

    # ---- Training loop ----
    for epoch in range(num_epochs):
        classifier.train()
        total_loss = 0.0

        for feats, labels in train_loader:
            feats = feats.to(device)
            labels = labels.to(device)

            optimizer.zero_grad(set_to_none=True)
            logits = classifier(feats)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # ---- Evaluation ----
        classifier.eval()
        correct, total = 0, 0

        with torch.no_grad():
            for feats, labels in test_loader:
                feats = feats.to(device)
                labels = labels.to(device)
                logits = classifier(feats)
                preds = logits.argmax(dim=1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        acc = 100.0 * correct / total
        print(f"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | Test Acc: {acc:.2f}%")

        scheduler.step()

    return acc

"""Running linear probe"""

lp_acc = linear_probe(
    train_feats, train_labels,
    test_feats, test_labels,
    num_epochs=20,
    batch_size=256,
    lr=0.01,
    device=device       # <-- REQUIRED
)