# -*- coding: utf-8 -*-
"""Self Supervised Learning-Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i_qMqFgNEPoOQYeF4IKqfasrY55njWHa

## STEP 1: Import All Required Libraries and Set Up the Environment
"""

# ================================================================
# Core Libraries
# ================================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import torchvision
import torchvision.transforms as T
from torchvision.models import resnet50

import numpy as np
from tqdm import tqdm
import random
import math
import time

# ================================================================
# GPU / Device Setup (Lambda Cloud Compatible)
# ================================================================
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using GPU:", torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print(" No GPU detected — running on CPU.")

# ================================================================
# Set random seeds for reproducibility
# ================================================================
seed = 41
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("Environment setup complete.")

"""## STEP 2: Define MoCo-v2 Style Data Augmentations

This class builds the MoCo-v2 data augmentation pipeline and applies it twice to the same input image.
The result is two different strongly-augmented views of the same image, which are used as the positive pair in contrastive self-supervised learning.

In other words:

- Take one image
- Apply strong random augmentations twice
- Produce two correlated but different views
- Feed them to the contrastive model (query/key)

This encourages the model to learn invariance to color, crop, blur, distortion, etc., which is essential for MoCo-style SSL.
"""

# These augmentations generate *two strongly augmented views* of each image.
# They are essential for contrastive learning because the model must learn
# invariances to color, crop, blur, distortion, etc.

class MoCoTransform:
    """Applies two strong augmentations to the same image."""

    def __init__(self, image_size=96):
        # MoCo-v2 Augmentation pipeline:
        self.base_transform = T.Compose([
            # 1. Random resized crop (strong spatial diversity)
            T.RandomResizedCrop(image_size, scale=(0.2, 1.0)),

            # 2. Random horizontal flip
            T.RandomHorizontalFlip(p=0.5),

            # 3. Color jitter (brightness, contrast, saturation, hue)
            T.RandomApply(
                [T.ColorJitter(0.4, 0.4, 0.4, 0.1)],
                p=0.8
            ),

            # 4. Random grayscale conversion
            T.RandomGrayscale(p=0.2),

            # Mild solarization (improves invariance)
            # T.RandomApply(
            #     [T.RandomSolarize(128)],
            #     p=0.2
            # ),

            # 5. Gaussian blur (MoCo v2 uses it heavily)
            T.RandomApply(
                [T.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0))],
                p=0.5
            ),

            # 6. Convert PIL → Tensor
            T.ToTensor(),

            # 7. Normalize to standard ImageNet stats
            T.Normalize(
                mean=(0.485, 0.456, 0.406),
                std=(0.229, 0.224, 0.225)
            ),
        ])

    def __call__(self, x):
        # Return TWO differently augmented views of the same image
        return self.base_transform(x), self.base_transform(x)


print("MoCo-v2 augmentations initialized successfully.")

"""## STEP 3: Implement the Contrastive Loss (InfoNCE) + Covariance Regularization Loss

This module defines a hybrid loss for our self-supervised model that combines:

1. MoCo-style InfoNCE contrastive loss

This part forces the model to pull together the embeddings of two augmented views of the same image (q and k⁺) and push them away from a large set of negative samples stored in a FIFO queue.
This teaches invariance to augmentations and builds discriminative features.

2. VICReg-style covariance regularization

In addition to contrastive learning, this part penalizes correlations between embedding dimensions so each feature dimension carries unique information.
This reduces redundancy and improves representation quality.

We can choose whether this covariance term uses:

- only the positive pair (q, k⁺), or
- (optionally) samples from the negative queue as well.

3. Final output

The function returns:
- the combined loss used for backprop,
- the pure contrastive component,
- the pure covariance regularization component.

So overall, this block creates a hybrid MoCo + VICReg loss that encourages:
- invariance to augmentations (via contrastive loss)
- diversity across feature dimensions (via covariance regularization)

This helps our model learn richer, more stable representations.
"""

class MoCoCovLoss(nn.Module):
    def __init__(self, temperature=0.07, lambda_cov=1e-2, use_queue_for_cov=False):
        super().__init__()
        self.tau = temperature
        self.lambda_cov = lambda_cov
        self.use_queue_for_cov = use_queue_for_cov

    def forward(self, q_raw, k_raw, queue_neg):
        """
        q_raw, k_raw: UNnormalized embeddings from projector
        """

        # =============================================
        # 1. Contrastive loss uses NORMALIZED features
        # =============================================
        q = F.normalize(q_raw, dim=1)
        k = F.normalize(k_raw, dim=1)

        pos_logits = torch.sum(q * k, dim=1, keepdim=True)   # (B,1)
        neg_logits = torch.einsum('nd,dk->nk', q, queue_neg) # (B,K)

        logits = torch.cat([pos_logits, neg_logits], dim=1) / self.tau
        labels = torch.zeros(q.size(0), dtype=torch.long, device=q.device)
        loss_contrast = F.cross_entropy(logits, labels)

        # =============================================
        # 2. Covariance loss uses DETACHED raw features
        # =============================================
        q_cov = q_raw.detach()
        k_cov = k_raw.detach()

        if self.use_queue_for_cov:
            K = queue_neg.shape[1]
            n = min(1024, K)
            idx = torch.randperm(K, device=q.device)[:n]
            q_extra = queue_neg[:, idx].T
            Z = torch.cat([q_cov, k_cov, q_extra], dim=0)
        else:
            Z = torch.cat([q_cov, k_cov], dim=0)

        Z = Z - Z.mean(dim=0, keepdim=True)
        C = (Z.T @ Z) / (Z.size(0) - 1)

        diag = torch.eye(C.size(0), device=C.device).bool()
        loss_cov = (C[~diag] ** 2).sum()

        # =============================================
        # 3. Combine losses
        # =============================================
        loss = loss_contrast + self.lambda_cov * loss_cov

        return loss, loss_contrast, loss_cov

"""## STEP 4 — MoCo-v2 Model Setup with 128-Dim Projection Head and Negative Queue

This block builds the full MoCo-v2 backbone we will use for self-supervised learning. It creates.
This entire block assembles a fully functional MoCo-v2 encoder (query + key), builds a matching 128-dim projection head, and initializes the large negative queue used for contrastive learning.

1. A Projection Head (2048 → 128)

A small MLP that takes ResNet-50 features (2048-dim) and maps them into a 128-dim contrastive space, normalized for InfoNCE training.
This is the standard MoCo-v2 projection design.

2. A Dual-Encoder MoCo Model (query + key)

The model contains:

- encoder_q – the normal ResNet-50 backbone + projection head (trainable)

- encoder_k – the momentum encoder (no gradient updates)

At initialization, encoder_k is copied from encoder_q, ensuring they start identical.
This is essential for stability in MoCo-style contrastive learning.

Both encoders output 128-dim embeddings, guaranteeing shape consistency with the queue and the loss function.

3. A 128-Dim FIFO Queue of Negative Samples

A memory queue that stores thousands of past key embeddings and acts as the large negative sample bank for contrastive learning.

As new batches arrive:

- new key embeddings are enqueued
- the oldest ones are removed
This keeps a constantly refreshed pool of negatives.

4. GPU Instantiation

Finally, the model and queue are moved to GPU, ready for SSL training.


"""

# ======================================================================
# UPDATED MoCo Model + Projection Head + Queue (128-dim)
# ======================================================================
# Fixes included:
#   - ProjectionMLP output is NOT normalized (essential for covariance loss)
#   - All normalization will be done in the training loop for contrastive
# ======================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----------------------------------------------------------
# 1. Projection Head: 2048 → 2048 → 128 (MoCo-v2 style MLP)
# ----------------------------------------------------------
class ProjectionMLP(nn.Module):
    """
    Standard MoCo-v2 MLP:
        - Input: 2048-dim backbone features (ResNet-50)
        - Output: 128-dim (RAW, not normalized)
    """
    def __init__(self, dim_in=2048, dim_hidden=2048, dim_out=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(dim_in, dim_hidden),
            nn.ReLU(inplace=True),
            nn.Linear(dim_hidden, dim_out)
        )

    def forward(self, x):
        return self.mlp(x)   # IMPORTANT: NO normalization here


# ----------------------------------------------------------
# 2. MoCo Encoder (ResNet-50 backbone + Projection MLP)
# ----------------------------------------------------------
class MoCoResNet50(nn.Module):
    """
    MoCo-v2 architecture:
      - encoder_q: trainable
      - encoder_k: EMA momentum encoder (no grads)
    """
    def __init__(self, dim_feature=128):
        super().__init__()

        # Query backbone
        backbone = models.resnet50(weights=None)
        backbone.fc = nn.Identity()

        # Key backbone
        backbone_k = models.resnet50(weights=None)
        backbone_k.fc = nn.Identity()

        # Query encoder
        self.encoder_q = nn.Sequential(
            backbone,
            ProjectionMLP(dim_in=2048, dim_hidden=2048, dim_out=dim_feature)
        )

        # Key encoder (EMA)
        self.encoder_k = nn.Sequential(
            backbone_k,
            ProjectionMLP(dim_in=2048, dim_hidden=2048, dim_out=dim_feature)
        )

        # Initialize key encoder weights to match query
        for param_q, param_k in zip(self.encoder_q.parameters(),
                                    self.encoder_k.parameters()):
            param_k.data.copy_(param_q.data)
            param_k.requires_grad = False


# ----------------------------------------------------------
# 3. FIFO Queue for Negative Keys (Size = 8192)
# ----------------------------------------------------------
class MoCoQueue(nn.Module):
    """
    FIFO queue storing normalized 128-d embeddings.
    """
    def __init__(self, size=8192, dim=128):
        super().__init__()
        self.size = size
        self.dim = dim

        # queue: (size, dim)
        self.register_buffer("queue", torch.randn(size, dim))
        self.queue = F.normalize(self.queue, dim=1)

        self.register_buffer("ptr", torch.zeros(1, dtype=torch.long))

    @torch.no_grad()
    def enqueue(self, keys):
        # normalize before writing into the queue
        keys = F.normalize(keys, dim=1)

        batch_size = keys.size(0)
        ptr = int(self.ptr)

        if ptr + batch_size <= self.size:
            self.queue[ptr:ptr + batch_size] = keys
            self.ptr[0] = (ptr + batch_size) % self.size
        else:
            n1 = self.size - ptr
            n2 = batch_size - n1
            self.queue[ptr:] = keys[:n1]
            self.queue[:n2] = keys[n1:]
            self.ptr[0] = n2


# ----------------------------------------------------------
# 4. Instantiate Model + Queue
# ----------------------------------------------------------
model = MoCoResNet50(dim_feature=128).to(device)
queue = MoCoQueue(size=8192, dim=128).to(device)

print(">>> MoCo-ResNet50 model + 128-d queue initialized successfully!")

"""## STEP 5: Local Dataset Preparation Using Snapshot Download and Random Subsampling


This block configures and prepares the unlabeled training dataset for our self-supervised pretraining. Instead of using the full 500k–700k Hugging Face dataset (which is large and slow to load), it downloads only the ZIP shards, extracts metadata, and saves a random subset of x images locally for fast pipeline testing.

Here’s what this code accomplishes at a high level:

1. Choose the data source

We specify that training images should come from a local x-image subset rather than the full Hugging Face dataset. This makes early debugging and model testing much faster.

2. Download raw dataset ZIP files

Using snapshot_download(), the code fetches only the zipped shards from the Hugging Face repo. This avoids downloading unnecessary metadata and keeps the process efficient.

3. Parse the ZIP files to see all image names

Instead of extracting everything, the code looks inside each ZIP and gathers a list of every .jpg in the dataset.

4. Randomly select x images

From the full list of available images, the script chooses a random sample of 30k images.
This subset is used to quickly test our pipeline before scaling up to the full dataset.

5. Extract and save the selected images locally

For each selected image:

read it directly from its ZIP file

This block gives us a clean folder of x .jpg files ready for our dataloader.

### Step 5
"""

# # ===============================================================
# # LOAD LOCAL 500K DATASET (EXTRACTED ON LAMBDA)
# # ===============================================================

# import os
# from torch.utils.data import DataLoader
# from torchvision import transforms

# DATA_DIR = "/home/ubuntu/dataset_500k/cc3m_all/train"
# print("Using local dataset:", DATA_DIR)

# # MoCo-style strong augmentations (same as your pipeline)
# ssl_transform = MoCoTransform(image_size=96)

# # Our SSL dataset: loads *.jpg and returns (view1, view2)
# ssl_dataset = LocalMoCoDataset(
#     root_dir=DATA_DIR,
#     transform=ssl_transform
# )

# train_loader = DataLoader(
#     ssl_dataset,
#     batch_size=256,        # Reduce to 128 if OOM
#     shuffle=True,
#     num_workers=8,
#     pin_memory=True,
#     drop_last=True
# )

# print("Loaded", len(ssl_dataset), "images.")

"""## STEP 6: Building a Local SSL Dataset and DataLoader for MoCo Training


This block constructs the dataset and dataloader that feed training images into our MoCo pipeline. It loads the locally saved JPEG images (e.g., the 30k subset we created earlier), applies the MoCo augmentation pipeline, and prepares batches for contrastive training.

Here’s the high-level purpose:

1. Custom Dataset for Local Images

The LocalMoCoDataset class:
- scans a directory of .jpg files,
- loads each image from disk,
- applies our MoCoTransform to produce two augmented views of the same image,
- returns these paired views for contrastive learning.

This turns our folder of unlabeled images into the correct supervised-by-augmentations structure needed for MoCo-style SSL.

2. Create a DataLoader for Efficient Training

The DataLoader:
- batches the augmented pairs (e.g., 128 images per batch),
- shuffles the dataset,
- preloads data with workers (num_workers=2),
- drops the last incomplete batch for consistency.

This ensures that our SSL training loop receives a steady stream of (x1, x2) pairs efficiently and with proper batching.
"""

# ================================================================
# Create dataset & dataloader for LOCAL images
# ================================================================
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import glob
import os

class LocalMoCoDataset(Dataset):
    """
    Loads images from a local directory.
    Returns two augmented views for MoCo training.
    """
    def __init__(self, root_dir, transform):
        self.root_dir = root_dir
        self.transform = transform
        self.files = sorted(glob.glob(os.path.join(root_dir, "*.jpg")))
        print(f"Local dataset: {len(self.files)} images found.")

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        img_path = self.files[idx]
        img = Image.open(img_path).convert("RGB")
        (x1, x2) = self.transform(img)
        return x1, x2


# ================================================================
# Instantiate dataset + loader
# ================================================================
ssl_transform = MoCoTransform(image_size=96)

ssl_dataset = LocalMoCoDataset(
    root_dir="/home/ubuntu/dataset_500k/cc3m_all/train",   # <= MUST be /home/ubuntu/... in Lambda
    transform=ssl_transform
)

ssl_loader = DataLoader(
    ssl_dataset,
    batch_size=256,
    shuffle=True,
    num_workers=8,
    pin_memory=True,
    drop_last=True
)

print("✓ Local SSL DataLoader created.")
print("Batches per epoch:", len(ssl_loader))

"""***

## STEP 7 — Setting Up Full MoCo-Cov Training Configuration (Hyperparameters, Optimizer, Scheduler, Checkpointing)

This block prepares everything needed for real MoCo-Cov self-supervised training, but does not start training yet. It sets up hyperparameters, optimizer, learning-rate schedule, and checkpoint paths—essentially the full training environment so our next step can immediately begin pretraining.

Here’s the high-level purpose:

1. Define All Training Hyperparameters

A single config dictionary stores every important setting for MoCo-Cov training, including:

- number of epochs
- batch size
- learning rate & momentum
- InfoNCE temperature
- EMA momentum for the key encoder
- covariance-loss strength
- queue size
- checkpoint file path

This gives our training loop a clear and centralized configuration we can easily adjust later.

2. Create the Optimizer (SGD + Momentum)

The model parameters are attached to an SGD optimizer, which is the standard optimizer used in MoCo-v2 and similar contrastive SSL methods.
This sets momentum and weight decay exactly as in the original papers.

3. Set Up a Cosine Learning-Rate Scheduler

We initialize a cosine-annealing schedule, which slowly decreases the LR from its initial value down to zero across all epochs.
This is a standard and effective schedule for contrastive pretraining.

4. Prepare Checkpointing Settings

Defines where to save training checkpoints and how often to save them (every epoch), ensuring training can resume if interrupted.
"""

# ================================================================
# Hyperparameters + Optimizer + LR Scheduler for MoCo-Cov Training
# ================================================================

import torch
import torch.nn as nn
import torch.optim as optim
import math
from torch.optim.lr_scheduler import CosineAnnealingLR

print("Configuring MoCo-Cov training ...")

# --------------------------------------------------------
# 1. Core hyperparameters for SSL training (20-epoch test)
# --------------------------------------------------------
config = {
    "epochs": 22,               # For testing. Use 100–200 later.
    "batch_size": 256,          # Correct for LR = 0.03
    "lr": 0.03,                 # MoCo-v2 LR for batch_size=256
    "momentum": 0.9,
    "weight_decay": 1e-4,
    "tau": 0.07,                # Temperature
    "m": 0.999,                 # Momentum for EMA encoder_k
    "lambda_cov": 1e-2,         # FIXED: covariance regularization strength
    "queue_size": 8192,
    "save_every": 1,
    "checkpoint_path": "/home/ubuntu/moco_cov_checkpoint.pth"
}

# Print configuration
for k, v in config.items():
    print(f"{k}: {v}")

# --------------------------------------------------------
# 2. Optimizer (SGD + Momentum)
# --------------------------------------------------------
optimizer = optim.SGD(
    model.parameters(),
    lr=config["lr"],
    momentum=config["momentum"],
    weight_decay=config["weight_decay"],
)

print("\nOptimizer ready (SGD with momentum).")

# --------------------------------------------------------
# 3. Cosine LR Scheduler (per-epoch stepping)
# --------------------------------------------------------
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=config["epochs"],   # decay over full training
    eta_min=0.0
)

print("Cosine LR scheduler ready (step once per epoch).")

# --------------------------------------------------------
# 4. End of setup
# --------------------------------------------------------
print("✓ MoCo-Cov hyperparameters configured successfully.")

"""## STEP 8: MoCo-Cov Sanity-Check Pretraining Loop

This block runs a complete self-supervised training loop using the MoCo-ResNet50 model on our local subset. Each iteration:

1. Computes q and k embeddings from two augmented views.
2. Builds the contrastive logits using the positive pair and the queue of negatives.
3. Adds a covariance penalty to reduce redundancy in the embeddings.
4. Backpropagates through encoder_q and updates encoder_k with momentum.
5. Updates the negative queue with the new keys.
6. Logs metrics and saves a checkpoint at the end of each epoch.

In short, this block performs the actual MoCo-Cov self-supervised pretraining, combining contrastive learning + covariance regularization in a full end-to-end training loop.
"""

# ================================================================
# STEP 8 — FINAL MoCo–Cov Self-Supervised Pretraining Loop
# ================================================================
# This is the ONLY correct training loop for your SSL model.
#   - Uses MoCoResNet50 (encoder_q, encoder_k)
#   - Uses MoCoQueue (128-d negative queue)
#   - Uses MoCoCovLoss (InfoNCE + Covariance penalty)
#   - Uses AMP for stable/fast A100 training
#   - Saves checkpoint EACH epoch:
#         /home/ubuntu/checkpoints/epoch_{i}.pth
# ================================================================

# print("Starting FINAL MoCo-Cov training...\n")

# loss_fn = MoCoCovLoss(
#     temperature=config["tau"],
#     lambda_cov=config["lambda_cov"],
#     use_queue_for_cov=False
# ).to(device)

# num_epochs = config["epochs"]
# log_every = 100
# m = config["m"]                        # EMA for encoder_k

# scaler = torch.cuda.amp.GradScaler()

# for epoch in range(num_epochs):

#     model.train()
#     epoch_loss = 0.0
#     epoch_contrast = 0.0
#     epoch_cov = 0.0

#     print(f"\n==== Epoch {epoch+1}/{num_epochs} ====")

#     for step, (x1, x2) in enumerate(ssl_loader):

#         # ---------------------------------------------------
#         # Move batch to GPU
#         # ---------------------------------------------------
#         x1 = x1.to(device, non_blocking=True)
#         x2 = x2.to(device, non_blocking=True)

#         optimizer.zero_grad(set_to_none=True)

#         # ---------------------------------------------------
#         # Compute SSL loss with AMP
#         # ---------------------------------------------------
#         with torch.cuda.amp.autocast():
#             # Encode queries
#             q = model.encoder_q(x1)
#             q = F.normalize(q, dim=1)

#             # Encode keys (ema)
#             with torch.no_grad():
#                 k = model.encoder_k(x2)
#                 k = F.normalize(k, dim=1)

#             # Unified MoCo-Cov loss
#             loss, loss_contrast, cov_loss_raw = loss_fn(
#                 q, k, queue.queue.clone().detach().T
#             )

#         # Backprop (AMP)
#         scaler.scale(loss).backward()
#         scaler.step(optimizer)
#         scaler.update()

#         # ---------------------------------------------------
#         # Momentum update of encoder_k
#         # ---------------------------------------------------
#         with torch.no_grad():
#             for param_q, param_k in zip(model.encoder_q.parameters(),
#                                         model.encoder_k.parameters()):
#                 param_k.data = param_k.data * m + param_q.data * (1 - m)

#         # ---------------------------------------------------
#         # Queue update
#         # ---------------------------------------------------
#         queue.enqueue(k.detach())

#         # ---------------------------------------------------
#         # Metric tracking
#         # ---------------------------------------------------
#         epoch_loss += loss.item()
#         epoch_contrast += loss_contrast.item()
#         epoch_cov += (config["lambda_cov"] * cov_loss_raw).item()

#         # ---------------------------------------------------
#         # Logging
#         # ---------------------------------------------------
#         if (step + 1) % log_every == 0:
#             print(
#                 f"[Epoch {epoch+1}/{num_epochs}] "
#                 f"Step {step+1}/{len(ssl_loader)} | "
#                 f"Loss: {loss.item():.4f} | "
#                 f"Contrast: {loss_contrast.item():.4f} | "
#                 f"Cov: {(config['lambda_cov'] * cov_loss_raw).item():.4f} | "
#                 f"LR: {scheduler.get_last_lr()[0]:.6f}"
#             )

#     # ---------------------------------------------------
#     # Epoch summary
#     # ---------------------------------------------------
#     print(
#         f"\n>>> Epoch {epoch+1} Summary: "
#         f"AvgLoss={epoch_loss/len(ssl_loader):.4f}, "
#         f"AvgContrast={epoch_contrast/len(ssl_loader):.4f}, "
#         f"AvgCov={epoch_cov/len(ssl_loader):.4f}"
#     )

#     scheduler.step()

#     # ---------------------------------------------------
#     # Save checkpoint for THIS epoch
#     # ---------------------------------------------------
#     save_path = f"/home/ubuntu/checkpoints/epoch_{epoch+1}.pth"
#     torch.save(
#         {
#             "epoch": epoch+1,
#             "state_dict": model.state_dict(),
#             "optimizer": optimizer.state_dict(),
#             "scheduler": scheduler.state_dict(),
#             "queue": queue.queue.clone(),
#         },
#         save_path
#     )
#     print(f"Checkpoint saved to {save_path}")

# print("\n=== FINAL SSL TRAINING COMPLETE ===")

# ================================================================
# STEP 8 — FINAL MoCo–Cov Self-Supervised Pretraining Loop (FIXED)
# ================================================================
# This version:
#   ✓ Uses RAW q_raw / k_raw for covariance loss     (critical)
#   ✓ Uses normalized keys ONLY when enqueuing       (MoCo rule)
#   ✓ Keeps encoder_k detached and updated via EMA
#   ✓ Uses AMP for speed + stability
#   ✓ Prevents covariance explosion
# ================================================================

print("Starting FINAL MoCo-Cov training...\n")

loss_fn = MoCoCovLoss(
    temperature=config["tau"],
    lambda_cov=config["lambda_cov"],
    use_queue_for_cov=False
).to(device)

num_epochs = config["epochs"]
log_every = 100
m = config["m"]  # EMA coefficient

scaler = torch.cuda.amp.GradScaler()

for epoch in range(num_epochs):

    model.train()
    epoch_loss = 0.0
    epoch_contrast = 0.0
    epoch_cov = 0.0

    print(f"\n==== Epoch {epoch+1}/{num_epochs} ====")

    for step, (x1, x2) in enumerate(ssl_loader):

        x1 = x1.to(device, non_blocking=True)
        x2 = x2.to(device, non_blocking=True)

        optimizer.zero_grad(set_to_none=True)

        # ---------------------------------------------------
        # FORWARD PASS (CRITICAL FIX: DO NOT NORMALIZE HERE)
        # ---------------------------------------------------
        with torch.cuda.amp.autocast():

            # Raw embeddings from projection head
            q_raw = model.encoder_q(x1)

            with torch.no_grad():
                k_raw = model.encoder_k(x2)

            # Compute MoCo + Covariance loss using RAW embeddings
            loss, loss_contrast, cov_loss_raw = loss_fn(
                q_raw,                      # raw q
                k_raw,                      # raw k
                queue.queue.clone().detach().T
            )

        # ---------------------------------------------------
        # BACKPROP (AMP)
        # ---------------------------------------------------
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # ---------------------------------------------------
        # EMA UPDATE (momentum encoder)
        # ---------------------------------------------------
        with torch.no_grad():
            for param_q, param_k in zip(
                model.encoder_q.parameters(),
                model.encoder_k.parameters()
            ):
                param_k.data = param_k.data * m + param_q.data * (1 - m)

        # ---------------------------------------------------
        # QUEUE UPDATE (normalize HERE ONLY — correct MoCo behavior)
        # ---------------------------------------------------
        k_norm = F.normalize(k_raw, dim=1)
        queue.enqueue(k_norm)

        # ---------------------------------------------------
        # METRICS
        # ---------------------------------------------------
        epoch_loss += loss.item()
        epoch_contrast += loss_contrast.item()
        epoch_cov += (config["lambda_cov"] * cov_loss_raw).item()

        # ---------------------------------------------------
        # LOGGING
        # ---------------------------------------------------
        print(
            f"\n>>> Epoch {epoch+1} Summary: "
            f"AvgLoss={epoch_loss/len(ssl_loader):.4f}, "
            f"AvgContrast={epoch_contrast/len(ssl_loader):.4f}, "
            f"AvgCov={epoch_cov/len(ssl_loader):.4f}"
        )

    # ---------------------------------------------------
    # END OF EPOCH SUMMARY
    # ---------------------------------------------------
    print("\n>>> Epoch {} Summary: AvgLoss={:.4f}, AvgContrast={:.4f}, AvgCov={:.4f}".format(
    epoch+1,
    epoch_loss/len(ssl_loader),
    epoch_contrast/len(ssl_loader),
    epoch_cov/len(ssl_loader)
    ))
    scheduler.step()

    # ---------------------------------------------------
    # SAVE CHECKPOINT
    # ---------------------------------------------------
    save_path = f"/home/ubuntu/checkpoints/epoch_{epoch+1}.pth"
    torch.save(
        {
            "epoch": epoch+1,
            "state_dict": model.state_dict(),
            "optimizer": optimizer.state_dict(),
            "scheduler": scheduler.state_dict(),
            "queue": queue.queue.clone(),
        },
        save_path
    )

    print(f"Checkpoint saved to {save_path}")

print("\n=== FINAL SSL TRAINING COMPLETE ===")

"""## STEP 9. Preparing CIFAR-100 for Downstream Evaluation  

This block loads the CIFAR-100 dataset and prepares it for downstream evaluation of our SSL encoder. It applies simple, non-contrastive transforms (resize + normalize), then builds train/test DataLoaders.
"""

import torchvision
import torchvision.transforms as T
from torch.utils.data import DataLoader

# Basic eval transforms (no strong augmentations)
cifar_transform = T.Compose([
    T.Resize((96, 96)),     # match our backbone resolution
    T.ToTensor(),
    T.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225)
    ),
])

# CIFAR-100 path for Lambda
cifar100_root = "/home/ubuntu/cifar100"   # <-- FIXED

# Load CIFAR-100 train/test
cifar_train = torchvision.datasets.CIFAR100(
    root=cifar100_root,
    train=True,
    download=True,
    transform=cifar_transform
)

cifar_test = torchvision.datasets.CIFAR100(
    root=cifar100_root,
    train=False,
    download=True,
    transform=cifar_transform
)

# Dataloaders
cifar_train_loader = DataLoader(
    cifar_train,
    batch_size=256,
    shuffle=False,
    num_workers=8,
    pin_memory=True
)

cifar_test_loader = DataLoader(
    cifar_test,
    batch_size=256,
    shuffle=False,
    num_workers=8,
    pin_memory=True
)

print("CIFAR-100 loaded successfully!")
print("Train samples:", len(cifar_train))
print("Test samples:", len(cifar_test))

"""## STEP 10: Extracting Frozen Features for Downstream Evaluation


This block defines a helper function that runs our pretrained encoder_q over an evaluation dataset (like CIFAR-100) to extract 128-dim feature vectors for every image. It keeps the encoder frozen, normalizes the embeddings, and returns:

one big tensor of all features

one big tensor of their labels

These extracted features are later used for downstream tasks such as k-NN classification or a linear probe.
"""

import torch
import torch.nn.functional as F

@torch.no_grad()
def extract_features(encoder_q, dataloader, device):
    """
    Extract frozen features from encoder_q.
    Returns:
        features: Tensor of shape [N, 128]
        labels:   Tensor of shape [N]
    """
    encoder_q.eval()
    features_list = []
    labels_list = []

    for images, labels in dataloader:
        images = images.to(device, non_blocking=True)

        # Forward pass through frozen encoder
        feats = encoder_q(images)

        # Normalize (safe)
        feats = F.normalize(feats, dim=1)

        features_list.append(feats.cpu())
        labels_list.append(labels.cpu())

    # Concatenate all batches into big tensors
    all_features = torch.cat(features_list, dim=0)
    all_labels = torch.cat(labels_list, dim=0)

    print("Finished extracting features.")
    print("Feature tensor shape:", all_features.shape)
    print("Labels tensor shape:", all_labels.shape)

    return all_features, all_labels

train_feats, train_labels = extract_features(
    model.encoder_q,
    cifar_train_loader,
    device
)

test_feats, test_labels = extract_features(
    model.encoder_q,
    cifar_test_loader,
    device
)

"""## STEP 11: k-NN Classifier for Evaluating SSL Representations


This block implements a k-nearest-neighbors classifier used to evaluate how good our SSL features are. It compares test features to all training features using cosine similarity, finds each sample’s top-k neighbors, weights their votes by similarity, and predicts the class.
"""

import torch
import torch.nn.functional as F

def knn_classifier(train_feats, train_labels, test_feats, test_labels, k=16, temperature=0.07, device=None):
    """
    Improved k-NN classifier:
      - uses GPU for similarity (if available)
      - avoids Python loops for class score accumulation
      - much faster and scalable
    """

    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Normalize
    train_feats = F.normalize(train_feats, dim=1).to(device)
    test_feats = F.normalize(test_feats, dim=1).to(device)

    # labels stay on CPU (safer, easier)
    train_labels = train_labels.cpu()
    test_labels = test_labels.cpu()

    num_test = test_feats.size(0)
    num_train = train_feats.size(0)
    num_classes = train_labels.max().item() + 1

    batch_size = 400   # small enough for 4090/A100/V100 memory

    correct = 0

    print("Running improved k-NN evaluation...")

    for i in range(0, num_test, batch_size):
        end = min(i + batch_size, num_test)
        batch = test_feats[i:end]  # [B, D]

        # ---------------------------
        # 1. Cosine similarity on GPU
        # ---------------------------
        sim = torch.mm(batch, train_feats.t())     # [B, N_train]

        # top-k neighbors
        sim_val, sim_idx = sim.topk(k=k, dim=1)    # [B, k]

        # bring labels for neighbors
        neighbor_labels = train_labels[sim_idx.cpu()]  # [B, k]

        # softmax weights on GPU
        weights = torch.exp(sim_val / temperature)  # [B, k]

        # ---------------------------
        # 2. FAST scoring:
        #    convert labels → one-hot, weighted sum
        # ---------------------------
        # one-hot encode neighbor labels
        one_hot = torch.zeros(end - i, k, num_classes)
        one_hot.scatter_(2, neighbor_labels.unsqueeze(2), 1)

        # weight by similarity × temperature
        # (sim weights are still on GPU)
        weighted = one_hot.to(device) * weights.unsqueeze(2)

        # sum over k neighbors → final class scores
        class_scores = weighted.sum(dim=1)   # [B, C]

        # get predictions
        preds = class_scores.argmax(dim=1).cpu()

        # count correct predictions
        correct += (preds == test_labels[i:end]).sum().item()

    acc = 100.0 * correct / num_test
    print(f"Improved k-NN accuracy (k={k}): {acc:.2f}%")

    return acc

"""Running the KNN"""

knn_acc = knn_classifier(
    train_feats, train_labels,
    test_feats, test_labels,
    k=16,
    temperature=0.07,
    device=device        # <--- REQUIRED
)

"""## STEP 12: Linear Probe Classifier for Evaluating SSL Features


This block trains a simple linear classifier on top of frozen SSL features to measure how well the MoCo encoder supports supervised tasks. It:

- Wraps the features/labels into DataLoaders.
- Trains a single linear layer (no nonlinearities) for a small number of epochs.
- Evaluates accuracy on a test set after each epoch.
- Returns the final accuracy.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader

def linear_probe(
    train_feats, train_labels,
    test_feats, test_labels,
    num_epochs=22,
    batch_size=256,
    lr=0.01,
    device=None
):

    # ---- Safe device handling ----
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ---- Dataset ----
    train_dataset = TensorDataset(train_feats, train_labels)
    test_dataset = TensorDataset(test_feats, test_labels)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # ---- Determine number of classes ----
    num_classes = train_labels.max().item() + 1

    # ---- Linear classifier on frozen features ----
    classifier = nn.Linear(train_feats.size(1), num_classes).to(device)

    # ---- Optimizer (Adam is fine) ----
    optimizer = torch.optim.Adam(
        classifier.parameters(),
        lr=lr,
        weight_decay=1e-4
    )

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs
    )

    criterion = nn.CrossEntropyLoss()

    # ---- Training loop ----
    for epoch in range(num_epochs):
        classifier.train()
        total_loss = 0.0

        for feats, labels in train_loader:
            feats = feats.to(device)
            labels = labels.to(device)

            optimizer.zero_grad(set_to_none=True)
            logits = classifier(feats)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # ---- Evaluation ----
        classifier.eval()
        correct, total = 0, 0

        with torch.no_grad():
            for feats, labels in test_loader:
                feats = feats.to(device)
                labels = labels.to(device)
                logits = classifier(feats)
                preds = logits.argmax(dim=1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        acc = 100.0 * correct / total
        print(f"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | Test Acc: {acc:.2f}%")

        scheduler.step()

    return acc

"""Running linear probe"""

lp_acc = linear_probe(
    train_feats, train_labels,
    test_feats, test_labels,
    num_epochs=22,
    batch_size=256,
    lr=0.01,
    device=device       # <-- REQUIRED
)

def load_frozen_encoder_resnet50(ckpt_path, device):
    """
    Loads a checkpoint and returns ONLY the frozen encoder_q
    """
    print(f"Loading frozen encoder from: {ckpt_path}")

    model = MoCoResNet50(dim_feature=128).to(device)
    ckpt = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(ckpt["state_dict"], strict=True)

    encoder = model.encoder_q
    for p in encoder.parameters():
        p.requires_grad = False
    encoder.eval()
    return encoder

"""## Select the best checkpoint"""

# ============================================================
# ===  AFTER TRAINING: AUTOMATICALLY PICK BEST CHECKPOINT  ===
# ============================================================

def evaluate_checkpoint(path):
    print(f"\nLoading checkpoint: {path}")
    encoder = load_frozen_encoder_resnet50(path, device)

    # Extract frozen CIFAR features
    train_feats, train_labels = extract_features(
        encoder, cifar_train_loader, device
    )
    test_feats, test_labels = extract_features(
        encoder, cifar_test_loader, device
    )

    # Run k-NN
    acc = knn_classifier(
        train_feats, train_labels,
        test_feats, test_labels,
        k=16, temperature=0.07, device=device
    )
    return acc


print("\n============================================")
print("   Evaluating all checkpoints to find best   ")
print("============================================")

best_acc = -1
best_epoch = -1

num_epochs = config["epochs"]
for epoch in range(1, num_epochs + 1):   # <-- update if you train more epochs
    ckpt_path = f"/home/ubuntu/checkpoints/epoch_{epoch}.pth"

    if not os.path.exists(ckpt_path):
        print(f"Checkpoint not found: {ckpt_path}")
        continue

    acc = evaluate_checkpoint(ckpt_path)

    if acc > best_acc:
        best_acc = acc
        best_epoch = epoch

print("\n==============================")
print(f"BEST CHECKPOINT = epoch {best_epoch}")
print(f"BEST k-NN ACC   = {best_acc:.2f}%")
print("==============================")

"""## Public dataset evaluation"""

# ================================================================
#       FINAL SUN397 EVALUATION PIPELINE (FROZEN ENCODER)
#  After best checkpoint has been identified as `best_epoch`
# ================================================================

import os
import pandas as pd
from torch.utils.data import Dataset
from PIL import Image

print("\n====================================================")
print("        STARTING SUN397 EVALUATION PIPELINE")
print("====================================================\n")

# ------------------------------------------------------------
# Add argparse for final_epochs
# ------------------------------------------------------------
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--final_epochs", type=int, default=30,
                    help="Number of epochs for final classifier training")
args, _ = parser.parse_known_args()
final_epochs = args.final_epochs


# ------------------------------------------------------------
# 1. Load BEST CHECKPOINT (frozen encoder)
# ------------------------------------------------------------

BEST_EPOCH = best_epoch
BEST_PATH = f"/home/ubuntu/checkpoints/epoch_{BEST_EPOCH}.pth"

print(f"Loading BEST checkpoint from: {BEST_PATH}")
encoder = load_frozen_encoder_resnet50(BEST_PATH, device)


# ------------------------------------------------------------
# 2. SUN397 Dataset (CSV-based)
# ------------------------------------------------------------

class SUN397Dataset(Dataset):
    def __init__(self, df, root, transform):
        self.df = df
        self.root = root
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = os.path.join(self.root, row["filename"])
        if not os.path.exists(img_path):
            print("MISSING IMAGE:", img_path)

        img = Image.open(img_path).convert("RGB")
        img = self.transform(img)

        if "class_id" in row:
            label = int(row["class_id"])
        else:
            label = -1

        return img, label


# ------------------------------------------------------------
# 3. Load SUN397 CSV metadata
# ------------------------------------------------------------

SUN397_DIR = "/home/ubuntu/kaggle_data_sun397"

train_csv = pd.read_csv(f"{SUN397_DIR}/train_labels.csv")
val_csv   = pd.read_csv(f"{SUN397_DIR}/val_labels.csv")
test_csv  = pd.read_csv(f"{SUN397_DIR}/test_images.csv")

print("Loaded SUN397 CSV metadata:")
print("Train:", len(train_csv))
print("Val:", len(val_csv))
print("Test:", len(test_csv))


# ------------------------------------------------------------
# 4. Build datasets + dataloaders
# ------------------------------------------------------------

train_dataset = SUN397Dataset(train_csv, f"{SUN397_DIR}/train", cifar_transform)
val_dataset   = SUN397Dataset(val_csv,   f"{SUN397_DIR}/val",   cifar_transform)
test_dataset  = SUN397Dataset(test_csv,  f"{SUN397_DIR}/test",  cifar_transform)

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False)
val_loader   = DataLoader(val_dataset,   batch_size=256, shuffle=False)
test_loader  = DataLoader(test_dataset,  batch_size=256, shuffle=False)


# ------------------------------------------------------------
# 5. Extract Frozen Features
# ------------------------------------------------------------

print("\nExtracting frozen features for SUN397 train/val/test...")

train_feats, train_labels = extract_features(encoder, train_loader, device)
val_feats,   val_labels   = extract_features(encoder, val_loader,   device)
test_feats, _             = extract_features(encoder, test_loader,  device)


# ------------------------------------------------------------
# 6. Linear Probe: Train on train → tune on val
# ------------------------------------------------------------

print("\nTraining linear probe on SUN397 (frozen encoder)...")

sun397_acc = linear_probe(
    train_feats, train_labels,
    val_feats,   val_labels,
    num_epochs=22,
    batch_size=256,
    lr=0.01,
    device=device
)

print(f"\nSUN397 Validation Accuracy: {sun397_acc:.2f}%\n")


# ------------------------------------------------------------
# 7. FINAL CLASSIFIER (train+val) — controlled by --final_epochs
# ------------------------------------------------------------

print("Training final classifier on (train + val)...")

all_feats  = torch.cat([train_feats, val_feats], dim=0)
all_labels = torch.cat([train_labels, val_labels], dim=0)

num_classes = all_labels.max().item() + 1

print("DEBUG LABEL CHECK:")
print("  Min label:", all_labels.min().item())
print("  Max label:", all_labels.max().item())
print("  Num classes:", num_classes)

final_classifier = nn.Linear(all_feats.size(1), num_classes).to(device)
optimizer = torch.optim.Adam(final_classifier.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

for epoch in range(final_epochs):
    final_classifier.train()
    optimizer.zero_grad()

    logits = final_classifier(all_feats.to(device))
    loss = criterion(logits, all_labels.to(device))
    loss.backward()
    optimizer.step()

    print(f"Final classifier epoch {epoch+1}/{final_epochs} | Loss: {loss.item():.4f}")


# ------------------------------------------------------------
# 8. Predict on SUN397 test set
# ------------------------------------------------------------

print("\nPredicting SUN397 test classes...")
final_classifier.eval()

test_logits = final_classifier(test_feats.to(device))
test_preds = test_logits.argmax(dim=1).cpu().numpy()


# ------------------------------------------------------------
# 9. Save submission.csv
# ------------------------------------------------------------

submission = pd.DataFrame({
    "id": test_csv["filename"],
    "label": test_preds
})

submission.to_csv("submission.csv", index=False)

print("\n====================================================")
print("SUN397 evaluation complete!")
print("Saved submission file: submission.csv")
print("====================================================\n")
