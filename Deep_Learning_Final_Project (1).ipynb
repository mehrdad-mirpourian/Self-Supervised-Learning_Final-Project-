{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehOyrZhNs4Bt"
      },
      "source": [
        "## Step 1.a: Import all required libraries and set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfyPCyHddzOf",
        "outputId": "c280a9fd-2ed3-4128-d1ef-52e44fde632c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Environment setup complete.\n"
          ]
        }
      ],
      "source": [
        "# PyTorch core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Torchvision (datasets, transforms, ResNet backbone)\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Utility libraries\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "# ================================================================\n",
        "# Check GPU availability\n",
        "# ================================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ================================================================\n",
        "# Set random seeds for reproducibility\n",
        "# ================================================================\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Ensures deterministic behavior (may slow down training slightly)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"Environment setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POB4xHzptXnF"
      },
      "source": [
        "## STEP 2 ‚Äî Define MoCo-v2 Style Data Augmentations\n",
        "\n",
        "MoCo-style self-supervised learning relies heavily on strong augmentations to create two different ‚Äúviews‚Äù of the same image.\n",
        "This block creates the standard MoCo-v2 augmentation pipeline we‚Äôll use during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaO9rOedtgCj",
        "outputId": "721df7df-cd26-40c3-ca94-8003a37be633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoCo-v2 augmentations initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# These augmentations generate *two strongly augmented views* of each image.\n",
        "# They are essential for contrastive learning because the model must learn\n",
        "# invariances to color, crop, blur, distortion, etc.\n",
        "\n",
        "class MoCoTransform:\n",
        "    \"\"\"Applies two strong augmentations to the same image.\"\"\"\n",
        "\n",
        "    def __init__(self, image_size=96):\n",
        "        # MoCo-v2 Augmentation pipeline:\n",
        "        self.base_transform = T.Compose([\n",
        "            # 1. Random resized crop (strong spatial diversity)\n",
        "            T.RandomResizedCrop(image_size, scale=(0.2, 1.0)),\n",
        "\n",
        "            # 2. Random horizontal flip\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "            # 3. Color jitter (brightness, contrast, saturation, hue)\n",
        "            T.RandomApply(\n",
        "                [T.ColorJitter(0.4, 0.4, 0.4, 0.1)],\n",
        "                p=0.8\n",
        "            ),\n",
        "\n",
        "            # 4. Random grayscale conversion\n",
        "            T.RandomGrayscale(p=0.2),\n",
        "\n",
        "            # Mild solarization (improves invariance)\n",
        "            T.RandomApply(\n",
        "                [T.RandomSolarize(128)],\n",
        "                p=0.2\n",
        "            ),\n",
        "\n",
        "            # 5. Gaussian blur (MoCo v2 uses it heavily)\n",
        "            T.RandomApply(\n",
        "                [T.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0))],\n",
        "                p=0.5\n",
        "            ),\n",
        "\n",
        "            # 6. Convert PIL ‚Üí Tensor\n",
        "            T.ToTensor(),\n",
        "\n",
        "            # 7. Normalize to standard ImageNet stats\n",
        "            T.Normalize(\n",
        "                mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Return TWO differently augmented views of the same image\n",
        "        return self.base_transform(x), self.base_transform(x)\n",
        "\n",
        "\n",
        "print(\"MoCo-v2 augmentations initialized successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhM2dZMFt3ab"
      },
      "source": [
        "## STEP 3 ‚Äî Build the Unlabeled Dataset Loader (Self-Supervised Dataset)\n",
        "\n",
        "This dataset:\n",
        "- Uses the MoCoTransform we defined\n",
        "- Returns two augmented views per image\n",
        "- Works for any folder of unlabeled images\n",
        "- Creates a PyTorch DataLoader ready for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r3M0OfevUdW"
      },
      "source": [
        "# Mehi delete this block later on. This is just for our tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atfwWy2EupZ-",
        "outputId": "419337d6-3711-4377-dd9a-5cef2bc7bdfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dummy dataset created.\n"
          ]
        }
      ],
      "source": [
        "# # ================================================================\n",
        "# # TEMPORARY FIX: Create a dummy dataset for debugging the pipeline\n",
        "# # ================================================================\n",
        "\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# # Create a small dummy directory\n",
        "# dummy_root = \"/content/unlabeled_dataset/\"\n",
        "# os.makedirs(dummy_root, exist_ok=True)\n",
        "\n",
        "# # Create 20 small random images for testing the DataLoader\n",
        "# for i in range(20):\n",
        "#     img = (np.random.rand(96, 96, 3) * 255).astype(np.uint8)\n",
        "#     Image.fromarray(img).save(f\"{dummy_root}/dummy_{i}.jpg\")\n",
        "\n",
        "# print(\"Dummy dataset created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI4e72Brtg8z",
        "outputId": "ce83753a-5f50-4b7c-fffa-11f64e79b291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 20 unlabeled images from /content/unlabeled_dataset/\n",
            "SSL DataLoader is ready.\n"
          ]
        }
      ],
      "source": [
        "# # We assume the dataset directory contains raw images organized like:\n",
        "# # root/\n",
        "# #    img1.jpg\n",
        "# #    img2.jpg\n",
        "# #    img3.jpg\n",
        "# #    ...\n",
        "# #\n",
        "# # There are NO labels in SSL pretraining.\n",
        "\n",
        "# class UnlabeledImageDataset(torch.utils.data.Dataset):\n",
        "#     \"\"\"Dataset that returns two augmented views for self-supervised learning.\"\"\"\n",
        "\n",
        "#     def __init__(self, root_dir, transform):\n",
        "#         \"\"\"\n",
        "#         root_dir: path to the directory with unlabeled images\n",
        "#         transform: MoCoTransform (returns two views)\n",
        "#         \"\"\"\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transform\n",
        "#         self.files = []\n",
        "\n",
        "#         # Collect all image file paths\n",
        "#         for ext in [\"png\", \"jpg\", \"jpeg\"]:\n",
        "#             self.files.extend(list(Path(root_dir).glob(f\"**/*.{ext}\")))\n",
        "\n",
        "#         print(f\"Loaded {len(self.files)} unlabeled images from {root_dir}\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.files)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Load image\n",
        "#         img_path = self.files[idx]\n",
        "#         image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "#         # Return two augmented views\n",
        "#         x1, x2 = self.transform(image)\n",
        "#         return x1, x2\n",
        "\n",
        "\n",
        "# # ================================================================\n",
        "# # Example: create the dataset and dataloader\n",
        "# # ================================================================\n",
        "# from pathlib import Path\n",
        "# from PIL import Image\n",
        "\n",
        "# # UPDATE THIS PATH to your unlabeled dataset in Colab:\n",
        "# unlabeled_root = \"/content/unlabeled_dataset/\"  # <-- I will change this later\n",
        "\n",
        "# # Instantiate dataset with the MoCoTransform\n",
        "# ssl_transform = MoCoTransform(image_size=96)\n",
        "# ssl_dataset = UnlabeledImageDataset(unlabeled_root, ssl_transform)\n",
        "\n",
        "# # Create dataloader (this will be used in training)\n",
        "# ssl_loader = torch.utils.data.DataLoader(\n",
        "#     ssl_dataset,\n",
        "#     batch_size=4,           # adjust based on GPU memory\n",
        "#     shuffle=True,\n",
        "#     num_workers=4,\n",
        "#     drop_last=True\n",
        "# )\n",
        "\n",
        "# print(\"SSL DataLoader is ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEKVuqYvfyp"
      },
      "source": [
        "## STEP 4 ‚Äî Build MoCo Encoders (ResNet-50 + Projection Head + Momentum Copy)\n",
        "\n",
        "This step creates:\n",
        "- ResNet-50 backbone (without its classification head)\n",
        "- Projection head (MLP) used by MoCo-v2\n",
        "- MoCoEncoder module that combines:\n",
        "- encoder_q (trainable)\n",
        "- encoder_k (momentum encoder)\n",
        "- projection head for each\n",
        "\n",
        "This is the backbone of our SSL method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3P5NUbhtg_b",
        "outputId": "10d1453a-3cf8-4dc2-c41c-ee74d888bd5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoCo encoders (ResNet-50 + projection heads) defined successfully.\n"
          ]
        }
      ],
      "source": [
        "# # We will create:\n",
        "# # - encoder_q: the main, trainable backbone\n",
        "# # - encoder_k: the EMA/momentum backbone\n",
        "# # - projection head: a 2-layer MLP as used in MoCo-v2\n",
        "\n",
        "# # IMPORTANT:\n",
        "# # We remove the classification head of ResNet-50 and keep only the\n",
        "# # convolutional feature extractor.\n",
        "\n",
        "\n",
        "# class ProjectionHead(nn.Module):\n",
        "#     \"\"\"\n",
        "#     2-layer MLP projection head used in MoCo-v2.\n",
        "#     It maps high-dimensional ResNet features into a smaller embedding space.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, in_dim=2048, hidden_dim=2048, out_dim=128):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Applies a simple 2-layer MLP: FC -> ReLU -> FC\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# def build_resnet50_backbone():\n",
        "#     \"\"\"\n",
        "#     Loads a ResNet-50 backbone WITHOUT the classification head.\n",
        "#     This is the standard backbone for MoCo-v2.\n",
        "#     \"\"\"\n",
        "#     backbone = resnet50(weights=None)  # randomly initialized\n",
        "#     backbone.fc = nn.Identity()        # remove classification layer\n",
        "#     return backbone\n",
        "\n",
        "\n",
        "# class MoCoEncoder(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Encapsulates:\n",
        "#     - encoder_q (trainable)\n",
        "#     - encoder_k (momentum/EMA)\n",
        "#     - projection heads for both encoders\n",
        "#     \"\"\"\n",
        "#     def __init__(self, feature_dim=128, m=0.999):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # Momentum coefficient\n",
        "#         self.m = m\n",
        "\n",
        "#         # ------------------------------------------------------------\n",
        "#         # 1. Build query encoder (trainable)\n",
        "#         # ------------------------------------------------------------\n",
        "#         self.encoder_q = build_resnet50_backbone()\n",
        "#         self.proj_q = ProjectionHead(in_dim=2048, out_dim=feature_dim)\n",
        "\n",
        "#         # ------------------------------------------------------------\n",
        "#         # 2. Build key encoder (momentum version)\n",
        "#         # ------------------------------------------------------------\n",
        "#         self.encoder_k = build_resnet50_backbone()\n",
        "#         self.proj_k = ProjectionHead(in_dim=2048, out_dim=feature_dim)\n",
        "\n",
        "#         # Key encoder should NOT update by gradient descent\n",
        "#         for param in self.encoder_k.parameters():\n",
        "#             param.requires_grad = False\n",
        "#         for param in self.proj_k.parameters():\n",
        "#             param.requires_grad = False\n",
        "\n",
        "#         # ------------------------------------------------------------\n",
        "#         # Initialize encoder_k with encoder_q weights\n",
        "#         # ------------------------------------------------------------\n",
        "#         self._momentum_update_key_encoder(initial=True)\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def _momentum_update_key_encoder(self, initial=False):\n",
        "#         \"\"\"\n",
        "#         Updates encoder_k parameters using exponential moving average (EMA).\n",
        "#         Called every training step.\n",
        "#         \"\"\"\n",
        "#         for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "#             if initial:\n",
        "#                 # At initialization, copy weights directly\n",
        "#                 param_k.data.copy_(param_q.data)\n",
        "#             else:\n",
        "#                 # Momentum update: k = m * k + (1-m) * q\n",
        "#                 param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)\n",
        "\n",
        "#         for param_q, param_k in zip(self.proj_q.parameters(), self.proj_k.parameters()):\n",
        "#             if initial:\n",
        "#                 param_k.data.copy_(param_q.data)\n",
        "#             else:\n",
        "#                 param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)\n",
        "\n",
        "#     def forward(self, x_q, x_k):\n",
        "#         \"\"\"\n",
        "#         Runs the query encoder and (optionally) the key encoder.\n",
        "#         Returns:\n",
        "#         - q: normalized query embedding\n",
        "#         - k: normalized key embedding (no grad)\n",
        "#         \"\"\"\n",
        "#         # Query branch (gradients flow here)\n",
        "#         q = self.encoder_q(x_q)\n",
        "#         q = self.proj_q(q)\n",
        "#         q = F.normalize(q, dim=1)\n",
        "\n",
        "#         # Key branch (momentum encoder, no grad)\n",
        "#         with torch.no_grad():\n",
        "#             k = self.encoder_k(x_k)\n",
        "#             k = self.proj_k(k)\n",
        "#             k = F.normalize(k, dim=1)\n",
        "\n",
        "#         return q, k\n",
        "\n",
        "\n",
        "# print(\"MoCo encoders (ResNet-50 + projection heads) defined successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPSqIWGsx5_8"
      },
      "source": [
        "## STEP 5 ‚Äî Implement the MoCo Negative Queue (Memory Bank)\n",
        "\n",
        "The queue stores thousands of negative keys used for contrastive learning.\n",
        "MoCo depends heavily on this queue to approximate a large batch size.\n",
        "\n",
        "We need:\n",
        "- a fixed-size FIFO queue\n",
        "- enqueue new keys (from each batch)\n",
        "- dequeue oldest entries\n",
        "- ensure keys are normalized\n",
        "- prevent gradients from flowing through queue values\n",
        "\n",
        "This class will be small but critical for stable training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2lM66githBw",
        "outputId": "59672ac3-91fc-47cd-f594-dab29eecc947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoCo negative queue initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# # This queue stores negative embeddings for the contrastive loss.\n",
        "\n",
        "# # Key ideas:\n",
        "# # - It is a fixed-size FIFO queue.\n",
        "# # - Each new batch of keys is enqueued.\n",
        "# # - The oldest entries are removed to keep size fixed.\n",
        "# # - Queue embeddings NEVER require gradients (detach safely).\n",
        "\n",
        "# class MoCoQueue:\n",
        "#     def __init__(self, feature_dim=128, queue_size=65536):\n",
        "#         \"\"\"\n",
        "#         feature_dim: dimensionality of embedding (default 128)\n",
        "#         queue_size: total number of negative keys stored\n",
        "#         \"\"\"\n",
        "#         self.queue_size = queue_size\n",
        "\n",
        "#         # Initialize queue as a (feature_dim x queue_size) tensor\n",
        "#         self.queue = torch.randn(feature_dim, queue_size)\n",
        "#         self.queue = F.normalize(self.queue, dim=0)  # normalize each key\n",
        "#         self.queue_ptr = 0  # pointer to the oldest entry\n",
        "\n",
        "#         # Queue does NOT require gradients\n",
        "#         self.queue.requires_grad = False\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def enqueue_dequeue(self, keys):\n",
        "#         \"\"\"\n",
        "#         keys: tensor of shape (batch_size, feature_dim)\n",
        "#         Inserts new keys and removes old ones in FIFO fashion.\n",
        "#         \"\"\"\n",
        "#         batch_size = keys.shape[0]\n",
        "\n",
        "#         # If batch is larger than queue size (rare), we trim\n",
        "#         if batch_size > self.queue_size:\n",
        "#             keys = keys[:self.queue_size]\n",
        "\n",
        "#         # Compute positions in the queue to replace\n",
        "#         end_ptr = (self.queue_ptr + batch_size) % self.queue_size\n",
        "\n",
        "#         if end_ptr > self.queue_ptr:\n",
        "#             # Simple case: no wrap-around\n",
        "#             self.queue[:, self.queue_ptr:end_ptr] = keys.T\n",
        "#         else:\n",
        "#             # Wrap-around case\n",
        "#             first_segment = self.queue_size - self.queue_ptr\n",
        "#             self.queue[:, self.queue_ptr:] = keys[:first_segment].T\n",
        "#             self.queue[:, :end_ptr] = keys[first_segment:].T\n",
        "\n",
        "#         # Update pointer\n",
        "#         self.queue_ptr = end_ptr\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def get_negatives(self):\n",
        "#         \"\"\"\n",
        "#         Returns all negative keys of shape (feature_dim, queue_size),\n",
        "#         with no gradients.\n",
        "#         \"\"\"\n",
        "#         return self.queue.clone().detach()\n",
        "\n",
        "\n",
        "# print(\"MoCo negative queue initialized successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NogvyTHhynxe"
      },
      "source": [
        "## STEP 6 ‚Äî Implement the Contrastive Loss (InfoNCE) + Covariance Regularization Loss\n",
        "\n",
        "This is the heart of our method.We will build:\n",
        "- InfoNCE loss for MoCo\n",
        "- Covariance regularization loss (VICReg-style)\n",
        "- A clean loss module we can plug directly into the training loop\n",
        "\n",
        "Our code supports both:\n",
        "- Covariance loss on (q, k+) only\n",
        "- Covariance loss on (q, k+, queue samples)\n",
        "Controlled by a flag: use_queue_for_cov = True/False\n",
        "\n",
        "This makes our method flexible, cleaner, and perfect for ablation studies in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ftI9JuethEL",
        "outputId": "c6d3cd8b-ff35-47c2-8376-6f91f9c60e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoCo InfoNCE + covariance loss module ready.\n"
          ]
        }
      ],
      "source": [
        "# This includes two main components:\n",
        "# 1. MoCo InfoNCE contrastive loss.\n",
        "# 2. VICReg-style covariance loss to decorrelate embedding dimensions.\n",
        "\n",
        "# We support:\n",
        "#   - Using only (q, k+) for covariance loss\n",
        "#   - OR including samples from the negative queue (optional)\n",
        "# via the flag `use_queue_for_cov`.\n",
        "\n",
        "\n",
        "class MoCoCovLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.2, lambda_cov=1.0, use_queue_for_cov=False):\n",
        "        \"\"\"\n",
        "        temperature: contrastive temperature parameter (tau)\n",
        "        lambda_cov: weight for covariance regularization\n",
        "        use_queue_for_cov: whether to include queue samples in covariance loss\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.tau = temperature\n",
        "        self.lambda_cov = lambda_cov\n",
        "        self.use_queue_for_cov = use_queue_for_cov\n",
        "\n",
        "    def forward(self, q, k_pos, queue_neg):\n",
        "        \"\"\"\n",
        "        q: (B, D) query embeddings\n",
        "        k_pos: (B, D) positive key embeddings\n",
        "        queue_neg: (D, K) negative keys from FIFO queue\n",
        "        \"\"\"\n",
        "        # ------------------------------------------------------------\n",
        "        # 1. Compute InfoNCE contrastive loss\n",
        "        # ------------------------------------------------------------\n",
        "        # Positive logits: q ¬∑ k+\n",
        "        pos_logits = torch.sum(q * k_pos, dim=1, keepdim=True)  # (B, 1)\n",
        "\n",
        "        # Negative logits: q ¬∑ K\n",
        "        neg_logits = torch.einsum('nd,dk->nk', q, queue_neg)     # (B, K)\n",
        "\n",
        "        # Scale by temperature\n",
        "        pos_logits = pos_logits / self.tau\n",
        "        neg_logits = neg_logits / self.tau\n",
        "\n",
        "        # Concatenate pos + neg logits\n",
        "        logits = torch.cat([pos_logits, neg_logits], dim=1)      # (B, 1+K)\n",
        "\n",
        "        # Labels: positive is always index 0\n",
        "        labels = torch.zeros(q.size(0), dtype=torch.long, device=q.device)\n",
        "\n",
        "        # Cross-entropy loss for contrastive learning\n",
        "        loss_contrast = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 2. Compute VICReg covariance regularization loss\n",
        "        # ------------------------------------------------------------\n",
        "        # Build feature matrix Z = [q; k+] or optionally add queue samples\n",
        "        if self.use_queue_for_cov:\n",
        "            # Take a random subset of negatives from the queue\n",
        "            K = queue_neg.shape[1]\n",
        "            num_samples = min(1024, K)  # limit sample size\n",
        "            idx = torch.randperm(K, device=q.device)[:num_samples]\n",
        "            queue_subset = queue_neg[:, idx].T  # shape: (num_samples, D)\n",
        "\n",
        "            Z = torch.cat([q, k_pos, queue_subset], dim=0)  # (B + B + num_samples, D)\n",
        "        else:\n",
        "            Z = torch.cat([q, k_pos], dim=0)  # (2B, D)\n",
        "\n",
        "        # Compute covariance matrix C = Cov(Z)\n",
        "        Z = Z - Z.mean(dim=0, keepdim=True)      # center\n",
        "        C = (Z.T @ Z) / (Z.size(0) - 1)          # covariance: (D, D)\n",
        "\n",
        "        # Penalize off-diagonal elements of covariance\n",
        "        diag_mask = torch.eye(C.size(0), device=C.device).bool()\n",
        "        off_diag = C[~diag_mask]\n",
        "        loss_cov = (off_diag ** 2).sum()\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 3. Combine losses\n",
        "        # ------------------------------------------------------------\n",
        "        loss = loss_contrast + self.lambda_cov * loss_cov\n",
        "\n",
        "        return loss, loss_contrast, loss_cov\n",
        "\n",
        "\n",
        "print(\"MoCo InfoNCE + covariance loss module ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqha3kgC0F9g"
      },
      "source": [
        "## Step 7 ‚Äî Putting everything together: The Full Training Step (forward + loss + backward + momentum update + queue update).\n",
        "\n",
        "This is our most important engineering step in this entire project.\n",
        "\n",
        "We now have all components:\n",
        "- MoCo encoders (query + momentum key)\n",
        "- Negative queue\n",
        "- Contrastive + covariance loss\n",
        "\n",
        "In this step we want to wire all of these together into one training step.This is where everything comes together:\n",
        "- Forward pass\n",
        "- Compute q and k+\n",
        "- Compute loss\n",
        "- Backprop on encoder_q\n",
        "- Momentum update of encoder_k\n",
        "- Update the queue\n",
        "- Return losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9FvECQxthHh"
      },
      "outputs": [],
      "source": [
        "# # This function performs a *single iteration* of the training loop:\n",
        "# #   1) Compute q and k+ embeddings\n",
        "# #   2) Compute contrastive + covariance loss\n",
        "# #   3) Backprop only through encoder_q\n",
        "# #   4) Momentum-update encoder_k\n",
        "# #   5) Enqueue new k+ embeddings into the queue\n",
        "# # ================================================================\n",
        "\n",
        "# def training_step(model, queue, loss_fn, batch, optimizer):\n",
        "#     \"\"\"\n",
        "#     model: MoCoEncoder\n",
        "#     queue: MoCoQueue\n",
        "#     loss_fn: MoCoCovLoss\n",
        "#     batch: (x1, x2) two augmented views from dataloader\n",
        "#     optimizer: optimizer for encoder_q only\n",
        "#     \"\"\"\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 1. Move data to device\n",
        "#     # ------------------------------------------------------------\n",
        "#     x1, x2 = batch\n",
        "#     x1 = x1.to(device, non_blocking=True)\n",
        "#     x2 = x2.to(device, non_blocking=True)\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 2. Forward pass:\n",
        "#     #    - q = encoder_q(x1)\n",
        "#     #    - k = encoder_k(x2)\n",
        "#     # ------------------------------------------------------------\n",
        "#     q, k_pos = model(x1, x2)  # both are (B, D)\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 3. Get all negative keys from the queue\n",
        "#     # ------------------------------------------------------------\n",
        "#     queue_neg = queue.get_negatives()  # (D, K)\n",
        "#     queue_neg = queue_neg.to(device)\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 4. Compute total loss = contrastive + lambda * covariance\n",
        "#     # ------------------------------------------------------------\n",
        "#     loss, loss_contrast, loss_cov = loss_fn(q, k_pos, queue_neg)\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 5. Backward pass: update encoder_q parameters\n",
        "#     # ------------------------------------------------------------\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 6. Momentum update of encoder_k\n",
        "#     # ------------------------------------------------------------\n",
        "#     model._momentum_update_key_encoder(initial=False)\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 7. Update the negative queue using the detached k+\n",
        "#     # ------------------------------------------------------------\n",
        "#     queue.enqueue_dequeue(k_pos.detach())\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 8. Return losses for logging\n",
        "#     # ------------------------------------------------------------\n",
        "#     return loss.item(), loss_contrast.item(), loss_cov.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt5czBgL0r-e"
      },
      "source": [
        "## STEP 8 ‚Äî Full MoCo-Cov Training Loop\n",
        "\n",
        "Now we move to the final major coding step before actual training.\n",
        "This is where we build the full training loop that:\n",
        "- iterates over epochs\n",
        "- iterates over batches\n",
        "- uses your training_step()\n",
        "- logs the 3 losses\n",
        "- saves checkpoints (MoCo encoder + queue)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGteYbbq0EjG"
      },
      "outputs": [],
      "source": [
        "# # This loop:\n",
        "# #  - Runs for multiple epochs\n",
        "# #  - Iterates through all batches\n",
        "# #  - Calls training_step() each iteration\n",
        "# #  - Logs losses\n",
        "# #  - Saves periodic checkpoints\n",
        "# # ================================================================\n",
        "\n",
        "# def train_moco(\n",
        "#     model,\n",
        "#     queue,\n",
        "#     loss_fn,\n",
        "#     dataloader,\n",
        "#     optimizer,\n",
        "#     num_epochs=10,\n",
        "#     log_every=50,\n",
        "#     save_every=1,\n",
        "#     checkpoint_path=\"/content/moco_cov_checkpoint.pth\"\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     model: MoCoEncoder\n",
        "#     queue: MoCoQueue\n",
        "#     loss_fn: MoCoCovLoss\n",
        "#     dataloader: unlabeled dataset loader\n",
        "#     optimizer: optimizer for encoder_q\n",
        "#     num_epochs: total training epochs\n",
        "#     log_every: how often to print loss\n",
        "#     save_every: save checkpoint every N epochs\n",
        "#     checkpoint_path: file to save model+queue\n",
        "#     \"\"\"\n",
        "\n",
        "#     model.train()\n",
        "#     model.to(device)\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         epoch_loss = 0.0\n",
        "#         epoch_contrast = 0.0\n",
        "#         epoch_cov = 0.0\n",
        "\n",
        "#         pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "#         for step, batch in enumerate(pbar):\n",
        "\n",
        "#             # ------------------------------------------------------------\n",
        "#             # Single training step\n",
        "#             # ------------------------------------------------------------\n",
        "#             loss, loss_contrast, loss_cov = training_step(\n",
        "#                 model, queue, loss_fn, batch, optimizer\n",
        "#             )\n",
        "\n",
        "#             # ------------------------------------------------------------\n",
        "#             # Track running averages\n",
        "#             # ------------------------------------------------------------\n",
        "#             epoch_loss += loss\n",
        "#             epoch_contrast += loss_contrast\n",
        "#             epoch_cov += loss_cov\n",
        "\n",
        "#             # ------------------------------------------------------------\n",
        "#             # Logging\n",
        "#             # ------------------------------------------------------------\n",
        "#             if (step + 1) % log_every == 0:\n",
        "#                 pbar.set_postfix({\n",
        "#                     \"loss\": f\"{loss:.4f}\",\n",
        "#                     \"contrast\": f\"{loss_contrast:.4f}\",\n",
        "#                     \"cov\": f\"{loss_cov:.4f}\"\n",
        "#                 })\n",
        "\n",
        "#         # ------------------------------------------------------------\n",
        "#         # End of epoch logging\n",
        "#         # ------------------------------------------------------------\n",
        "#         print(f\"\\n[Epoch {epoch+1}] \"\n",
        "#               f\"Avg Loss: {epoch_loss/len(dataloader):.4f}, \"\n",
        "#               f\"Avg Contrast: {epoch_contrast/len(dataloader):.4f}, \"\n",
        "#               f\"Avg Cov: {epoch_cov/len(dataloader):.4f}\")\n",
        "\n",
        "#         # ------------------------------------------------------------\n",
        "#         # Save checkpoint periodically\n",
        "#         # ------------------------------------------------------------\n",
        "#         if (epoch + 1) % save_every == 0:\n",
        "#             save_state = {\n",
        "#                 \"epoch\": epoch + 1,\n",
        "#                 \"model_state\": model.state_dict(),\n",
        "#                 \"queue_state\": queue.queue,\n",
        "#                 \"optimizer_state\": optimizer.state_dict()\n",
        "#             }\n",
        "#             torch.save(save_state, checkpoint_path)\n",
        "#             print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "#     print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfCMAQCE1c7S"
      },
      "source": [
        "## STEP 9 ‚Äî MoCo-ResNet50 + Queue Code Block\n",
        "\n",
        "This code block defines the core architecture for our self-supervised learning method. It creates the MoCo-v2 model using ResNet-50 as the backbone and adds the required contrastive-learning components. The block contains four major parts:\n",
        "\n",
        "1. ProjectionMLP ‚Äî the MoCo-v2 projection head\n",
        "\n",
        "MoCo-v2 requires projecting backbone features (2048-dim from ResNet-50) into a smaller space (128-dim) for contrastive learning.\n",
        "\n",
        "This class builds the projection head:\n",
        "\n",
        "2048 ‚Üí 2048 ‚Üí 128\n",
        "\n",
        "\n",
        "with:\n",
        "- a hidden layer (ReLU)\n",
        "- an output layer\n",
        "- normalization\n",
        "\n",
        "This is exactly the architecture recommended in the MoCo v2 paper.\n",
        "\n",
        "2. MoCoResNet50 ‚Äî query encoder and momentum encoder\n",
        "\n",
        "MoCo requires two networks:\n",
        "\n",
        "üîπ encoder_q\n",
        "\n",
        "- the main trainable network\n",
        "- receives one augmented view of each image\n",
        "- updated by gradients\n",
        "\n",
        "üîπ encoder_k\n",
        "- a slowly updated ‚Äúmomentum encoder‚Äù\n",
        "- receives the second augmented view\n",
        "- no gradients\n",
        "- updated via EMA:\n",
        "encoder_k ‚Üê m * encoder_k + (1 - m) * encoder_q\n",
        "\n",
        "Both encoders:\n",
        "- use a randomly initialized ResNet-50 backbone\n",
        "- remove the classification layer (fc = Identity())\n",
        "- attach the same 128-dim projection MLP\n",
        "\n",
        "The code also initializes encoder_k with the same weights as encoder_q.\n",
        "\n",
        "3. MoCoQueue ‚Äî the FIFO memory bank of negative samples\n",
        "\n",
        "MoCo uses a large queue of previously encoded features (negatives), instead of relying only on the current batch.\n",
        "\n",
        "This queue:\n",
        "- stores 65,536 embeddings\n",
        "- each embedding has 128 dimensions\n",
        "- behaves like a circular buffer\n",
        "- automatically overwrites the oldest entries\n",
        "- is used every iteration for contrastive loss\n",
        "\n",
        "The queue dramatically improves contrastive learning stability and performance.\n",
        "\n",
        "4. Instantiate the model and queue on GPU\n",
        "\n",
        "At the end, the code creates:\n",
        "\n",
        "- model = MoCoResNet50(dim_feature=128).cuda()\n",
        "- queue = MoCoQueue(size=65536, dim=128).cuda()\n",
        "\n",
        "and prints a confirmation message.\n",
        "\n",
        "These objects will be used later in:\n",
        "- the training step (contrastive_covariance_step)\n",
        "- the optimizer setup\n",
        "- the main SSL training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIM8Pnqm1CC-",
        "outputId": "ce652dfa-368c-4ba2-9a82-cff6df500b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> MoCo-ResNet50 model + 128-d queue initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# ======================================================================\n",
        "# FIXED & CORRECTED MoCo Model + Projection Head + Queue (128-dim)\n",
        "# ======================================================================\n",
        "# This block ensures that:\n",
        "#   - encoder_q outputs 128-d features\n",
        "#   - encoder_k outputs 128-d features\n",
        "#   - queue stores 128-d features\n",
        "#   - q, k, queue embeddings perfectly match in dimension\n",
        "# ======================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Projection Head: 2048 ‚Üí 2048 ‚Üí 128 (MoCo-v2 style MLP)\n",
        "# ----------------------------------------------------------\n",
        "class ProjectionMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard MoCo-v2 projection head:\n",
        "        - Input: 2048-dim backbone features (ResNet-50)\n",
        "        - Output: 128-dim normalized projection for contrastive learning\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_in=2048, dim_hidden=2048, dim_out=128):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim_in, dim_hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_hidden, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        return F.normalize(x, dim=1)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. MoCo Model (ResNet-50 backbone + projection head)\n",
        "# ----------------------------------------------------------\n",
        "class MoCoResNet50(nn.Module):\n",
        "    \"\"\"\n",
        "    MoCo-v2 architecture:\n",
        "      - encoder_q: trainable\n",
        "      - encoder_k: EMA momentum encoder (no grads)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_feature=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load ResNet-50 backbone\n",
        "        backbone = models.resnet50(weights=None)   # must be randomly initialized\n",
        "        backbone.fc = nn.Identity()               # remove classification layer\n",
        "\n",
        "        # Query encoder: backbone + projection head\n",
        "        self.encoder_q = nn.Sequential(\n",
        "            backbone,\n",
        "            ProjectionMLP(dim_in=2048, dim_hidden=2048, dim_out=dim_feature)\n",
        "        )\n",
        "\n",
        "        # Key encoder (EMA)\n",
        "        backbone_k = models.resnet50(weights=None)\n",
        "        backbone_k.fc = nn.Identity()\n",
        "        self.encoder_k = nn.Sequential(\n",
        "            backbone_k,\n",
        "            ProjectionMLP(dim_in=2048, dim_hidden=2048, dim_out=dim_feature)\n",
        "        )\n",
        "\n",
        "        # Initialize key encoder to match query encoder\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(),\n",
        "                                    self.encoder_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)\n",
        "            param_k.requires_grad = False\n",
        "\n",
        "    # no forward() needed, we call encoder_q and encoder_k separately\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. FIFO Queue for Negative Samples (Dimension = 128)\n",
        "# ----------------------------------------------------------\n",
        "class MoCoQueue(nn.Module):\n",
        "    \"\"\"\n",
        "    FIFO memory queue storing 128-d embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, size=65536, dim=128):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.dim = dim\n",
        "\n",
        "        # queue: (size, dim)\n",
        "        self.register_buffer(\"queue\", torch.randn(size, dim))\n",
        "        self.queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "        self.register_buffer(\"ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def enqueue(self, keys):\n",
        "        \"\"\"\n",
        "        Add new keys to queue, remove oldest entries.\n",
        "        keys: (batch_size, dim)\n",
        "        \"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "        ptr = int(self.ptr)\n",
        "\n",
        "        # Replace entries\n",
        "        if ptr + batch_size <= self.size:\n",
        "            self.queue[ptr:ptr+batch_size] = keys\n",
        "            self.ptr[0] = (ptr + batch_size) % self.size\n",
        "        else:\n",
        "            # wrap around\n",
        "            n1 = self.size - ptr\n",
        "            n2 = batch_size - n1\n",
        "            self.queue[ptr:] = keys[:n1]\n",
        "            self.queue[:n2] = keys[n1:]\n",
        "            self.ptr[0] = n2\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Instantiate Model + Queue (DIM=128) on GPU\n",
        "# ----------------------------------------------------------\n",
        "model = MoCoResNet50(dim_feature=128).cuda()\n",
        "# queue = MoCoQueue(size=65536, dim=128).cuda()\n",
        "queue = MoCoQueue(size=4096, dim=128).cuda()\n",
        "\n",
        "\n",
        "print(\">>> MoCo-ResNet50 model + 128-d queue initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P38AKEb4jzFw"
      },
      "source": [
        "## STEP 10 ‚Äî Load only a subset of 500k (here we test on 3k) Unlabeled Pretraining Dataset from Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LapvCVR4BJhy"
      },
      "source": [
        "### Step 10.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-RaXu3kBJzI",
        "outputId": "1da544bf-e56b-43c5-b6b9-9836b4dbdac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA SOURCE = local_30k\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# DATA SOURCE CONFIGURATION\n",
        "# ===============================================================\n",
        "# Choose where training images come from.\n",
        "# For pipeline testing, use \"local\".\n",
        "# Later, we can change this to \"hf_full\" or \"local_500k\".\n",
        "\n",
        "DATA_SOURCE = \"local_30k\"    # options: \"local_15k\", \"hf_full\"\n",
        "\n",
        "# Path to local subset in /content or Drive\n",
        "LOCAL_DATA_DIR = \"/content/local_pretrain_30k\"\n",
        "\n",
        "print(\"DATA SOURCE =\", DATA_SOURCE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160,
          "referenced_widgets": [
            "56099ea2aaa84b90ad0b3f75e684c865",
            "428dddcbb898493cb244bcd05857147a",
            "c2ac3c5829274457a55ec2775f43963a",
            "9959a4e962754429adedfcb3506a7c82",
            "0b7034d9ea9c4e2ab22e196d702fb83a",
            "32dc832a46384d0cabe497ead8f0b77e",
            "d4f95804c8334bdaad7bc1265d882f89",
            "6ba856ec423549e29b393e810d9d5bc4",
            "2bbd7b74bf16402f92bc767fa654bbc6",
            "e2b525e8e60f42c79c498617417d66c9",
            "0d11ed76d4b8445cb70d65465e8136fc"
          ]
        },
        "id": "pBnw74-oKGW0",
        "outputId": "0a0c8d39-2150-47fd-8422-1731365f9a1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Using snapshot_download() to fetch dataset shards...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56099ea2aaa84b90ad0b3f75e684c865",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Download complete. Extracting shards...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ZIPs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving 15000 images locally...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30000/30000 [5:25:22<00:00,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Saved 30000 images to /content/local_pretrain_30k\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# STEP 10 ‚Äî FAST DOWNLOAD USING snapshot_download()  (3‚Äì5 minutes)\n",
        "# ===============================================================\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "LOCAL_DATA_DIR = \"/content/local_pretrain_30k\"\n",
        "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
        "\n",
        "if DATA_SOURCE == \"local_30k\":\n",
        "\n",
        "    print(\"‚úì Using snapshot_download() to fetch dataset shards...\")\n",
        "\n",
        "    repo_dir = snapshot_download(\n",
        "        repo_id=\"tsbpp/fall2025_deeplearning\",\n",
        "        repo_type=\"dataset\",\n",
        "        local_dir=\"/content/hf_raw\",\n",
        "        allow_patterns=[\"*.zip\"],       # only zip files\n",
        "    )\n",
        "\n",
        "    print(\"‚úì Download complete. Extracting shards...\")\n",
        "\n",
        "    zip_files = sorted([f for f in os.listdir(repo_dir) if f.endswith(\".zip\")])\n",
        "\n",
        "    # extract only what you need\n",
        "    extracted_images = []\n",
        "    for z in tqdm(zip_files, desc=\"Extracting ZIPs\"):\n",
        "        zip_path = os.path.join(repo_dir, z)\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            # list all image file names inside the zip\n",
        "            for name in zf.namelist():\n",
        "                if name.endswith(\".jpg\"):\n",
        "                    extracted_images.append((zip_path, name))\n",
        "\n",
        "    # pick random 30000\n",
        "    import random\n",
        "    random.seed(42)\n",
        "    selected = random.sample(extracted_images, 30000)\n",
        "\n",
        "    print(\"Saving 30000 images locally...\")\n",
        "    for i, (zip_path, name) in enumerate(tqdm(selected)):\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            data = zf.read(name)\n",
        "        out_path = os.path.join(LOCAL_DATA_DIR, f\"img_{i:05d}.jpg\")\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            f.write(data)\n",
        "\n",
        "    print(f\"‚úì Saved 30000 images to {LOCAL_DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-aZyzynBXKN"
      },
      "source": [
        "Step 10.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdusBVtLBWSa",
        "outputId": "5497e9aa-232b-4a73-cc58-96099953c7de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local dataset: 30000 images found.\n",
            "‚úì Local SSL DataLoader created.\n",
            "Batches per epoch: 234\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# STEP ‚Äî Create dataset & dataloader for LOCAL 15k images\n",
        "# ================================================================\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "\n",
        "class LocalMoCoDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads images saved locally in /content/local_pretrain_3k\n",
        "    Returns two augmented views for MoCo training.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, transform):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.files = sorted(glob.glob(os.path.join(root_dir, \"*.jpg\")))\n",
        "        print(f\"Local dataset: {len(self.files)} images found.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.files[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        x1, x2 = self.transform(img)\n",
        "        return x1, x2\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Instantiate dataset + loader\n",
        "# ================================================================\n",
        "ssl_transform = MoCoTransform(image_size=96)\n",
        "\n",
        "ssl_dataset = LocalMoCoDataset(\n",
        "    root_dir=LOCAL_DATA_DIR,\n",
        "    transform=ssl_transform\n",
        ")\n",
        "\n",
        "ssl_loader = DataLoader(\n",
        "    ssl_dataset,\n",
        "    batch_size=128,\n",
        "    # batch_size=config[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "print(\"‚úì Local SSL DataLoader created.\")\n",
        "print(\"Batches per epoch:\", len(ssl_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eus0oT_Y1xmq"
      },
      "outputs": [],
      "source": [
        "# # The dataset is hosted at: tsbpp/fall2025_deeplearning\n",
        "# # It contains:\n",
        "# #   (1) pretraining split: ~500,000 unlabeled images\n",
        "# #   (2) eval_public split: labeled dataset for downstream classification\n",
        "# #\n",
        "# # We will:\n",
        "# #   1. Install Hugging Face \"datasets\" library\n",
        "# #   2. Load the unlabeled dataset directly (no manual download needed)\n",
        "# #   3. Wrap it into our self-supervised MoCo dataset class\n",
        "# #   4. Create the final DataLoader used for SSL training\n",
        "# # ================================================================\n",
        "\n",
        "# # ------------------------------------------------\n",
        "# # 1. Install Hugging Face Datasets\n",
        "# # ------------------------------------------------\n",
        "# !pip install datasets --quiet\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# from torchvision.transforms import functional as TF\n",
        "\n",
        "# print(\"Hugging Face 'datasets' library installed.\")\n",
        "\n",
        "# # ------------------------------------------------\n",
        "# # 2. Load the unlabeled pretraining dataset\n",
        "# # ------------------------------------------------\n",
        "# # The \"train\" split of this dataset corresponds to the pretrain/ folder.\n",
        "# # These are ALL unlabeled images used for MoCo pretraining.\n",
        "# print(\"Loading unlabeled 500k dataset from Hugging Face...\")\n",
        "# hf_pretrain = load_dataset(\"tsbpp/fall2025_deeplearning\", split=\"train\")\n",
        "\n",
        "# print(\"Unlabeled dataset loaded!\")\n",
        "# print(\"Number of images:\", len(hf_pretrain))\n",
        "# # Each element is like: {\"image\": PIL.Image}\n",
        "\n",
        "# # ------------------------------------------------\n",
        "# # 3. Define a dataset wrapper compatible with our MoCo code\n",
        "# # ------------------------------------------------\n",
        "# class HFMoCoDataset(torch.utils.data.Dataset):\n",
        "#     \"\"\"\n",
        "#     Wraps the Hugging Face dataset so that it:\n",
        "#       - returns two augmented views of each image\n",
        "#       - matches the expected format for the MoCo training step\n",
        "#     \"\"\"\n",
        "#     def __init__(self, hf_dataset, transform):\n",
        "#         self.ds = hf_dataset\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.ds)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # HF dataset returns a dict: {\"image\": PIL.Image}\n",
        "#         img = self.ds[idx][\"image\"].convert(\"RGB\")\n",
        "\n",
        "#         # Return two MoCo-style augmentations\n",
        "#         x1, x2 = self.transform(img)\n",
        "#         return x1, x2\n",
        "\n",
        "\n",
        "# # ------------------------------------------------\n",
        "# # 4. Create SSL dataset and DataLoader\n",
        "# # ------------------------------------------------\n",
        "# print(\"Preparing MoCo-style dataset...\")\n",
        "\n",
        "# # Use your existing MoCo augmentations\n",
        "# ssl_transform = MoCoTransform(image_size=96)\n",
        "\n",
        "# ssl_dataset = HFMoCoDataset(hf_pretrain, ssl_transform)\n",
        "\n",
        "# # IMPORTANT NOTES:\n",
        "# #   - batch_size=128 works well for T4 GPUs\n",
        "# #   - drop_last=True is recommended for MoCo training stability\n",
        "# ssl_loader = torch.utils.data.DataLoader(\n",
        "#     ssl_dataset,\n",
        "#     batch_size=128,        # adjust later depending on GPU memory\n",
        "#     shuffle=True,\n",
        "#     num_workers=0,\n",
        "#     pin_memory=True,\n",
        "#     drop_last=True\n",
        "# )\n",
        "\n",
        "# print(\"SSL DataLoader ready.\")\n",
        "# print(\"Number of batches per epoch:\", len(ssl_loader))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fqjX3wh3Ind"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMiRISjAulPK"
      },
      "source": [
        "## STEP 11 ‚Äî Configure Full SSL Pretraining (the ‚ÄúReal Training Loop‚Äù)\n",
        "\n",
        "Goal: Now that we have MoCo-v2 architecture, VICReg covariance loss, 500k dataloader, dummy training validated etc, it‚Äôs time to build the actual training loop that we will run on the full dataset.\n",
        "\n",
        "This step will:\n",
        "- configure training hyperparameters\n",
        "- set the optimizer\n",
        "- set the learning rate schedule\n",
        "- prepare checkpointing\n",
        "- create the full train_moco call for real training\n",
        "- carefully tune parameters to run on Colab T4 / A100\n",
        "- ensure the loop is safe, resumable, and stable\n",
        "\n",
        "This step does NOT start training yet ‚Äî it only defines and prepares everything.\n",
        "\n",
        "Once Step 11 is ready and stable, Step 12 will actually launch the long pretraining run.\n",
        "\n",
        "WHAT THE CODE WILL DO?\n",
        "1. Define recommended hyperparameters(batch size, epochs, temperature œÑ,\n",
        "EMA momentum m, queue size, Œª for covariance)\n",
        "2. Build the optimizer (SGD or AdamW)\n",
        "3. Add a cosine learning rate scheduler\n",
        "4. Prepare checkpoint saving every N epochs\n",
        "5. Bind all components together in a training configuration object\n",
        "6. Leave you ready to run:\n",
        "\n",
        "train_moco(model, queue, loss_fn, ssl_loader, optimizer, ...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqRBYakYj42G",
        "outputId": "0bbd2034-167f-4dca-f6e4-bee7a40769e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring MoCo-Cov training ...\n",
            "epochs: 20\n",
            "batch_size: 128\n",
            "lr: 0.03\n",
            "momentum: 0.9\n",
            "weight_decay: 0.0001\n",
            "tau: 0.2\n",
            "m: 0.999\n",
            "lambda_cov: 1.0\n",
            "queue_size: 4096\n",
            "save_every: 1\n",
            "checkpoint_path: /content/moco_cov_checkpoint.pth\n",
            "\n",
            "Optimizer ready (SGD with momentum).\n",
            "Cosine LR scheduler ready.\n",
            "\n",
            "*** Step 11 completed. ***\n",
            "You can now proceed to Step 12 ‚Äî Running full SSL pretraining.\n",
            "We will start training ONLY when you request it.\n"
          ]
        }
      ],
      "source": [
        "# Goal:\n",
        "#   - Define all hyperparameters for the REAL MoCo-Cov training\n",
        "#   - Create optimizer (SGD)\n",
        "#   - Create cosine LR scheduler\n",
        "#   - Prepare checkpointing\n",
        "#   - DO NOT start training yet (training will be Step 12)\n",
        "\n",
        "# This block sets up everything needed for long SSL pretraining.\n",
        "# ================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "print(\"Configuring MoCo-Cov training ...\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 1. Hyperparameters for sanity-check SSL training\n",
        "# --------------------------------------------------------\n",
        "config = {\n",
        "    \"epochs\": 20,              # short sanity check\n",
        "    \"batch_size\": 128,        # matches your DataLoader\n",
        "    \"lr\": 0.03,               # standard MoCo-v2 LR for batch 256 ‚Üí scaled here\n",
        "    \"momentum\": 0.9,          # SGD momentum\n",
        "    \"weight_decay\": 1e-4,     # standard regularization for ResNet-50\n",
        "    \"tau\": 0.2,               # InfoNCE temperature\n",
        "    \"m\": 0.999,               # EMA momentum for encoder_k\n",
        "    \"lambda_cov\": 1.0,        # strength of VICReg covariance loss\n",
        "    # \"queue_size\": 65536,      # typical MoCo queue size (~65k)   # later we need to change this\n",
        "    \"queue_size\": 4096,\n",
        "    \"save_every\": 1,          # save model every epoch for safety\n",
        "    \"checkpoint_path\": \"/content/moco_cov_checkpoint.pth\"\n",
        "}\n",
        "\n",
        "# Print configuration\n",
        "for k,v in config.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2. Re-create optimizer (SGD)\n",
        "# --------------------------------------------------------\n",
        "# We assume 'model' already exists (encoder_q + proj head)\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=config[\"lr\"],\n",
        "    momentum=config[\"momentum\"],\n",
        "    weight_decay=config[\"weight_decay\"],\n",
        ")\n",
        "\n",
        "print(\"\\nOptimizer ready (SGD with momentum).\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 3. Cosine learning rate schedule\n",
        "# --------------------------------------------------------\n",
        "# Cosine annealing is very standard for MoCo-v2:\n",
        "# LR(t) = 0.5 * lr * (1 + cos(pi * t / T))\n",
        "\n",
        "scheduler = CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=config[\"epochs\"],   # one full cosine cycle over training\n",
        "    eta_min=0.0               # final LR goes to zero\n",
        ")\n",
        "\n",
        "print(\"Cosine LR scheduler ready.\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 4. Final print-out\n",
        "# --------------------------------------------------------\n",
        "print(\"\\n*** Step 11 completed. ***\")\n",
        "print(\"You can now proceed to Step 12 ‚Äî Running full SSL pretraining.\")\n",
        "print(\"We will start training ONLY when you request it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzB1kCEoxqWq"
      },
      "source": [
        "## STEP 12 ‚Äî performs SSL pretraining using the MoCo-Cov architecture on the full 500k dataset.\n",
        "\n",
        "1. Each iteration receives a batch:\n",
        "x1, x2 = augmented views of each image\n",
        "\n",
        "\n",
        "These are strong MoCo-v2 augmentations.\n",
        "\n",
        "2. Compute embeddings\n",
        "\n",
        "We compute:\n",
        "\n",
        "Query embedding (trainable)\n",
        "q = model.encoder_q(x1)     # B √ó 128\n",
        "\n",
        "Key embedding (momentum encoder)\n",
        "k = model.encoder_k(x2)     # B √ó 128\n",
        "\n",
        "\n",
        "These are both normalized to unit length.\n",
        "\n",
        "3. Compute contrastive loss (InfoNCE)\n",
        "\n",
        "We compute:\n",
        "\n",
        "- Positive similarity ‚Üí q¬∑k\n",
        "- Negative similarities ‚Üí q¬∑queue\n",
        "- Temperature scaling\n",
        "- Cross-entropy classification (positive is class 0)\n",
        "\n",
        "This trains q to match its own positive (k) and be different from all negatives in the queue.\n",
        "\n",
        "4. Compute covariance regularization (VICReg‚Äôs C-loss)\n",
        "\n",
        "We concatenate q and k:\n",
        "\n",
        "Z = [q; k]\n",
        "\n",
        "\n",
        "Then compute covariance matrix C:\n",
        "\n",
        "C = (Z·µÄ Z) / (N-1)\n",
        "\n",
        "\n",
        "The covariance penalty is:\n",
        "\n",
        "- Only off-diagonal elements (penalizes redundancy)\n",
        "- Makes embeddings less correlated and more stable\n",
        "\n",
        "5. Total Loss\n",
        "loss = contrastive_loss + lambda_cov * covariance_loss\n",
        "\n",
        "6. SGD update on encoder_q\n",
        "\n",
        "Only encoder_q updates with gradients:\n",
        "\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "7. Momentum update of encoder_k\n",
        "\n",
        "This is MoCo's main trick:\n",
        "\n",
        "param_k = m * param_k + (1-m) * param_q\n",
        "\n",
        "\n",
        "This keeps encoder_k stable.\n",
        "\n",
        "8. Add new keys to queue (FIFO)\n",
        "queue.enqueue(k.detach())\n",
        "\n",
        "\n",
        "This provides thousands of negative examples cheaply.\n",
        "\n",
        "9. Cosine LR Scheduler\n",
        "\n",
        "Learning rate decays smoothly each step.\n",
        "\n",
        "10. Logging + checkpoint saving\n",
        "\n",
        "Every 500 steps ‚Üí print loss\n",
        "Every epoch ‚Üí save model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLvY8e2Y1kqG"
      },
      "outputs": [],
      "source": [
        "# # Prep code for step 12\n",
        "# # ================================================================\n",
        "# # Function: contrastive_covariance_step\n",
        "# # This performs one full MoCo-Cov training step:\n",
        "# #   1. Forward pass (encoder_q and encoder_k)\n",
        "# #   2. Compute contrastive InfoNCE loss\n",
        "# #   3. Compute VICReg covariance regularization\n",
        "# #   4. Momentum update for encoder_k\n",
        "# #   5. Update the negative queue\n",
        "# # ================================================================\n",
        "\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# def contrastive_covariance_step(model, queue, loss_fn,\n",
        "#                                 x1, x2, m, lambda_cov, temperature):\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 1. Compute queries from encoder_q\n",
        "#     # ------------------------------------------------------------\n",
        "#     q = model.encoder_q(x1)         # (B, dim)\n",
        "#     q = F.normalize(q, dim=1)\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 2. Compute positive keys from encoder_k (EMA network)\n",
        "#     # ------------------------------------------------------------\n",
        "#     with torch.no_grad():\n",
        "#         k = model.encoder_k(x2)\n",
        "#         k = F.normalize(k, dim=1)\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 3. Compute contrastive logits using the queue\n",
        "#     # ------------------------------------------------------------\n",
        "#     # Positive logit: (B, 1)\n",
        "#     l_pos = torch.einsum('nc,nc->n', q, k).unsqueeze(-1)\n",
        "\n",
        "#     # Negative logits: (B, K)\n",
        "#     l_neg = torch.einsum('nc,kc->nk', q, queue.queue.clone().detach())\n",
        "\n",
        "#     # Concatenate [positive | negatives]\n",
        "#     logits = torch.cat([l_pos, l_neg], dim=1)\n",
        "\n",
        "#     # Apply temperature\n",
        "#     logits /= temperature\n",
        "\n",
        "#     # Labels: positive key is index 0\n",
        "#     labels = torch.zeros(logits.shape[0], dtype=torch.long, device=logits.device)\n",
        "\n",
        "#     # Contrastive loss (InfoNCE)\n",
        "#     loss_contrast = loss_fn(logits, labels)\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 4. VICReg covariance regularizer\n",
        "#     # ------------------------------------------------------------\n",
        "#     # We compute covariance across embedding dimensions.\n",
        "#     # Option: concat q and k (this is the typical batch-based approach)\n",
        "#     Z = torch.cat([q, k], dim=0)             # shape: (2B, dim)\n",
        "#     Z = Z - Z.mean(dim=0, keepdim=True)\n",
        "\n",
        "#     # Covariance matrix: dim x dim\n",
        "#     C = (Z.T @ Z) / (Z.shape[0] - 1)\n",
        "\n",
        "#     # Penalize off-diagonal entries (reduce redundancy)\n",
        "#     cov_loss = (C ** 2).sum() - (C.diag() ** 2).sum()\n",
        "\n",
        "#     # Rescale covariance loss\n",
        "#     loss_cov = lambda_cov * cov_loss\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 5. Total loss\n",
        "#     # ------------------------------------------------------------\n",
        "#     loss = loss_contrast + loss_cov\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 6. Momentum update of encoder_k\n",
        "#     # ------------------------------------------------------------\n",
        "#     with torch.no_grad():\n",
        "#         for param_q, param_k in zip(model.encoder_q.parameters(),\n",
        "#                                     model.encoder_k.parameters()):\n",
        "#             param_k.data = param_k.data * m + param_q.data * (1.0 - m)\n",
        "\n",
        "#     # ------------------------------------------------------------\n",
        "#     # 7. Update the queue\n",
        "#     # ------------------------------------------------------------\n",
        "#     queue.enqueue(k)\n",
        "\n",
        "#     return loss, float(loss_contrast.item()), float(loss_cov.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otumri05xnm3"
      },
      "outputs": [],
      "source": [
        "# # ================================================================\n",
        "# # STEP 12 ‚Äî Run Sanity-Check SSL Pretraining (5 epochs)\n",
        "# # Option B: Recommended logging (print every 500 steps)\n",
        "# # ================================================================\n",
        "# # What this block does:\n",
        "# #   - runs full MoCo-Cov training on the real 500k dataset\n",
        "# #   - prints logs every 500 training steps\n",
        "# #   - saves checkpoint every epoch\n",
        "# #   - uses the optimizer and scheduler from Step 11\n",
        "# # ================================================================\n",
        "\n",
        "# print(\"Starting SANITY-CHECK MoCo-Cov training on full 500k dataset...\\n\")\n",
        "\n",
        "# num_epochs = config[\"epochs\"]\n",
        "# log_every = 500          # recommended logging frequency\n",
        "# save_every = config[\"save_every\"]\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     epoch_loss = 0.0\n",
        "#     epoch_contrast = 0.0\n",
        "#     epoch_cov = 0.0\n",
        "\n",
        "#     print(f\"\\n==== Epoch {epoch+1}/{num_epochs} ====\")\n",
        "\n",
        "#     for step, (x1, x2) in enumerate(ssl_loader):\n",
        "\n",
        "#         # ----------------------------------------------------------\n",
        "#         # Move batch to GPU\n",
        "#         # ----------------------------------------------------------\n",
        "#         x1 = x1.cuda(non_blocking=True)\n",
        "#         x2 = x2.cuda(non_blocking=True)\n",
        "\n",
        "#         # ----------------------------------------------------------\n",
        "#         # 1. Forward pass through MoCo-Cov step\n",
        "#         # ----------------------------------------------------------\n",
        "#         loss, loss_contrast, loss_cov = contrastive_covariance_step(\n",
        "#             model=model,\n",
        "#             queue=queue,\n",
        "#             loss_fn=loss_fn,\n",
        "#             x1=x1,\n",
        "#             x2=x2,\n",
        "#             m=config[\"m\"],               # EMA momentum\n",
        "#             lambda_cov=config[\"lambda_cov\"],\n",
        "#             temperature=config[\"tau\"],\n",
        "#         )\n",
        "\n",
        "#         # ----------------------------------------------------------\n",
        "#         # 2. Backprop + optimizer update\n",
        "#         # ----------------------------------------------------------\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # ----------------------------------------------------------\n",
        "#         # 3. Update learning rate with cosine scheduler\n",
        "#         # ----------------------------------------------------------\n",
        "#         scheduler.step()\n",
        "\n",
        "#         # ----------------------------------------------------------\n",
        "#         # 4. Logging accumulator\n",
        "#         # ----------------------------------------------------------\n",
        "#         epoch_loss += loss.item()\n",
        "#         epoch_contrast += loss_contrast\n",
        "#         epoch_cov += loss_cov\n",
        "\n",
        "#         # ----------------------------------------------------------\n",
        "#         # 5. Print logs every 500 steps\n",
        "#         # ----------------------------------------------------------\n",
        "#         if (step + 1) % log_every == 0:\n",
        "#             print(\n",
        "#                 f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
        "#                 f\"Step {step+1}/{len(ssl_loader)} | \"\n",
        "#                 f\"Loss: {loss.item():.4f} | \"\n",
        "#                 f\"Contrast: {loss_contrast:.4f} | \"\n",
        "#                 f\"Cov: {loss_cov:.4f} | \"\n",
        "#                 f\"LR: {scheduler.get_last_lr()[0]:.6f}\"\n",
        "#             )\n",
        "\n",
        "#     # --------------------------------------------------------------\n",
        "#     # 6. Epoch summary\n",
        "#     # --------------------------------------------------------------\n",
        "#     avg_loss = epoch_loss / len(ssl_loader)\n",
        "#     avg_contrast = epoch_contrast / len(ssl_loader)\n",
        "#     avg_cov = epoch_cov / len(ssl_loader)\n",
        "\n",
        "#     print(\n",
        "#         f\"\\n>>> Epoch {epoch+1} Summary: \"\n",
        "#         f\"AvgLoss={avg_loss:.4f}, \"\n",
        "#         f\"AvgContrast={avg_contrast:.4f}, \"\n",
        "#         f\"AvgCov={avg_cov:.4f}\"\n",
        "#     )\n",
        "\n",
        "#     # --------------------------------------------------------------\n",
        "#     # 7. Save checkpoint\n",
        "#     # --------------------------------------------------------------\n",
        "#     if (epoch + 1) % save_every == 0:\n",
        "#         torch.save(\n",
        "#             {\n",
        "#                 \"epoch\": epoch+1,\n",
        "#                 \"state_dict\": model.state_dict(),\n",
        "#                 \"optimizer\": optimizer.state_dict(),\n",
        "#                 \"scheduler\": scheduler.state_dict(),\n",
        "#                 \"queue\": queue.queue.clone(),\n",
        "#             },\n",
        "#             config[\"checkpoint_path\"]\n",
        "#         )\n",
        "#         print(f\"Checkpoint saved to {config['checkpoint_path']}\")\n",
        "\n",
        "# print(\"\\n=== SANITY-CHECK TRAINING COMPLETE ===\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJV4skk8-8dU"
      },
      "outputs": [],
      "source": [
        "# # ================================================================\n",
        "# # STEP ‚Äî Create dataset & dataloader for LOCAL 30k images\n",
        "# # ================================================================\n",
        "\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# import glob\n",
        "# import os\n",
        "\n",
        "# class LocalMoCoDataset(Dataset):\n",
        "#     \"\"\"\n",
        "#     Loads images saved locally in Drive, e.g.\n",
        "#         img_00000.jpg\n",
        "#         img_00001.jpg\n",
        "#         ...\n",
        "#     Returns two augmented views for MoCo training.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, root_dir, transform):\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transform\n",
        "#         self.files = sorted(glob.glob(os.path.join(root_dir, \"*.jpg\")))\n",
        "#         print(f\"Local dataset: {len(self.files)} images found.\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.files)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = self.files[idx]\n",
        "#         img = Image.open(img_path).convert(\"RGB\")\n",
        "#         x1, x2 = self.transform(img)\n",
        "#         return x1, x2\n",
        "\n",
        "\n",
        "# # ================================================================\n",
        "# # Instantiate dataset + loader\n",
        "# # ================================================================\n",
        "# local_dataset_dir = \"/content/drive/MyDrive/moco_pretrain_30k\"\n",
        "\n",
        "# ssl_transform = MoCoTransform(image_size=96)\n",
        "\n",
        "# ssl_dataset = LocalMoCoDataset(\n",
        "#     root_dir=local_dataset_dir,\n",
        "#     transform=ssl_transform\n",
        "# )\n",
        "\n",
        "# ssl_loader = DataLoader(\n",
        "#     ssl_dataset,\n",
        "#     batch_size=128,\n",
        "#     shuffle=True,\n",
        "#     num_workers=2,\n",
        "#     pin_memory=True,\n",
        "#     drop_last=True\n",
        "# )\n",
        "\n",
        "# print(\"‚úì Local SSL DataLoader created.\")\n",
        "# print(\"Batches per epoch:\", len(ssl_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApMRKM63j44d",
        "outputId": "b99c4969-1989-4e7f-f70e-c436e353ce33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting SANITY-CHECK MoCo-Cov training on local 15k subset of the data...\n",
            "\n",
            "\n",
            "==== Epoch 1/20 ====\n",
            "[Epoch 1/20] Step 100/234 | Loss: 8.1277 | Contrast: 8.0951 | Cov: 0.0326 | LR: 0.030000\n",
            "[Epoch 1/20] Step 200/234 | Loss: 8.2089 | Contrast: 8.1866 | Cov: 0.0223 | LR: 0.030000\n",
            "\n",
            ">>> Epoch 1 Summary: AvgLoss=8.0504, AvgContrast=8.0313, AvgCov=0.0191\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 2/20 ====\n",
            "[Epoch 2/20] Step 100/234 | Loss: 8.2489 | Contrast: 8.2340 | Cov: 0.0149 | LR: 0.029815\n",
            "[Epoch 2/20] Step 200/234 | Loss: 8.2528 | Contrast: 8.2403 | Cov: 0.0125 | LR: 0.029815\n",
            "\n",
            ">>> Epoch 2 Summary: AvgLoss=8.2503, AvgContrast=8.2356, AvgCov=0.0148\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 3/20 ====\n",
            "[Epoch 3/20] Step 100/234 | Loss: 8.2534 | Contrast: 8.2426 | Cov: 0.0107 | LR: 0.029266\n",
            "[Epoch 3/20] Step 200/234 | Loss: 8.2216 | Contrast: 8.2098 | Cov: 0.0117 | LR: 0.029266\n",
            "\n",
            ">>> Epoch 3 Summary: AvgLoss=8.2585, AvgContrast=8.2470, AvgCov=0.0114\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 4/20 ====\n",
            "[Epoch 4/20] Step 100/234 | Loss: 8.2775 | Contrast: 8.2633 | Cov: 0.0142 | LR: 0.028365\n",
            "[Epoch 4/20] Step 200/234 | Loss: 8.2404 | Contrast: 8.2261 | Cov: 0.0143 | LR: 0.028365\n",
            "\n",
            ">>> Epoch 4 Summary: AvgLoss=8.2368, AvgContrast=8.2247, AvgCov=0.0120\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 5/20 ====\n",
            "[Epoch 5/20] Step 100/234 | Loss: 8.2188 | Contrast: 8.2076 | Cov: 0.0113 | LR: 0.027135\n",
            "[Epoch 5/20] Step 200/234 | Loss: 8.2243 | Contrast: 8.2097 | Cov: 0.0145 | LR: 0.027135\n",
            "\n",
            ">>> Epoch 5 Summary: AvgLoss=8.2257, AvgContrast=8.2126, AvgCov=0.0130\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 6/20 ====\n",
            "[Epoch 6/20] Step 100/234 | Loss: 8.1971 | Contrast: 8.1821 | Cov: 0.0150 | LR: 0.025607\n",
            "[Epoch 6/20] Step 200/234 | Loss: 8.2017 | Contrast: 8.1876 | Cov: 0.0141 | LR: 0.025607\n",
            "\n",
            ">>> Epoch 6 Summary: AvgLoss=8.2007, AvgContrast=8.1849, AvgCov=0.0158\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 7/20 ====\n",
            "[Epoch 7/20] Step 100/234 | Loss: 8.1244 | Contrast: 8.1038 | Cov: 0.0206 | LR: 0.023817\n",
            "[Epoch 7/20] Step 200/234 | Loss: 8.1838 | Contrast: 8.1626 | Cov: 0.0212 | LR: 0.023817\n",
            "\n",
            ">>> Epoch 7 Summary: AvgLoss=8.1665, AvgContrast=8.1469, AvgCov=0.0196\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 8/20 ====\n",
            "[Epoch 8/20] Step 100/234 | Loss: 8.0263 | Contrast: 8.0009 | Cov: 0.0254 | LR: 0.021810\n",
            "[Epoch 8/20] Step 200/234 | Loss: 8.1378 | Contrast: 8.1078 | Cov: 0.0300 | LR: 0.021810\n",
            "\n",
            ">>> Epoch 8 Summary: AvgLoss=8.1179, AvgContrast=8.0922, AvgCov=0.0258\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 9/20 ====\n",
            "[Epoch 9/20] Step 100/234 | Loss: 8.0101 | Contrast: 7.9744 | Cov: 0.0358 | LR: 0.019635\n",
            "[Epoch 9/20] Step 200/234 | Loss: 7.9637 | Contrast: 7.9376 | Cov: 0.0260 | LR: 0.019635\n",
            "\n",
            ">>> Epoch 9 Summary: AvgLoss=8.0448, AvgContrast=8.0156, AvgCov=0.0292\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 10/20 ====\n",
            "[Epoch 10/20] Step 100/234 | Loss: 7.9567 | Contrast: 7.9237 | Cov: 0.0330 | LR: 0.017347\n",
            "[Epoch 10/20] Step 200/234 | Loss: 8.0317 | Contrast: 8.0082 | Cov: 0.0234 | LR: 0.017347\n",
            "\n",
            ">>> Epoch 10 Summary: AvgLoss=7.9606, AvgContrast=7.9339, AvgCov=0.0268\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 11/20 ====\n",
            "[Epoch 11/20] Step 100/234 | Loss: 7.8494 | Contrast: 7.8229 | Cov: 0.0266 | LR: 0.015000\n",
            "[Epoch 11/20] Step 200/234 | Loss: 7.7341 | Contrast: 7.7068 | Cov: 0.0272 | LR: 0.015000\n",
            "\n",
            ">>> Epoch 11 Summary: AvgLoss=7.8967, AvgContrast=7.8690, AvgCov=0.0277\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 12/20 ====\n",
            "[Epoch 12/20] Step 100/234 | Loss: 7.8802 | Contrast: 7.8451 | Cov: 0.0350 | LR: 0.012653\n",
            "[Epoch 12/20] Step 200/234 | Loss: 7.8417 | Contrast: 7.8061 | Cov: 0.0356 | LR: 0.012653\n",
            "\n",
            ">>> Epoch 12 Summary: AvgLoss=7.8382, AvgContrast=7.8052, AvgCov=0.0331\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 13/20 ====\n",
            "[Epoch 13/20] Step 100/234 | Loss: 7.7244 | Contrast: 7.6878 | Cov: 0.0366 | LR: 0.010365\n",
            "[Epoch 13/20] Step 200/234 | Loss: 7.7809 | Contrast: 7.7374 | Cov: 0.0435 | LR: 0.010365\n",
            "\n",
            ">>> Epoch 13 Summary: AvgLoss=7.7758, AvgContrast=7.7360, AvgCov=0.0398\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 14/20 ====\n",
            "[Epoch 14/20] Step 100/234 | Loss: 7.5670 | Contrast: 7.5241 | Cov: 0.0429 | LR: 0.008190\n",
            "[Epoch 14/20] Step 200/234 | Loss: 7.6810 | Contrast: 7.6370 | Cov: 0.0440 | LR: 0.008190\n",
            "\n",
            ">>> Epoch 14 Summary: AvgLoss=7.6708, AvgContrast=7.6250, AvgCov=0.0458\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 15/20 ====\n",
            "[Epoch 15/20] Step 100/234 | Loss: 7.6978 | Contrast: 7.6386 | Cov: 0.0592 | LR: 0.006183\n",
            "[Epoch 15/20] Step 200/234 | Loss: 7.5237 | Contrast: 7.4705 | Cov: 0.0532 | LR: 0.006183\n",
            "\n",
            ">>> Epoch 15 Summary: AvgLoss=7.5756, AvgContrast=7.5246, AvgCov=0.0509\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 16/20 ====\n",
            "[Epoch 16/20] Step 100/234 | Loss: 7.4595 | Contrast: 7.4045 | Cov: 0.0550 | LR: 0.004393\n",
            "[Epoch 16/20] Step 200/234 | Loss: 7.4179 | Contrast: 7.3644 | Cov: 0.0535 | LR: 0.004393\n",
            "\n",
            ">>> Epoch 16 Summary: AvgLoss=7.4735, AvgContrast=7.4178, AvgCov=0.0558\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 17/20 ====\n",
            "[Epoch 17/20] Step 100/234 | Loss: 7.4883 | Contrast: 7.4308 | Cov: 0.0575 | LR: 0.002865\n",
            "[Epoch 17/20] Step 200/234 | Loss: 7.2825 | Contrast: 7.2281 | Cov: 0.0544 | LR: 0.002865\n",
            "\n",
            ">>> Epoch 17 Summary: AvgLoss=7.3754, AvgContrast=7.3157, AvgCov=0.0597\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 18/20 ====\n",
            "[Epoch 18/20] Step 100/234 | Loss: 7.2664 | Contrast: 7.2055 | Cov: 0.0610 | LR: 0.001635\n",
            "[Epoch 18/20] Step 200/234 | Loss: 7.0640 | Contrast: 6.9977 | Cov: 0.0663 | LR: 0.001635\n",
            "\n",
            ">>> Epoch 18 Summary: AvgLoss=7.2701, AvgContrast=7.2069, AvgCov=0.0633\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 19/20 ====\n",
            "[Epoch 19/20] Step 100/234 | Loss: 7.0037 | Contrast: 6.9400 | Cov: 0.0637 | LR: 0.000734\n",
            "[Epoch 19/20] Step 200/234 | Loss: 7.1232 | Contrast: 7.0564 | Cov: 0.0668 | LR: 0.000734\n",
            "\n",
            ">>> Epoch 19 Summary: AvgLoss=7.2031, AvgContrast=7.1376, AvgCov=0.0655\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "==== Epoch 20/20 ====\n",
            "[Epoch 20/20] Step 100/234 | Loss: 6.9549 | Contrast: 6.8841 | Cov: 0.0709 | LR: 0.000185\n",
            "[Epoch 20/20] Step 200/234 | Loss: 7.0998 | Contrast: 7.0302 | Cov: 0.0696 | LR: 0.000185\n",
            "\n",
            ">>> Epoch 20 Summary: AvgLoss=7.1487, AvgContrast=7.0802, AvgCov=0.0685\n",
            "Checkpoint saved to /content/moco_cov_checkpoint.pth\n",
            "\n",
            "=== SANITY-CHECK TRAINING COMPLETE ===\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# STEP 12 ‚Äî SANITY-CHECK SSL PRETRAINING WITH MoCo-RESNET50\n",
        "# ================================================================\n",
        "# This final training loop:\n",
        "#   - Uses your final MoCoResNet50 model\n",
        "#   - Uses MoCoQueue (128-d)\n",
        "#   - Uses MoCoCovLoss (InfoNCE + covariance reg.)\n",
        "#   - Trains for config[\"epochs\"]\n",
        "#   - Logs every 500 steps\n",
        "#   - Saves checkpoint every epoch\n",
        "# ================================================================\n",
        "\n",
        "print(\"Starting SANITY-CHECK MoCo-Cov training on local 15k subset of the data...\\n\")\n",
        "\n",
        "loss_fn = MoCoCovLoss(\n",
        "    temperature=config[\"tau\"],\n",
        "    lambda_cov=config[\"lambda_cov\"],\n",
        "    use_queue_for_cov=False            # best for stability\n",
        ").cuda()\n",
        "\n",
        "num_epochs = config[\"epochs\"]\n",
        "log_every = 100\n",
        "save_every = config[\"save_every\"]\n",
        "m = config[\"m\"]                        # EMA momentum\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_contrast = 0.0\n",
        "    epoch_cov = 0.0\n",
        "\n",
        "    print(f\"\\n==== Epoch {epoch+1}/{num_epochs} ====\")\n",
        "\n",
        "    for step, (x1, x2) in enumerate(ssl_loader):\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # Move batch to GPU\n",
        "        # ---------------------------------------------\n",
        "        x1 = x1.cuda(non_blocking=True)\n",
        "        x2 = x2.cuda(non_blocking=True)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 1. Compute embeddings\n",
        "        # ---------------------------------------------\n",
        "        q = model.encoder_q(x1)\n",
        "        q = F.normalize(q, dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            k = model.encoder_k(x2)\n",
        "            k = F.normalize(k, dim=1)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 2. InfoNCE contrastive loss\n",
        "        # ---------------------------------------------\n",
        "        # positive logits: (B, 1)\n",
        "        l_pos = torch.einsum('nc,nc->n', q, k).unsqueeze(1)\n",
        "\n",
        "        # negative logits: (B, K)\n",
        "        l_neg = torch.einsum('nc,kc->nk', q, queue.queue.clone().detach())\n",
        "\n",
        "        # concatenate logits\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
        "        logits /= config[\"tau\"]\n",
        "\n",
        "        labels = torch.zeros(q.shape[0], dtype=torch.long, device=q.device)\n",
        "\n",
        "        loss_contrast = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 3. VICReg covariance penalty\n",
        "        # ---------------------------------------------\n",
        "        Z = torch.cat([q, k], dim=0)\n",
        "        Z = Z - Z.mean(dim=0, keepdim=True)\n",
        "\n",
        "        C = (Z.T @ Z) / (Z.size(0) - 1)\n",
        "        cov_loss = (C ** 2).sum() - (C.diag() ** 2).sum()\n",
        "\n",
        "        loss_cov = config[\"lambda_cov\"] * cov_loss\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 4. Total loss\n",
        "        # ---------------------------------------------\n",
        "        loss = loss_contrast + loss_cov\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 5. Backprop on encoder_q\n",
        "        # ---------------------------------------------\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 6. Momentum update of encoder_k\n",
        "        # ---------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            for param_q, param_k in zip(model.encoder_q.parameters(), model.encoder_k.parameters()):\n",
        "                param_k.data = param_k.data * m + param_q.data * (1 - m)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 7. Update the queue\n",
        "        # ---------------------------------------------\n",
        "        queue.enqueue(k.detach())\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 8. Track metrics\n",
        "        # ---------------------------------------------\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_contrast += loss_contrast.item()\n",
        "        epoch_cov += loss_cov.item()\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 9. Logging every 100 steps\n",
        "        # ---------------------------------------------\n",
        "        if (step + 1) % log_every == 0:\n",
        "            print(\n",
        "                f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
        "                f\"Step {step+1}/{len(ssl_loader)} | \"\n",
        "                f\"Loss: {loss.item():.4f} | \"\n",
        "                f\"Contrast: {loss_contrast.item():.4f} | \"\n",
        "                f\"Cov: {loss_cov.item():.4f} | \"\n",
        "                f\"LR: {scheduler.get_last_lr()[0]:.6f}\"\n",
        "            )\n",
        "\n",
        "        # scheduler step every batch\n",
        "        # scheduler.step()\n",
        "\n",
        "    # ==================================================\n",
        "    # Epoch summary\n",
        "    # ==================================================\n",
        "    print(\n",
        "        f\"\\n>>> Epoch {epoch+1} Summary: \"\n",
        "        f\"AvgLoss={epoch_loss/len(ssl_loader):.4f}, \"\n",
        "        f\"AvgContrast={epoch_contrast/len(ssl_loader):.4f}, \"\n",
        "        f\"AvgCov={epoch_cov/len(ssl_loader):.4f}\"\n",
        "    )\n",
        "    # Step LR scheduler ONCE per epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    # ==================================================\n",
        "    # Save checkpoint\n",
        "    # ==================================================\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": epoch+1,\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"scheduler\": scheduler.state_dict(),\n",
        "            \"queue\": queue.queue.clone(),\n",
        "        },\n",
        "        config[\"checkpoint_path\"]\n",
        "    )\n",
        "    print(f\"Checkpoint saved to {config['checkpoint_path']}\")\n",
        "\n",
        "print(\"\\n=== SANITY-CHECK TRAINING COMPLETE ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d811Y1e116Fw"
      },
      "source": [
        "## Step. CIFAR-10 Loader\n",
        "\n",
        "What we do here?\n",
        "\n",
        "We will use only simple transforms:\n",
        "- Resize to 96√ó96\n",
        "- Convert to tensor\n",
        "- Normalize using ImageNet stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv_aNTX99dgu",
        "outputId": "8e6b5d83-e7a7-405e-c00a-11a792c40265"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:19<00:00, 8.65MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIFAR-10 loaded successfully!\n",
            "Train samples: 50000\n",
            "Test samples: 10000\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Basic eval transforms (no strong augmentations)\n",
        "cifar_transform = T.Compose([\n",
        "    T.Resize((96, 96)),     # match your backbone resolution\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 train/test\n",
        "cifar_train = torchvision.datasets.CIFAR10(\n",
        "    root=\"/content/cifar\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "cifar_test = torchvision.datasets.CIFAR10(\n",
        "    root=\"/content/cifar\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "# Dataloaders\n",
        "cifar_train_loader = DataLoader(\n",
        "    cifar_train,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "cifar_test_loader = DataLoader(\n",
        "    cifar_test,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"CIFAR-10 loaded successfully!\")\n",
        "print(\"Train samples:\", len(cifar_train))\n",
        "print(\"Test samples:\", len(cifar_test))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC-o1jgrYZXw"
      },
      "source": [
        "## Cifar 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQxY6HajYZoq",
        "outputId": "114e5118-7169-47e7-a087-98739ca49ffd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 169M/169M [00:13<00:00, 12.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIFAR-100 loaded successfully!\n",
            "Train samples: 50000\n",
            "Test samples: 10000\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Basic eval transforms (no strong augmentations)\n",
        "cifar_transform = T.Compose([\n",
        "    T.Resize((96, 96)),     # match your backbone resolution\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 train/test\n",
        "cifar_train = torchvision.datasets.CIFAR100(\n",
        "    root=\"/content/cifar100\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "cifar_test = torchvision.datasets.CIFAR100(\n",
        "    root=\"/content/cifar100\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "# Dataloaders\n",
        "cifar_train_loader = DataLoader(\n",
        "    cifar_train,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "cifar_test_loader = DataLoader(\n",
        "    cifar_test,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"CIFAR-100 loaded successfully!\")\n",
        "print(\"Train samples:\", len(cifar_train))\n",
        "print(\"Test samples:\", len(cifar_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07pWaVZD_nuR"
      },
      "source": [
        "## Step: Feature Extraction Function\n",
        "\n",
        "What this function does:\n",
        "- It puts the encoder in eval() mode\n",
        "- Freezes parameters\n",
        "- Processes CIFAR batches\n",
        "- Computes the 128-dim MoCo embedding for each image\n",
        "- Saves two tensors:\n",
        "1) features ‚Üí shape [N, 128]\n",
        "2) labels ‚Üí shape [N]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmS_fZPU9djA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_features(encoder_q, dataloader, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Extract frozen features from encoder_q.\n",
        "    Returns:\n",
        "        features: Tensor of shape [N, 128]\n",
        "        labels:   Tensor of shape [N]\n",
        "    \"\"\"\n",
        "    encoder_q.eval()\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "\n",
        "        # Forward pass through frozen encoder\n",
        "        feats = encoder_q(images)\n",
        "\n",
        "        # In case encoder_q doesn't normalize (but you do ‚Äî this is safe)\n",
        "        feats = F.normalize(feats, dim=1)\n",
        "\n",
        "        features_list.append(feats.cpu())\n",
        "        labels_list.append(labels.cpu())\n",
        "\n",
        "    # Concatenate all batches into big tensors\n",
        "    all_features = torch.cat(features_list, dim=0)\n",
        "    all_labels = torch.cat(labels_list, dim=0)\n",
        "\n",
        "    print(\"Finished extracting features.\")\n",
        "    print(\"Feature tensor shape:\", all_features.shape)\n",
        "    print(\"Labels tensor shape:\", all_labels.shape)\n",
        "\n",
        "    return all_features, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3PEZfH59dmf",
        "outputId": "caddf697-f7f1-4ccb-ecf6-fc6cb7a94cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished extracting features.\n",
            "Feature tensor shape: torch.Size([50000, 128])\n",
            "Labels tensor shape: torch.Size([50000])\n",
            "Finished extracting features.\n",
            "Feature tensor shape: torch.Size([10000, 128])\n",
            "Labels tensor shape: torch.Size([10000])\n"
          ]
        }
      ],
      "source": [
        "train_feats, train_labels = extract_features(\n",
        "    model.encoder_q,\n",
        "    cifar_train_loader\n",
        ")\n",
        "\n",
        "test_feats, test_labels = extract_features(\n",
        "    model.encoder_q,\n",
        "    cifar_test_loader\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLw9NBmHBhks"
      },
      "source": [
        "## STEP 3 ‚Äî k-NN Classifier for SSL Evaluation\n",
        "\n",
        "This implementation uses:\n",
        "- cosine similarity\n",
        "- top-k = 20 neighbors\n",
        "- distance weighting (stronger version of k-NN used in SSL papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob9gDnGVj474"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def knn_classifier(train_feats, train_labels, test_feats, test_labels, k=10, temperature=0.1):\n",
        "    \"\"\"\n",
        "    Standard SSL k-NN classifier.\n",
        "    Args:\n",
        "        train_feats: [N_train, D] tensor\n",
        "        train_labels: [N_train] tensor\n",
        "        test_feats: [N_test, D] tensor\n",
        "        test_labels: [N_test] tensor\n",
        "        k: number of neighbors (default 20, used in MoCo/SimCLR)\n",
        "        temperature: softmax temperature for similarity weighting\n",
        "    Returns:\n",
        "        top-1 accuracy (%)\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize features (important for cosine similarity)\n",
        "    train_feats = F.normalize(train_feats, dim=1)\n",
        "    test_feats = F.normalize(test_feats, dim=1)\n",
        "\n",
        "    num_test = test_feats.size(0)\n",
        "    batch_size = 100\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    print(\"Running k-NN evaluation...\")\n",
        "\n",
        "    for i in range(0, num_test, batch_size):\n",
        "        # Batch slice\n",
        "        end = min(i + batch_size, num_test)\n",
        "        batch = test_feats[i:end]   # shape [B, D]\n",
        "\n",
        "        # Compute cosine similarity: [B, D] x [D, N_train] = [B, N_train]\n",
        "        sim = torch.mm(batch, train_feats.t())\n",
        "\n",
        "        # For each test sample ‚Üí get top-k neighbors\n",
        "        sim_val, sim_idx = sim.topk(k=k, dim=1)\n",
        "\n",
        "        # Retrieve their labels\n",
        "        neighbor_labels = train_labels[sim_idx]   # shape [B, k]\n",
        "\n",
        "        # Weight votes using softmax over similarity / temperature\n",
        "        weights = torch.exp(sim_val / temperature)\n",
        "\n",
        "        # Score per class\n",
        "        # num_classes = 10 for CIFAR-10\n",
        "        num_classes = train_labels.max().item() + 1\n",
        "        class_scores = torch.zeros((batch.size(0), num_classes))\n",
        "\n",
        "        for j in range(batch.size(0)):\n",
        "            neighbors = neighbor_labels[j]    # shape [k]\n",
        "            w = weights[j]                    # shape [k]\n",
        "            for n, weight in zip(neighbors, w):\n",
        "                class_scores[j, n] += weight.item()\n",
        "\n",
        "        # Prediction = the class with highest score\n",
        "        preds = class_scores.argmax(dim=1)\n",
        "\n",
        "        # Count correct predictions\n",
        "        correct += (preds == test_labels[i:end]).sum().item()\n",
        "\n",
        "    acc = 100.0 * correct / num_test\n",
        "    print(f\"k-NN accuracy (k={k}): {acc:.2f}%\")\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CteOJjenB4cT"
      },
      "source": [
        "Running the KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNVWYx78j5BZ",
        "outputId": "b8ec7cf6-d298-453d-b824-6e0f54f30365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running k-NN evaluation...\n",
            "k-NN accuracy (k=20): 12.82%\n"
          ]
        }
      ],
      "source": [
        "knn_acc = knn_classifier(\n",
        "    train_feats, train_labels,\n",
        "    test_feats, test_labels,\n",
        "    k=20,\n",
        "    temperature=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB5t9IV9DVxl"
      },
      "source": [
        "## STEP ‚Äî Linear Probe on Frozen Features\n",
        "This is what this block does for us:\n",
        "Use the CIFAR extracted features you already computed:\n",
        "- train_feats (50k √ó 128)\n",
        "- train_labels\n",
        "- test_feats\n",
        "- test_labels\n",
        "\n",
        "Train a small linear classifier:\n",
        "- 128 ‚Üí 10\n",
        "- No hidden layers\n",
        "- No dropout\n",
        "- This is the standard evaluation protocol in SSL papers.\n",
        "- Train for ~20 epochs (quick and enough for CIFAR)\n",
        "\n",
        "Optimization:\n",
        "- CrossEntropyLoss\n",
        "- Adam or SGD\n",
        "- Batch size = 1024 (fast ‚Äî data fits into RAM)\n",
        "- Shuffle training batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrmA3jDQ7Y4C"
      },
      "outputs": [],
      "source": [
        "def linear_probe(\n",
        "    train_feats, train_labels,\n",
        "    test_feats, test_labels,\n",
        "    num_epochs=50,\n",
        "    batch_size=1024,\n",
        "    lr=0.1,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "\n",
        "    train_dataset = TensorDataset(train_feats, train_labels)\n",
        "    test_dataset = TensorDataset(test_feats, test_labels)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Linear layer\n",
        "    # classifier = nn.Linear(train_feats.size(1), 10).to(device)  # for cifar 10 use this\n",
        "    classifier = nn.Linear(train_feats.size(1), 100).to(device) # for cifar 100, use this\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    num_epochs = 50\n",
        "    # SGD optimizer (better for linear probe)\n",
        "    # optimizer = torch.optim.SGD(\n",
        "    #     classifier.parameters(),\n",
        "    #     lr=lr,\n",
        "    #     momentum=0.9,\n",
        "    #     weight_decay=1e-4\n",
        "    # )\n",
        "\n",
        "    # Cosine LR schedule\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=num_epochs\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for feats, labels in train_loader:\n",
        "            feats = feats.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = classifier(feats)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluate\n",
        "        classifier.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for feats, labels in test_loader:\n",
        "                feats = feats.to(device)\n",
        "                labels = labels.to(device)\n",
        "                logits = classifier(feats)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | Test Acc: {acc:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd3S9W_5EJFE"
      },
      "source": [
        "Running linear probe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPSygQK77Y61",
        "outputId": "a0a25d3f-8e6b-4ab4-c4c4-83e8a8b1c817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 | Loss: 225.7438 | Test Acc: 1.00%\n",
            "Epoch 2/50 | Loss: 225.6652 | Test Acc: 1.05%\n",
            "Epoch 3/50 | Loss: 225.6352 | Test Acc: 1.54%\n",
            "Epoch 4/50 | Loss: 225.6112 | Test Acc: 1.99%\n",
            "Epoch 5/50 | Loss: 225.5855 | Test Acc: 1.62%\n",
            "Epoch 6/50 | Loss: 225.5641 | Test Acc: 1.59%\n",
            "Epoch 7/50 | Loss: 225.5410 | Test Acc: 1.60%\n",
            "Epoch 8/50 | Loss: 225.5192 | Test Acc: 1.88%\n",
            "Epoch 9/50 | Loss: 225.5122 | Test Acc: 2.27%\n",
            "Epoch 10/50 | Loss: 225.4850 | Test Acc: 1.87%\n",
            "Epoch 11/50 | Loss: 225.4779 | Test Acc: 2.22%\n",
            "Epoch 12/50 | Loss: 225.4595 | Test Acc: 2.02%\n",
            "Epoch 13/50 | Loss: 225.4494 | Test Acc: 2.18%\n",
            "Epoch 14/50 | Loss: 225.4356 | Test Acc: 2.67%\n",
            "Epoch 15/50 | Loss: 225.4223 | Test Acc: 2.59%\n",
            "Epoch 16/50 | Loss: 225.4124 | Test Acc: 2.78%\n",
            "Epoch 17/50 | Loss: 225.3978 | Test Acc: 2.85%\n",
            "Epoch 18/50 | Loss: 225.3915 | Test Acc: 2.65%\n",
            "Epoch 19/50 | Loss: 225.3806 | Test Acc: 2.26%\n",
            "Epoch 20/50 | Loss: 225.3695 | Test Acc: 2.84%\n",
            "Epoch 21/50 | Loss: 225.3616 | Test Acc: 2.97%\n",
            "Epoch 22/50 | Loss: 225.3540 | Test Acc: 3.07%\n",
            "Epoch 23/50 | Loss: 225.3450 | Test Acc: 3.63%\n",
            "Epoch 24/50 | Loss: 225.3391 | Test Acc: 3.26%\n",
            "Epoch 25/50 | Loss: 225.3340 | Test Acc: 3.55%\n",
            "Epoch 26/50 | Loss: 225.3270 | Test Acc: 3.15%\n",
            "Epoch 27/50 | Loss: 225.3228 | Test Acc: 3.56%\n",
            "Epoch 28/50 | Loss: 225.3168 | Test Acc: 3.28%\n",
            "Epoch 29/50 | Loss: 225.3101 | Test Acc: 3.91%\n",
            "Epoch 30/50 | Loss: 225.3040 | Test Acc: 4.45%\n",
            "Epoch 31/50 | Loss: 225.2991 | Test Acc: 3.78%\n",
            "Epoch 32/50 | Loss: 225.2964 | Test Acc: 4.30%\n",
            "Epoch 33/50 | Loss: 225.2906 | Test Acc: 4.11%\n",
            "Epoch 34/50 | Loss: 225.2873 | Test Acc: 3.90%\n",
            "Epoch 35/50 | Loss: 225.2849 | Test Acc: 4.23%\n",
            "Epoch 36/50 | Loss: 225.2809 | Test Acc: 4.85%\n",
            "Epoch 37/50 | Loss: 225.2782 | Test Acc: 4.41%\n",
            "Epoch 38/50 | Loss: 225.2751 | Test Acc: 5.14%\n",
            "Epoch 39/50 | Loss: 225.2744 | Test Acc: 4.83%\n",
            "Epoch 40/50 | Loss: 225.2715 | Test Acc: 4.96%\n",
            "Epoch 41/50 | Loss: 225.2697 | Test Acc: 4.79%\n",
            "Epoch 42/50 | Loss: 225.2679 | Test Acc: 4.88%\n",
            "Epoch 43/50 | Loss: 225.2666 | Test Acc: 4.93%\n",
            "Epoch 44/50 | Loss: 225.2654 | Test Acc: 4.87%\n",
            "Epoch 45/50 | Loss: 225.2647 | Test Acc: 5.16%\n",
            "Epoch 46/50 | Loss: 225.2640 | Test Acc: 4.95%\n",
            "Epoch 47/50 | Loss: 225.2633 | Test Acc: 4.95%\n",
            "Epoch 48/50 | Loss: 225.2629 | Test Acc: 5.03%\n",
            "Epoch 49/50 | Loss: 225.2621 | Test Acc: 5.03%\n",
            "Epoch 50/50 | Loss: 225.2623 | Test Acc: 5.04%\n"
          ]
        }
      ],
      "source": [
        "lp_acc = linear_probe(\n",
        "    train_feats, train_labels,\n",
        "    test_feats, test_labels,\n",
        "    num_epochs=50,\n",
        "    batch_size=1024,\n",
        "    lr=1e-3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-5iBFOz7Y9i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrp1FrZT7ZAc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b7034d9ea9c4e2ab22e196d702fb83a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d11ed76d4b8445cb70d65465e8136fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bbd7b74bf16402f92bc767fa654bbc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32dc832a46384d0cabe497ead8f0b77e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "428dddcbb898493cb244bcd05857147a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32dc832a46384d0cabe497ead8f0b77e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d4f95804c8334bdaad7bc1265d882f89",
            "value": "Fetching‚Äá5‚Äáfiles:‚Äá100%"
          }
        },
        "56099ea2aaa84b90ad0b3f75e684c865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_428dddcbb898493cb244bcd05857147a",
              "IPY_MODEL_c2ac3c5829274457a55ec2775f43963a",
              "IPY_MODEL_9959a4e962754429adedfcb3506a7c82"
            ],
            "layout": "IPY_MODEL_0b7034d9ea9c4e2ab22e196d702fb83a"
          }
        },
        "6ba856ec423549e29b393e810d9d5bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9959a4e962754429adedfcb3506a7c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2b525e8e60f42c79c498617417d66c9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0d11ed76d4b8445cb70d65465e8136fc",
            "value": "‚Äá5/5‚Äá[00:00&lt;00:00,‚Äá521.50it/s]"
          }
        },
        "c2ac3c5829274457a55ec2775f43963a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ba856ec423549e29b393e810d9d5bc4",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bbd7b74bf16402f92bc767fa654bbc6",
            "value": 5
          }
        },
        "d4f95804c8334bdaad7bc1265d882f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2b525e8e60f42c79c498617417d66c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}