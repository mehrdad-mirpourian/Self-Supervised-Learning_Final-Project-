{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehOyrZhNs4Bt"
      },
      "source": [
        "## STEP 1: Import All Required Libraries and Set Up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfyPCyHddzOf"
      },
      "outputs": [],
      "source": [
        "# PyTorch core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Torchvision (datasets, transforms, ResNet backbone)\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Utility libraries\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "# ================================================================\n",
        "# Check GPU availability\n",
        "# ================================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ================================================================\n",
        "# Set random seeds for reproducibility\n",
        "# ================================================================\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Ensures deterministic behavior (may slow down training slightly)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"Environment setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POB4xHzptXnF"
      },
      "source": [
        "## STEP 2: Define MoCo-v2 Style Data Augmentations\n",
        "\n",
        "This class builds the MoCo-v2 data augmentation pipeline and applies it twice to the same input image.\n",
        "The result is two different strongly-augmented views of the same image, which are used as the positive pair in contrastive self-supervised learning.\n",
        "\n",
        "In other words:\n",
        "\n",
        "- Take one image\n",
        "- Apply strong random augmentations twice\n",
        "- Produce two correlated but different views\n",
        "- Feed them to the contrastive model (query/key)\n",
        "\n",
        "This encourages the model to learn invariance to color, crop, blur, distortion, etc., which is essential for MoCo-style SSL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaO9rOedtgCj"
      },
      "outputs": [],
      "source": [
        "# These augmentations generate *two strongly augmented views* of each image.\n",
        "# They are essential for contrastive learning because the model must learn\n",
        "# invariances to color, crop, blur, distortion, etc.\n",
        "\n",
        "class MoCoTransform:\n",
        "    \"\"\"Applies two strong augmentations to the same image.\"\"\"\n",
        "\n",
        "    def __init__(self, image_size=96):\n",
        "        # MoCo-v2 Augmentation pipeline:\n",
        "        self.base_transform = T.Compose([\n",
        "            # 1. Random resized crop (strong spatial diversity)\n",
        "            T.RandomResizedCrop(image_size, scale=(0.2, 1.0)),\n",
        "\n",
        "            # 2. Random horizontal flip\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "            # 3. Color jitter (brightness, contrast, saturation, hue)\n",
        "            T.RandomApply(\n",
        "                [T.ColorJitter(0.4, 0.4, 0.4, 0.1)],\n",
        "                p=0.8\n",
        "            ),\n",
        "\n",
        "            # 4. Random grayscale conversion\n",
        "            T.RandomGrayscale(p=0.2),\n",
        "\n",
        "            # Mild solarization (improves invariance)\n",
        "            T.RandomApply(\n",
        "                [T.RandomSolarize(128)],\n",
        "                p=0.2\n",
        "            ),\n",
        "\n",
        "            # 5. Gaussian blur (MoCo v2 uses it heavily)\n",
        "            T.RandomApply(\n",
        "                [T.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0))],\n",
        "                p=0.5\n",
        "            ),\n",
        "\n",
        "            # 6. Convert PIL → Tensor\n",
        "            T.ToTensor(),\n",
        "\n",
        "            # 7. Normalize to standard ImageNet stats\n",
        "            T.Normalize(\n",
        "                mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Return TWO differently augmented views of the same image\n",
        "        return self.base_transform(x), self.base_transform(x)\n",
        "\n",
        "\n",
        "print(\"MoCo-v2 augmentations initialized successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NogvyTHhynxe"
      },
      "source": [
        "## STEP 3: Implement the Contrastive Loss (InfoNCE) + Covariance Regularization Loss\n",
        "\n",
        "This module defines a hybrid loss for our self-supervised model that combines:\n",
        "\n",
        "1. MoCo-style InfoNCE contrastive loss\n",
        "\n",
        "This part forces the model to pull together the embeddings of two augmented views of the same image (q and k⁺) and push them away from a large set of negative samples stored in a FIFO queue.\n",
        "This teaches invariance to augmentations and builds discriminative features.\n",
        "\n",
        "2. VICReg-style covariance regularization\n",
        "\n",
        "In addition to contrastive learning, this part penalizes correlations between embedding dimensions so each feature dimension carries unique information.\n",
        "This reduces redundancy and improves representation quality.\n",
        "\n",
        "We can choose whether this covariance term uses:\n",
        "\n",
        "- only the positive pair (q, k⁺), or\n",
        "- (optionally) samples from the negative queue as well.\n",
        "\n",
        "3. Final output\n",
        "\n",
        "The function returns:\n",
        "- the combined loss used for backprop,\n",
        "- the pure contrastive component,\n",
        "- the pure covariance regularization component.\n",
        "\n",
        "So overall, this block creates a hybrid MoCo + VICReg loss that encourages:\n",
        "- invariance to augmentations (via contrastive loss)\n",
        "- diversity across feature dimensions (via covariance regularization)\n",
        "\n",
        "This helps our model learn richer, more stable representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ftI9JuethEL"
      },
      "outputs": [],
      "source": [
        "# This includes two main components:\n",
        "# 1. MoCo InfoNCE contrastive loss.\n",
        "# 2. VICReg-style covariance loss to decorrelate embedding dimensions.\n",
        "\n",
        "# We support:\n",
        "#   - Using only (q, k+) for covariance loss\n",
        "#   - OR including samples from the negative queue (optional)\n",
        "# via the flag `use_queue_for_cov`.\n",
        "\n",
        "\n",
        "class MoCoCovLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.2, lambda_cov=1.0, use_queue_for_cov=False):\n",
        "        \"\"\"\n",
        "        temperature: contrastive temperature parameter (tau)\n",
        "        lambda_cov: weight for covariance regularization\n",
        "        use_queue_for_cov: whether to include queue samples in covariance loss\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.tau = temperature\n",
        "        self.lambda_cov = lambda_cov\n",
        "        self.use_queue_for_cov = use_queue_for_cov\n",
        "\n",
        "    def forward(self, q, k_pos, queue_neg):\n",
        "        \"\"\"\n",
        "        q: (B, D) query embeddings\n",
        "        k_pos: (B, D) positive key embeddings\n",
        "        queue_neg: (D, K) negative keys from FIFO queue\n",
        "        \"\"\"\n",
        "        # ------------------------------------------------------------\n",
        "        # 1. Compute InfoNCE contrastive loss\n",
        "        # ------------------------------------------------------------\n",
        "        # Positive logits: q · k+\n",
        "        pos_logits = torch.sum(q * k_pos, dim=1, keepdim=True)  # (B, 1)\n",
        "\n",
        "        # Negative logits: q · K\n",
        "        neg_logits = torch.einsum('nd,dk->nk', q, queue_neg)     # (B, K)\n",
        "\n",
        "        # Scale by temperature\n",
        "        pos_logits = pos_logits / self.tau\n",
        "        neg_logits = neg_logits / self.tau\n",
        "\n",
        "        # Concatenate pos + neg logits\n",
        "        logits = torch.cat([pos_logits, neg_logits], dim=1)      # (B, 1+K)\n",
        "\n",
        "        # Labels: positive is always index 0\n",
        "        labels = torch.zeros(q.size(0), dtype=torch.long, device=q.device)\n",
        "\n",
        "        # Cross-entropy loss for contrastive learning\n",
        "        loss_contrast = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 2. Compute VICReg covariance regularization loss\n",
        "        # ------------------------------------------------------------\n",
        "        # Build feature matrix Z = [q; k+] or optionally add queue samples\n",
        "        if self.use_queue_for_cov:\n",
        "            # Take a random subset of negatives from the queue\n",
        "            K = queue_neg.shape[1]\n",
        "            num_samples = min(1024, K)  # limit sample size\n",
        "            idx = torch.randperm(K, device=q.device)[:num_samples]\n",
        "            queue_subset = queue_neg[:, idx].T  # shape: (num_samples, D)\n",
        "\n",
        "            Z = torch.cat([q, k_pos, queue_subset], dim=0)  # (B + B + num_samples, D)\n",
        "        else:\n",
        "            Z = torch.cat([q, k_pos], dim=0)  # (2B, D)\n",
        "\n",
        "        # Compute covariance matrix C = Cov(Z)\n",
        "        Z = Z - Z.mean(dim=0, keepdim=True)      # center\n",
        "        C = (Z.T @ Z) / (Z.size(0) - 1)          # covariance: (D, D)\n",
        "\n",
        "        # Penalize off-diagonal elements of covariance\n",
        "        diag_mask = torch.eye(C.size(0), device=C.device).bool()\n",
        "        off_diag = C[~diag_mask]\n",
        "        loss_cov = (off_diag ** 2).sum()\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 3. Combine losses\n",
        "        # ------------------------------------------------------------\n",
        "        loss = loss_contrast + self.lambda_cov * loss_cov\n",
        "\n",
        "        return loss, loss_contrast, loss_cov\n",
        "\n",
        "\n",
        "print(\"MoCo InfoNCE + covariance loss module ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfCMAQCE1c7S"
      },
      "source": [
        "## STEP 4 — MoCo-v2 Model Setup with 128-Dim Projection Head and Negative Queue\n",
        "\n",
        "This block builds the full MoCo-v2 backbone we will use for self-supervised learning. It creates.\n",
        "This entire block assembles a fully functional MoCo-v2 encoder (query + key), builds a matching 128-dim projection head, and initializes the large negative queue used for contrastive learning.\n",
        "\n",
        "1. A Projection Head (2048 → 128)\n",
        "\n",
        "A small MLP that takes ResNet-50 features (2048-dim) and maps them into a 128-dim contrastive space, normalized for InfoNCE training.\n",
        "This is the standard MoCo-v2 projection design.\n",
        "\n",
        "2. A Dual-Encoder MoCo Model (query + key)\n",
        "\n",
        "The model contains:\n",
        "\n",
        "- encoder_q – the normal ResNet-50 backbone + projection head (trainable)\n",
        "\n",
        "- encoder_k – the momentum encoder (no gradient updates)\n",
        "\n",
        "At initialization, encoder_k is copied from encoder_q, ensuring they start identical.\n",
        "This is essential for stability in MoCo-style contrastive learning.\n",
        "\n",
        "Both encoders output 128-dim embeddings, guaranteeing shape consistency with the queue and the loss function.\n",
        "\n",
        "3. A 128-Dim FIFO Queue of Negative Samples\n",
        "\n",
        "A memory queue that stores thousands of past key embeddings and acts as the large negative sample bank for contrastive learning.\n",
        "\n",
        "As new batches arrive:\n",
        "\n",
        "- new key embeddings are enqueued\n",
        "- the oldest ones are removed\n",
        "This keeps a constantly refreshed pool of negatives.\n",
        "\n",
        "4. GPU Instantiation\n",
        "\n",
        "Finally, the model and queue are moved to GPU, ready for SSL training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIM8Pnqm1CC-"
      },
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# FIXED & CORRECTED MoCo Model + Projection Head + Queue (128-dim)\n",
        "# ======================================================================\n",
        "# This block ensures that:\n",
        "#   - encoder_q outputs 128-d features\n",
        "#   - encoder_k outputs 128-d features\n",
        "#   - queue stores 128-d features\n",
        "#   - q, k, queue embeddings perfectly match in dimension\n",
        "# ======================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Projection Head: 2048 → 2048 → 128 (MoCo-v2 style MLP)\n",
        "# ----------------------------------------------------------\n",
        "class ProjectionMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard MoCo-v2 projection head:\n",
        "        - Input: 2048-dim backbone features (ResNet-50)\n",
        "        - Output: 128-dim normalized projection for contrastive learning\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_in=2048, dim_hidden=2048, dim_out=128):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim_in, dim_hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_hidden, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        return F.normalize(x, dim=1)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. MoCo Model (ResNet-50 backbone + projection head)\n",
        "# ----------------------------------------------------------\n",
        "class MoCoResNet50(nn.Module):\n",
        "    \"\"\"\n",
        "    MoCo-v2 architecture:\n",
        "      - encoder_q: trainable\n",
        "      - encoder_k: EMA momentum encoder (no grads)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_feature=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load ResNet-50 backbone\n",
        "        backbone = models.resnet50(weights=None)   # must be randomly initialized\n",
        "        backbone.fc = nn.Identity()               # remove classification layer\n",
        "\n",
        "        # Query encoder: backbone + projection head\n",
        "        self.encoder_q = nn.Sequential(\n",
        "            backbone,\n",
        "            ProjectionMLP(dim_in=2048, dim_hidden=2048, dim_out=dim_feature)\n",
        "        )\n",
        "\n",
        "        # Key encoder (EMA)\n",
        "        backbone_k = models.resnet50(weights=None)\n",
        "        backbone_k.fc = nn.Identity()\n",
        "        self.encoder_k = nn.Sequential(\n",
        "            backbone_k,\n",
        "            ProjectionMLP(dim_in=2048, dim_hidden=2048, dim_out=dim_feature)\n",
        "        )\n",
        "\n",
        "        # Initialize key encoder to match query encoder\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(),\n",
        "                                    self.encoder_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)\n",
        "            param_k.requires_grad = False\n",
        "\n",
        "    # no forward() needed, we call encoder_q and encoder_k separately\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. FIFO Queue for Negative Samples (Dimension = 128)\n",
        "# ----------------------------------------------------------\n",
        "class MoCoQueue(nn.Module):\n",
        "    \"\"\"\n",
        "    FIFO memory queue storing 128-d embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, size=65536, dim=128):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.dim = dim\n",
        "\n",
        "        # queue: (size, dim)\n",
        "        self.register_buffer(\"queue\", torch.randn(size, dim))\n",
        "        self.queue = F.normalize(self.queue, dim=1)\n",
        "\n",
        "        self.register_buffer(\"ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def enqueue(self, keys):\n",
        "        \"\"\"\n",
        "        Add new keys to queue, remove oldest entries.\n",
        "        keys: (batch_size, dim)\n",
        "        \"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "        ptr = int(self.ptr)\n",
        "\n",
        "        # Replace entries\n",
        "        if ptr + batch_size <= self.size:\n",
        "            self.queue[ptr:ptr+batch_size] = keys\n",
        "            self.ptr[0] = (ptr + batch_size) % self.size\n",
        "        else:\n",
        "            # wrap around\n",
        "            n1 = self.size - ptr\n",
        "            n2 = batch_size - n1\n",
        "            self.queue[ptr:] = keys[:n1]\n",
        "            self.queue[:n2] = keys[n1:]\n",
        "            self.ptr[0] = n2\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Instantiate Model + Queue (DIM=128) on GPU\n",
        "# ----------------------------------------------------------\n",
        "model = MoCoResNet50(dim_feature=128).cuda()\n",
        "# queue = MoCoQueue(size=65536, dim=128).cuda()\n",
        "queue = MoCoQueue(size=4096, dim=128).cuda()\n",
        "\n",
        "\n",
        "print(\">>> MoCo-ResNet50 model + 128-d queue initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P38AKEb4jzFw"
      },
      "source": [
        "## STEP 5: Local Dataset Preparation Using Snapshot Download and Random Subsampling\n",
        "\n",
        "\n",
        "This block configures and prepares the unlabeled training dataset for our self-supervised pretraining. Instead of using the full 500k–700k Hugging Face dataset (which is large and slow to load), it downloads only the ZIP shards, extracts metadata, and saves a random subset of x images locally for fast pipeline testing.\n",
        "\n",
        "Here’s what this code accomplishes at a high level:\n",
        "\n",
        "1. Choose the data source\n",
        "\n",
        "We specify that training images should come from a local x-image subset rather than the full Hugging Face dataset. This makes early debugging and model testing much faster.\n",
        "\n",
        "2. Download raw dataset ZIP files\n",
        "\n",
        "Using snapshot_download(), the code fetches only the zipped shards from the Hugging Face repo. This avoids downloading unnecessary metadata and keeps the process efficient.\n",
        "\n",
        "3. Parse the ZIP files to see all image names\n",
        "\n",
        "Instead of extracting everything, the code looks inside each ZIP and gathers a list of every .jpg in the dataset.\n",
        "\n",
        "4. Randomly select x images\n",
        "\n",
        "From the full list of available images, the script chooses a random sample of 30k images.\n",
        "This subset is used to quickly test our pipeline before scaling up to the full dataset.\n",
        "\n",
        "5. Extract and save the selected images locally\n",
        "\n",
        "For each selected image:\n",
        "\n",
        "read it directly from its ZIP file\n",
        "\n",
        "This block gives us a clean folder of x .jpg files ready for our dataloader.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LapvCVR4BJhy"
      },
      "source": [
        "### Step 5.a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-RaXu3kBJzI"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# DATA SOURCE CONFIGURATION\n",
        "# ===============================================================\n",
        "# Choose where training images come from.\n",
        "# For pipeline testing, use \"local\".\n",
        "# Later, we can change this to \"hf_full\" or \"local_500k\".\n",
        "\n",
        "DATA_SOURCE = \"local_30k\"    # options: \"local_15k\", \"hf_full\"\n",
        "\n",
        "# Path to local subset in /content or Drive\n",
        "LOCAL_DATA_DIR = \"/content/local_pretrain_30k\"\n",
        "\n",
        "print(\"DATA SOURCE =\", DATA_SOURCE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.b"
      ],
      "metadata": {
        "id": "I-0o3O5gk0qr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBnw74-oKGW0"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# STEP 10 — FAST DOWNLOAD USING snapshot_download()  (3–5 minutes)\n",
        "# ===============================================================\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "LOCAL_DATA_DIR = \"/content/local_pretrain_30k\"\n",
        "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
        "\n",
        "if DATA_SOURCE == \"local_30k\":\n",
        "\n",
        "    print(\"✓ Using snapshot_download() to fetch dataset shards...\")\n",
        "\n",
        "    repo_dir = snapshot_download(\n",
        "        repo_id=\"tsbpp/fall2025_deeplearning\",\n",
        "        repo_type=\"dataset\",\n",
        "        local_dir=\"/content/hf_raw\",\n",
        "        allow_patterns=[\"*.zip\"],       # only zip files\n",
        "    )\n",
        "\n",
        "    print(\"✓ Download complete. Extracting shards...\")\n",
        "\n",
        "    zip_files = sorted([f for f in os.listdir(repo_dir) if f.endswith(\".zip\")])\n",
        "\n",
        "    # extract only what we need\n",
        "    extracted_images = []\n",
        "    for z in tqdm(zip_files, desc=\"Extracting ZIPs\"):\n",
        "        zip_path = os.path.join(repo_dir, z)\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            # list all image file names inside the zip\n",
        "            for name in zf.namelist():\n",
        "                if name.endswith(\".jpg\"):\n",
        "                    extracted_images.append((zip_path, name))\n",
        "\n",
        "    # pick random 30000\n",
        "    import random\n",
        "    random.seed(42)\n",
        "    selected = random.sample(extracted_images, 30000)\n",
        "\n",
        "    print(\"Saving 30000 images locally...\")\n",
        "    for i, (zip_path, name) in enumerate(tqdm(selected)):\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            data = zf.read(name)\n",
        "        out_path = os.path.join(LOCAL_DATA_DIR, f\"img_{i:05d}.jpg\")\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            f.write(data)\n",
        "\n",
        "    print(f\"✓ Saved 30000 images to {LOCAL_DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 6: Building a Local SSL Dataset and DataLoader for MoCo Training\n",
        "\n",
        "\n",
        "This block constructs the dataset and dataloader that feed training images into our MoCo pipeline. It loads the locally saved JPEG images (e.g., the 30k subset we created earlier), applies the MoCo augmentation pipeline, and prepares batches for contrastive training.\n",
        "\n",
        "Here’s the high-level purpose:\n",
        "\n",
        "1. Custom Dataset for Local Images\n",
        "\n",
        "The LocalMoCoDataset class:\n",
        "- scans a directory of .jpg files,\n",
        "- loads each image from disk,\n",
        "- applies our MoCoTransform to produce two augmented views of the same image,\n",
        "- returns these paired views for contrastive learning.\n",
        "\n",
        "This turns our folder of unlabeled images into the correct supervised-by-augmentations structure needed for MoCo-style SSL.\n",
        "\n",
        "2. Create a DataLoader for Efficient Training\n",
        "\n",
        "The DataLoader:\n",
        "- batches the augmented pairs (e.g., 128 images per batch),\n",
        "- shuffles the dataset,\n",
        "- preloads data with workers (num_workers=2),\n",
        "- drops the last incomplete batch for consistency.\n",
        "\n",
        "This ensures that our SSL training loop receives a steady stream of (x1, x2) pairs efficiently and with proper batching."
      ],
      "metadata": {
        "id": "CA0G1d5ek8wz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdusBVtLBWSa"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# STEP — Create dataset & dataloader for LOCAL 15k images\n",
        "# ================================================================\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "\n",
        "class LocalMoCoDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads images saved locally in /content/local_pretrain_3k\n",
        "    Returns two augmented views for MoCo training.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, transform):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.files = sorted(glob.glob(os.path.join(root_dir, \"*.jpg\")))\n",
        "        print(f\"Local dataset: {len(self.files)} images found.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.files[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        x1, x2 = self.transform(img)\n",
        "        return x1, x2\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Instantiate dataset + loader\n",
        "# ================================================================\n",
        "ssl_transform = MoCoTransform(image_size=96)\n",
        "\n",
        "ssl_dataset = LocalMoCoDataset(\n",
        "    root_dir=LOCAL_DATA_DIR,\n",
        "    transform=ssl_transform\n",
        ")\n",
        "\n",
        "ssl_loader = DataLoader(\n",
        "    ssl_dataset,\n",
        "    batch_size=128,\n",
        "    # batch_size=config[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "print(\"✓ Local SSL DataLoader created.\")\n",
        "print(\"Batches per epoch:\", len(ssl_loader))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fqjX3wh3Ind"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMiRISjAulPK"
      },
      "source": [
        "## STEP 7 — Setting Up Full MoCo-Cov Training Configuration (Hyperparameters, Optimizer, Scheduler, Checkpointing)\n",
        "\n",
        "This block prepares everything needed for real MoCo-Cov self-supervised training, but does not start training yet. It sets up hyperparameters, optimizer, learning-rate schedule, and checkpoint paths—essentially the full training environment so our next step can immediately begin pretraining.\n",
        "\n",
        "Here’s the high-level purpose:\n",
        "\n",
        "1. Define All Training Hyperparameters\n",
        "\n",
        "A single config dictionary stores every important setting for MoCo-Cov training, including:\n",
        "\n",
        "- number of epochs\n",
        "- batch size\n",
        "- learning rate & momentum\n",
        "- InfoNCE temperature\n",
        "- EMA momentum for the key encoder\n",
        "- covariance-loss strength\n",
        "- queue size\n",
        "- checkpoint file path\n",
        "\n",
        "This gives our training loop a clear and centralized configuration we can easily adjust later.\n",
        "\n",
        "2. Create the Optimizer (SGD + Momentum)\n",
        "\n",
        "The model parameters are attached to an SGD optimizer, which is the standard optimizer used in MoCo-v2 and similar contrastive SSL methods.\n",
        "This sets momentum and weight decay exactly as in the original papers.\n",
        "\n",
        "3. Set Up a Cosine Learning-Rate Scheduler\n",
        "\n",
        "We initialize a cosine-annealing schedule, which slowly decreases the LR from its initial value down to zero across all epochs.\n",
        "This is a standard and effective schedule for contrastive pretraining.\n",
        "\n",
        "4. Prepare Checkpointing Settings\n",
        "\n",
        "Defines where to save training checkpoints and how often to save them (every epoch), ensuring training can resume if interrupted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqRBYakYj42G"
      },
      "outputs": [],
      "source": [
        "# Goal:\n",
        "#   - Define all hyperparameters for the REAL MoCo-Cov training\n",
        "#   - Create optimizer (SGD)\n",
        "#   - Create cosine LR scheduler\n",
        "#   - Prepare checkpointing\n",
        "#   - DO NOT start training yet (training will be Step 12)\n",
        "\n",
        "# This block sets up everything needed for long SSL pretraining.\n",
        "# ================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "print(\"Configuring MoCo-Cov training ...\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 1. Hyperparameters for sanity-check SSL training\n",
        "# --------------------------------------------------------\n",
        "config = {\n",
        "    \"epochs\": 20,              # short sanity check\n",
        "    \"batch_size\": 128,        # matches our DataLoader\n",
        "    \"lr\": 0.03,               # standard MoCo-v2 LR for batch 256 → scaled here\n",
        "    \"momentum\": 0.9,          # SGD momentum\n",
        "    \"weight_decay\": 1e-4,     # standard regularization for ResNet-50\n",
        "    \"tau\": 0.2,               # InfoNCE temperature\n",
        "    \"m\": 0.999,               # EMA momentum for encoder_k\n",
        "    \"lambda_cov\": 1.0,        # strength of VICReg covariance loss\n",
        "    # \"queue_size\": 65536,      # typical MoCo queue size (~65k)   # later we need to change this\n",
        "    \"queue_size\": 4096,\n",
        "    \"save_every\": 1,          # save model every epoch for safety\n",
        "    \"checkpoint_path\": \"/content/moco_cov_checkpoint.pth\"\n",
        "}\n",
        "\n",
        "# Print configuration\n",
        "for k,v in config.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2. Re-create optimizer (SGD)\n",
        "# --------------------------------------------------------\n",
        "# We assume 'model' already exists (encoder_q + proj head)\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=config[\"lr\"],\n",
        "    momentum=config[\"momentum\"],\n",
        "    weight_decay=config[\"weight_decay\"],\n",
        ")\n",
        "\n",
        "print(\"\\nOptimizer ready (SGD with momentum).\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 3. Cosine learning rate schedule\n",
        "# --------------------------------------------------------\n",
        "# Cosine annealing is very standard for MoCo-v2:\n",
        "# LR(t) = 0.5 * lr * (1 + cos(pi * t / T))\n",
        "\n",
        "scheduler = CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=config[\"epochs\"],   # one full cosine cycle over training\n",
        "    eta_min=0.0               # final LR goes to zero\n",
        ")\n",
        "\n",
        "print(\"Cosine LR scheduler ready.\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 4. Final print-out\n",
        "# --------------------------------------------------------\n",
        "print(\"\\n*** Step 11 completed. ***\")\n",
        "print(\"We can now proceed to Step 12 — Running full SSL pretraining.\")\n",
        "print(\"We will start training ONLY when we request it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzB1kCEoxqWq"
      },
      "source": [
        "## STEP 8: MoCo-Cov Sanity-Check Pretraining Loop\n",
        "\n",
        "This block runs a complete self-supervised training loop using the MoCo-ResNet50 model on our local subset. Each iteration:\n",
        "\n",
        "1. Computes q and k embeddings from two augmented views.\n",
        "2. Builds the contrastive logits using the positive pair and the queue of negatives.\n",
        "3. Adds a covariance penalty to reduce redundancy in the embeddings.\n",
        "4. Backpropagates through encoder_q and updates encoder_k with momentum.\n",
        "5. Updates the negative queue with the new keys.\n",
        "6. Logs metrics and saves a checkpoint at the end of each epoch.\n",
        "\n",
        "In short, this block performs the actual MoCo-Cov self-supervised pretraining, combining contrastive learning + covariance regularization in a full end-to-end training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ApMRKM63j44d"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# STEP 12 — SANITY-CHECK SSL PRETRAINING WITH MoCo-RESNET50\n",
        "# ================================================================\n",
        "# This final training loop:\n",
        "#   - Uses our final MoCoResNet50 model\n",
        "#   - Uses MoCoQueue (128-d)\n",
        "#   - Uses MoCoCovLoss (InfoNCE + covariance reg.)\n",
        "#   - Trains for config[\"epochs\"]\n",
        "#   - Logs every 500 steps\n",
        "#   - Saves checkpoint every epoch\n",
        "# ================================================================\n",
        "\n",
        "print(\"Starting SANITY-CHECK MoCo-Cov training on local 15k subset of the data...\\n\")\n",
        "\n",
        "loss_fn = MoCoCovLoss(\n",
        "    temperature=config[\"tau\"],\n",
        "    lambda_cov=config[\"lambda_cov\"],\n",
        "    use_queue_for_cov=False            # best for stability\n",
        ").cuda()\n",
        "\n",
        "num_epochs = config[\"epochs\"]\n",
        "log_every = 100\n",
        "save_every = config[\"save_every\"]\n",
        "m = config[\"m\"]                        # EMA momentum\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_contrast = 0.0\n",
        "    epoch_cov = 0.0\n",
        "\n",
        "    print(f\"\\n==== Epoch {epoch+1}/{num_epochs} ====\")\n",
        "\n",
        "    for step, (x1, x2) in enumerate(ssl_loader):\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # Move batch to GPU\n",
        "        # ---------------------------------------------\n",
        "        x1 = x1.cuda(non_blocking=True)\n",
        "        x2 = x2.cuda(non_blocking=True)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 1. Compute embeddings\n",
        "        # ---------------------------------------------\n",
        "        q = model.encoder_q(x1)\n",
        "        q = F.normalize(q, dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            k = model.encoder_k(x2)\n",
        "            k = F.normalize(k, dim=1)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 2. InfoNCE contrastive loss\n",
        "        # ---------------------------------------------\n",
        "        # positive logits: (B, 1)\n",
        "        l_pos = torch.einsum('nc,nc->n', q, k).unsqueeze(1)\n",
        "\n",
        "        # negative logits: (B, K)\n",
        "        l_neg = torch.einsum('nc,kc->nk', q, queue.queue.clone().detach())\n",
        "\n",
        "        # concatenate logits\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
        "        logits /= config[\"tau\"]\n",
        "\n",
        "        labels = torch.zeros(q.shape[0], dtype=torch.long, device=q.device)\n",
        "\n",
        "        loss_contrast = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 3. VICReg covariance penalty\n",
        "        # ---------------------------------------------\n",
        "        Z = torch.cat([q, k], dim=0)\n",
        "        Z = Z - Z.mean(dim=0, keepdim=True)\n",
        "\n",
        "        C = (Z.T @ Z) / (Z.size(0) - 1)\n",
        "        cov_loss = (C ** 2).sum() - (C.diag() ** 2).sum()\n",
        "\n",
        "        loss_cov = config[\"lambda_cov\"] * cov_loss\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 4. Total loss\n",
        "        # ---------------------------------------------\n",
        "        loss = loss_contrast + loss_cov\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 5. Backprop on encoder_q\n",
        "        # ---------------------------------------------\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 6. Momentum update of encoder_k\n",
        "        # ---------------------------------------------\n",
        "        with torch.no_grad():\n",
        "            for param_q, param_k in zip(model.encoder_q.parameters(), model.encoder_k.parameters()):\n",
        "                param_k.data = param_k.data * m + param_q.data * (1 - m)\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 7. Update the queue\n",
        "        # ---------------------------------------------\n",
        "        queue.enqueue(k.detach())\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 8. Track metrics\n",
        "        # ---------------------------------------------\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_contrast += loss_contrast.item()\n",
        "        epoch_cov += loss_cov.item()\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # 9. Logging every 100 steps\n",
        "        # ---------------------------------------------\n",
        "        if (step + 1) % log_every == 0:\n",
        "            print(\n",
        "                f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
        "                f\"Step {step+1}/{len(ssl_loader)} | \"\n",
        "                f\"Loss: {loss.item():.4f} | \"\n",
        "                f\"Contrast: {loss_contrast.item():.4f} | \"\n",
        "                f\"Cov: {loss_cov.item():.4f} | \"\n",
        "                f\"LR: {scheduler.get_last_lr()[0]:.6f}\"\n",
        "            )\n",
        "\n",
        "        # scheduler step every batch\n",
        "        # scheduler.step()\n",
        "\n",
        "    # ==================================================\n",
        "    # Epoch summary\n",
        "    # ==================================================\n",
        "    print(\n",
        "        f\"\\n>>> Epoch {epoch+1} Summary: \"\n",
        "        f\"AvgLoss={epoch_loss/len(ssl_loader):.4f}, \"\n",
        "        f\"AvgContrast={epoch_contrast/len(ssl_loader):.4f}, \"\n",
        "        f\"AvgCov={epoch_cov/len(ssl_loader):.4f}\"\n",
        "    )\n",
        "    # Step LR scheduler ONCE per epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    # ==================================================\n",
        "    # Save checkpoint\n",
        "    # ==================================================\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": epoch+1,\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"scheduler\": scheduler.state_dict(),\n",
        "            \"queue\": queue.queue.clone(),\n",
        "        },\n",
        "        config[\"checkpoint_path\"]\n",
        "    )\n",
        "    print(f\"Checkpoint saved to {config['checkpoint_path']}\")\n",
        "\n",
        "print(\"\\n=== SANITY-CHECK TRAINING COMPLETE ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d811Y1e116Fw"
      },
      "source": [
        "## STEP 9 a. Preparing CIFAR-10 for Downstream Evaluation  \n",
        "\n",
        "This block loads the CIFAR-10 dataset and prepares it for downstream evaluation of our SSL encoder. It applies simple, non-contrastive transforms (resize + normalize), then builds train/test DataLoaders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv_aNTX99dgu"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Basic eval transforms (no strong augmentations)\n",
        "cifar_transform = T.Compose([\n",
        "    T.Resize((96, 96)),     # match our backbone resolution\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 train/test\n",
        "cifar_train = torchvision.datasets.CIFAR10(\n",
        "    root=\"/content/cifar\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "cifar_test = torchvision.datasets.CIFAR10(\n",
        "    root=\"/content/cifar\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "# Dataloaders\n",
        "cifar_train_loader = DataLoader(\n",
        "    cifar_train,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "cifar_test_loader = DataLoader(\n",
        "    cifar_test,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"CIFAR-10 loaded successfully!\")\n",
        "print(\"Train samples:\", len(cifar_train))\n",
        "print(\"Test samples:\", len(cifar_test))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC-o1jgrYZXw"
      },
      "source": [
        "## STEP 9 b. Preparing CIFAR-100 for Downstream Evaluation  \n",
        "\n",
        "This block loads the CIFAR-100 dataset and prepares it for downstream evaluation of our SSL encoder. It applies simple, non-contrastive transforms (resize + normalize), then builds train/test DataLoaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQxY6HajYZoq"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Basic eval transforms (no strong augmentations)\n",
        "cifar_transform = T.Compose([\n",
        "    T.Resize((96, 96)),     # match our backbone resolution\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 train/test\n",
        "cifar_train = torchvision.datasets.CIFAR100(\n",
        "    root=\"/content/cifar100\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "cifar_test = torchvision.datasets.CIFAR100(\n",
        "    root=\"/content/cifar100\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "# Dataloaders\n",
        "cifar_train_loader = DataLoader(\n",
        "    cifar_train,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "cifar_test_loader = DataLoader(\n",
        "    cifar_test,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"CIFAR-100 loaded successfully!\")\n",
        "print(\"Train samples:\", len(cifar_train))\n",
        "print(\"Test samples:\", len(cifar_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07pWaVZD_nuR"
      },
      "source": [
        "## STEP 10: Extracting Frozen Features for Downstream Evaluation\n",
        "\n",
        "\n",
        "This block defines a helper function that runs our pretrained encoder_q over an evaluation dataset (like CIFAR-10) to extract 128-dim feature vectors for every image. It keeps the encoder frozen, normalizes the embeddings, and returns:\n",
        "\n",
        "one big tensor of all features\n",
        "\n",
        "one big tensor of their labels\n",
        "\n",
        "These extracted features are later used for downstream tasks such as k-NN classification or a linear probe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmS_fZPU9djA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_features(encoder_q, dataloader, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Extract frozen features from encoder_q.\n",
        "    Returns:\n",
        "        features: Tensor of shape [N, 128]\n",
        "        labels:   Tensor of shape [N]\n",
        "    \"\"\"\n",
        "    encoder_q.eval()\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "\n",
        "        # Forward pass through frozen encoder\n",
        "        feats = encoder_q(images)\n",
        "\n",
        "        # In case encoder_q doesn't normalize (but we do — this is safe)\n",
        "        feats = F.normalize(feats, dim=1)\n",
        "\n",
        "        features_list.append(feats.cpu())\n",
        "        labels_list.append(labels.cpu())\n",
        "\n",
        "    # Concatenate all batches into big tensors\n",
        "    all_features = torch.cat(features_list, dim=0)\n",
        "    all_labels = torch.cat(labels_list, dim=0)\n",
        "\n",
        "    print(\"Finished extracting features.\")\n",
        "    print(\"Feature tensor shape:\", all_features.shape)\n",
        "    print(\"Labels tensor shape:\", all_labels.shape)\n",
        "\n",
        "    return all_features, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3PEZfH59dmf"
      },
      "outputs": [],
      "source": [
        "train_feats, train_labels = extract_features(\n",
        "    model.encoder_q,\n",
        "    cifar_train_loader\n",
        ")\n",
        "\n",
        "test_feats, test_labels = extract_features(\n",
        "    model.encoder_q,\n",
        "    cifar_test_loader\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLw9NBmHBhks"
      },
      "source": [
        "## STEP 11: k-NN Classifier for Evaluating SSL Representations\n",
        "\n",
        "\n",
        "This block implements a k-nearest-neighbors classifier used to evaluate how good our SSL features are. It compares test features to all training features using cosine similarity, finds each sample’s top-k neighbors, weights their votes by similarity, and predicts the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob9gDnGVj474"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def knn_classifier(train_feats, train_labels, test_feats, test_labels, k=10, temperature=0.1):\n",
        "    \"\"\"\n",
        "    Standard SSL k-NN classifier.\n",
        "    Args:\n",
        "        train_feats: [N_train, D] tensor\n",
        "        train_labels: [N_train] tensor\n",
        "        test_feats: [N_test, D] tensor\n",
        "        test_labels: [N_test] tensor\n",
        "        k: number of neighbors (default 20, used in MoCo/SimCLR)\n",
        "        temperature: softmax temperature for similarity weighting\n",
        "    Returns:\n",
        "        top-1 accuracy (%)\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize features (important for cosine similarity)\n",
        "    train_feats = F.normalize(train_feats, dim=1)\n",
        "    test_feats = F.normalize(test_feats, dim=1)\n",
        "\n",
        "    num_test = test_feats.size(0)\n",
        "    batch_size = 100\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    print(\"Running k-NN evaluation...\")\n",
        "\n",
        "    for i in range(0, num_test, batch_size):\n",
        "        # Batch slice\n",
        "        end = min(i + batch_size, num_test)\n",
        "        batch = test_feats[i:end]   # shape [B, D]\n",
        "\n",
        "        # Compute cosine similarity: [B, D] x [D, N_train] = [B, N_train]\n",
        "        sim = torch.mm(batch, train_feats.t())\n",
        "\n",
        "        # For each test sample → get top-k neighbors\n",
        "        sim_val, sim_idx = sim.topk(k=k, dim=1)\n",
        "\n",
        "        # Retrieve their labels\n",
        "        neighbor_labels = train_labels[sim_idx]   # shape [B, k]\n",
        "\n",
        "        # Weight votes using softmax over similarity / temperature\n",
        "        weights = torch.exp(sim_val / temperature)\n",
        "\n",
        "        # Score per class\n",
        "        # num_classes = 10 for CIFAR-10\n",
        "        num_classes = train_labels.max().item() + 1\n",
        "        class_scores = torch.zeros((batch.size(0), num_classes))\n",
        "\n",
        "        for j in range(batch.size(0)):\n",
        "            neighbors = neighbor_labels[j]    # shape [k]\n",
        "            w = weights[j]                    # shape [k]\n",
        "            for n, weight in zip(neighbors, w):\n",
        "                class_scores[j, n] += weight.item()\n",
        "\n",
        "        # Prediction = the class with highest score\n",
        "        preds = class_scores.argmax(dim=1)\n",
        "\n",
        "        # Count correct predictions\n",
        "        correct += (preds == test_labels[i:end]).sum().item()\n",
        "\n",
        "    acc = 100.0 * correct / num_test\n",
        "    print(f\"k-NN accuracy (k={k}): {acc:.2f}%\")\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CteOJjenB4cT"
      },
      "source": [
        "Running the KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNVWYx78j5BZ"
      },
      "outputs": [],
      "source": [
        "knn_acc = knn_classifier(\n",
        "    train_feats, train_labels,\n",
        "    test_feats, test_labels,\n",
        "    k=20,\n",
        "    temperature=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB5t9IV9DVxl"
      },
      "source": [
        "## STEP 12: Linear Probe Classifier for Evaluating SSL Features\n",
        "\n",
        "\n",
        "This block trains a simple linear classifier on top of frozen SSL features to measure how well the MoCo encoder supports supervised tasks. It:\n",
        "\n",
        "- Wraps the features/labels into DataLoaders.\n",
        "- Trains a single linear layer (no nonlinearities) for a small number of epochs.\n",
        "- Evaluates accuracy on a test set after each epoch.\n",
        "- Returns the final accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrmA3jDQ7Y4C"
      },
      "outputs": [],
      "source": [
        "def linear_probe(\n",
        "    train_feats, train_labels,\n",
        "    test_feats, test_labels,\n",
        "    num_epochs=50,\n",
        "    batch_size=1024,\n",
        "    lr=0.1,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "\n",
        "    train_dataset = TensorDataset(train_feats, train_labels)\n",
        "    test_dataset = TensorDataset(test_feats, test_labels)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Linear layer\n",
        "    # classifier = nn.Linear(train_feats.size(1), 10).to(device)  # for cifar 10 use this\n",
        "    classifier = nn.Linear(train_feats.size(1), 100).to(device) # for cifar 100, use this\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    num_epochs = 50\n",
        "    # SGD optimizer (better for linear probe)\n",
        "    # optimizer = torch.optim.SGD(\n",
        "    #     classifier.parameters(),\n",
        "    #     lr=lr,\n",
        "    #     momentum=0.9,\n",
        "    #     weight_decay=1e-4\n",
        "    # )\n",
        "\n",
        "    # Cosine LR schedule\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=num_epochs\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for feats, labels in train_loader:\n",
        "            feats = feats.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = classifier(feats)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluate\n",
        "        classifier.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for feats, labels in test_loader:\n",
        "                feats = feats.to(device)\n",
        "                labels = labels.to(device)\n",
        "                logits = classifier(feats)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | Test Acc: {acc:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd3S9W_5EJFE"
      },
      "source": [
        "Running linear probe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPSygQK77Y61"
      },
      "outputs": [],
      "source": [
        "lp_acc = linear_probe(\n",
        "    train_feats, train_labels,\n",
        "    test_feats, test_labels,\n",
        "    num_epochs=50,\n",
        "    batch_size=1024,\n",
        "    lr=1e-3\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}